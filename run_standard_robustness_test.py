#!/usr/bin/env python3
"""
CAACé²æ£’æ€§æ ‡å‡†æµ‹è¯• - å‚æ•°åŒ–å¿«é€Ÿå¯åŠ¨è„šæœ¬
æ”¯æŒè‡ªå®šä¹‰å™ªå£°æ°´å¹³å’Œç½‘ç»œç»“æ„å‚æ•°
"""

import sys
import argparse
sys.path.append('src')

def run_standard_robustness_test(
    noise_levels=None,
    representation_dim=128,
    feature_hidden_dims=None,
    abduction_hidden_dims=None,
    batch_size=64,
    epochs=150,
    learning_rate=0.001,
    early_stopping_patience=15,
    datasets=None
):
    """
    è¿è¡Œå‚æ•°åŒ–çš„æ ‡å‡†é²æ£’æ€§æµ‹è¯•
    
    Args:
        noise_levels: List[float] - å™ªå£°æ°´å¹³åˆ—è¡¨ï¼Œå¦‚ [0.0, 0.05, 0.10, 0.15, 0.20]
        representation_dim: int - è¡¨å¾ç»´åº¦ï¼Œé»˜è®¤128
        feature_hidden_dims: List[int] - ç‰¹å¾ç½‘ç»œéšè—å±‚ç»´åº¦ï¼Œé»˜è®¤[64]
        abduction_hidden_dims: List[int] - æ¨æ–­ç½‘ç»œéšè—å±‚ç»´åº¦ï¼Œé»˜è®¤[128, 64]
        batch_size: int - æ‰¹é‡å¤§å°ï¼Œé»˜è®¤64
        epochs: int - è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤150
        learning_rate: float - å­¦ä¹ ç‡ï¼Œé»˜è®¤0.001
        early_stopping_patience: int - æ—©åœè€å¿ƒå€¼ï¼Œé»˜è®¤15
        datasets: List[str] - æ•°æ®é›†åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨æ ‡å‡†æ•°æ®é›†
    """
    # è®¾ç½®é»˜è®¤å‚æ•°
    if noise_levels is None:
        noise_levels = [0.0, 0.05, 0.10, 0.15, 0.20]
    if feature_hidden_dims is None:
        feature_hidden_dims = [64]
    if abduction_hidden_dims is None:
        abduction_hidden_dims = [128, 64]
    if datasets is None:
        datasets = ['breast_cancer', 'optical_digits', 'digits', 'synthetic_imbalanced', 'covertype', 'letter']
    
    print("ğŸ§ª CAACæ–¹æ³•Outlieré²æ£’æ€§å‚æ•°åŒ–æµ‹è¯•")
    print("=" * 60)
    print("ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"  â€¢ æ•°æ®é›†: 1ä¸ªå°æ•°æ®é›† + 5ä¸ªä¸­ç­‰è§„æ¨¡æ•°æ®é›† (å…±{len(datasets)}ä¸ªæ•°æ®é›†)")
    print("  â€¢ æ–¹æ³•: CAAC(Cauchy), CAAC(Gaussian), MLP(Softmax), MLP(OvR), MLP(Hinge)")
    print(f"  â€¢ å™ªå£°æ°´å¹³: {[f'{x:.1%}' for x in noise_levels]}")
    print("  â€¢ æ•°æ®åˆ†å‰²: 70% train / 15% val / 15% test")
    print("ğŸ“ˆ ç½‘ç»œç»“æ„:")
    print(f"  â€¢ è¡¨å¾ç»´åº¦: {representation_dim}")
    print(f"  â€¢ ç‰¹å¾ç½‘ç»œéšè—å±‚: {feature_hidden_dims}")
    print(f"  â€¢ æ¨æ–­ç½‘ç»œéšè—å±‚: {abduction_hidden_dims}")
    print("âš™ï¸ è®­ç»ƒå‚æ•°:")
    print(f"  â€¢ æ‰¹é‡å¤§å°: {batch_size}")
    print(f"  â€¢ è®­ç»ƒè½®æ•°: {epochs}")
    print(f"  â€¢ å­¦ä¹ ç‡: {learning_rate}")
    print(f"  â€¢ æ—©åœè€å¿ƒå€¼: {early_stopping_patience}")
    print(f"  â€¢ é¢„è®¡æ—¶é—´: 15-25åˆ†é’Ÿ")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®é›†å¹¶æ˜¾ç¤ºé€‰æ‹©
    from compare_methods_outlier_robustness import load_datasets, create_robust_comparison_methods
    from compare_methods_outlier_robustness import create_robustness_visualizations, create_robustness_heatmap
    from compare_methods_outlier_robustness import analyze_robustness_results, generate_robustness_report
    
    print("\nğŸ“Š åŠ è½½æ•°æ®é›†...")
    all_datasets = load_datasets()
    
    # è¿‡æ»¤é€‰æ‹©çš„æ•°æ®é›†
    filtered_datasets = {k: v for k, v in all_datasets.items() if k in datasets}
    
    print(f"\nğŸ¯ æ ‡å‡†æµ‹è¯•å°†ä½¿ç”¨ä»¥ä¸‹{len(filtered_datasets)}ä¸ªæ•°æ®é›†:")
    total_samples = 0
    for key, dataset in filtered_datasets.items():
        n_samples, n_features = dataset['data'].shape
        n_classes = len(set(dataset['target']))
        size_label = dataset.get('size', 'unknown')
        total_samples += n_samples
        print(f"  â€¢ {dataset['name']}: {n_samples:,}æ ·æœ¬, {n_features}ç‰¹å¾, {n_classes}ç±» [{size_label}]")
    
    print(f"\nğŸ“ˆ æ€»è®¡: {total_samples:,}æ ·æœ¬ across {len(filtered_datasets)}ä¸ªæ•°æ®é›†")
    
    # ç¡®è®¤å¼€å§‹
    print(f"\nâš ï¸  æ³¨æ„: è¿™å°†è¿è¡Œ {len(filtered_datasets)} Ã— {len(noise_levels)}å™ªå£°æ°´å¹³ Ã— 5æ–¹æ³• = {len(filtered_datasets)*len(noise_levels)*5} ä¸ªå®éªŒ")
    
    import time
    print("\nâ° å®éªŒå°†åœ¨5ç§’åè‡ªåŠ¨å¼€å§‹...")
    for i in range(5, 0, -1):
        print(f"   {i}...")
        time.sleep(1)
    
    print("\nğŸš€ å¼€å§‹å‚æ•°åŒ–æµ‹è¯•!")
    print("=" * 60)
    
    try:
        # è¿è¡Œå®éªŒï¼Œä¼ é€’å‚æ•°åŒ–é…ç½®
        results_df = run_parameterized_outlier_robustness_experiments(
            filtered_datasets,
            noise_levels=noise_levels,
            representation_dim=representation_dim,
            feature_hidden_dims=feature_hidden_dims,
            abduction_hidden_dims=abduction_hidden_dims,
            batch_size=batch_size,
            epochs=epochs,
            learning_rate=learning_rate,
            early_stopping_patience=early_stopping_patience
        )
        
        # åˆ›å»ºå¯è§†åŒ–
        print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–...")
        create_robustness_visualizations(results_df)
        create_robustness_heatmap(results_df)
        
        # åˆ†æç»“æœ
        print("\nğŸ” åˆ†æç»“æœ...")
        robustness_df = analyze_robustness_results(results_df)
        
        # ç”ŸæˆæŠ¥å‘Š
        print("\nğŸ“„ ç”ŸæˆæŠ¥å‘Š...")
        report_file = generate_robustness_report(results_df, robustness_df)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ å‚æ•°åŒ–æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        print("ğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  â€¢ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print("  â€¢ é²æ£’æ€§æ›²çº¿: results/caac_outlier_robustness_curves.png")
        print("  â€¢ é²æ£’æ€§çƒ­åŠ›å›¾: results/caac_outlier_robustness_heatmap.png")
        print("  â€¢ åŸå§‹æ•°æ®: results/caac_outlier_robustness_detailed_*.csv")
        print("  â€¢ æ±‡æ€»æ•°æ®: results/caac_outlier_robustness_summary_*.csv")
        
        # æ˜¾ç¤ºå…³é”®å‘ç°
        print("\nğŸ” å…³é”®å‘ç°é¢„è§ˆ:")
        print(f"  â€¢ æœ€é²æ£’æ–¹æ³•: {robustness_df.iloc[0]['Method']}")
        print(f"  â€¢ é²æ£’æ€§å¾—åˆ†: {robustness_df.iloc[0]['Overall_Robustness']:.4f}")
        print(f"  â€¢ æ€§èƒ½è¡°å‡: {robustness_df.iloc[0]['Performance_Drop']:.1f}%")
        
        print("\nğŸ“– æŸ¥çœ‹å®Œæ•´æŠ¥å‘Šè·å–è¯¦ç»†åˆ†æå’Œç»“è®º")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def run_parameterized_outlier_robustness_experiments(
    datasets,
    noise_levels,
    representation_dim,
    feature_hidden_dims,
    abduction_hidden_dims,
    batch_size,
    epochs,
    learning_rate,
    early_stopping_patience
):
    """è¿è¡Œå‚æ•°åŒ–çš„outlieré²æ£’æ€§å¯¹æ¯”å®éªŒ"""
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import StandardScaler
    
    from src.models.caac_ovr_model import (
        CAACOvRModel, 
        SoftmaxMLPModel,
        OvRCrossEntropyMLPModel,
        CAACOvRGaussianModel,
        CrammerSingerMLPModel
    )
    from src.data.data_processor import DataProcessor
    from compare_methods_outlier_robustness import evaluate_method_with_outliers
    
    print("ğŸ”¬ å¼€å§‹è¿è¡Œå‚æ•°åŒ–outlieré²æ£’æ€§å¯¹æ¯”å®éªŒ")
    print("åŒ…å«æ–¹æ³•: CAAC(Cauchy), CAAC(Gaussian), MLP(Softmax), MLP(OvR), MLP(Hinge)")
    print("=" * 80)
    
    # åˆ›å»ºå‚æ•°åŒ–çš„æ–¹æ³•é…ç½®
    methods = create_parameterized_comparison_methods(
        representation_dim=representation_dim,
        feature_hidden_dims=feature_hidden_dims,
        abduction_hidden_dims=abduction_hidden_dims,
        batch_size=batch_size,
        epochs=epochs,
        learning_rate=learning_rate,
        early_stopping_patience=early_stopping_patience
    )
    
    results = []
    
    for dataset_name, dataset in datasets.items():
        print(f"\nğŸ“Š æ­£åœ¨æµ‹è¯•æ•°æ®é›†: {dataset['name']}")
        print("-" * 50)
        
        # æ•°æ®é¢„å¤„ç†
        X = dataset['data']
        y = dataset['target']
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # æ˜¾ç¤ºåŸå§‹ç±»åˆ«åˆ†å¸ƒ
        unique, counts = np.unique(y, return_counts=True)
        print(f"åŸå§‹ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique, counts))}")
        
        for outlier_ratio in noise_levels:
            print(f"\n  ğŸ¯ æµ‹è¯•outlieræ¯”ä¾‹: {outlier_ratio:.1%}")
            
            # ä½¿ç”¨æ–°çš„æ•°æ®åˆ†å‰²ç­–ç•¥
            if outlier_ratio > 0:
                result = DataProcessor.split_classification_data_with_outliers(
                    X_scaled, y,
                    train_size=0.7, val_size=0.15, test_size=0.15,
                    outlier_ratio=outlier_ratio, balance_strategy='proportional',
                    random_state=42
                )
                X_train, X_val, X_test, y_train, y_val, y_test, outlier_info = result
                
                print(f"    Outliersæ·»åŠ : Train={outlier_info['outliers_in_train']}, Val={outlier_info['outliers_in_val']}")
            else:
                # æ— outliersçš„åŸºçº¿
                X_train, X_val, X_test, y_train, y_val, y_test = DataProcessor.split_data(
                    X_scaled, y, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42
                )
                print(f"    åŸºçº¿å®éªŒ (æ— outliers)")
            
            # æµ‹è¯•æ¯ç§æ–¹æ³•
            for method_key, method_info in methods.items():
                print(f"    ğŸ§ª æµ‹è¯•æ–¹æ³•: {method_info['name']}")
                
                try:
                    metrics = evaluate_method_with_outliers(
                        method_info, 
                        X_train, X_val, X_test, 
                        y_train, y_val, y_test
                    )
                    
                    results.append({
                        'Dataset': dataset['name'],
                        'Dataset_Key': dataset_name,
                        'Outlier_Ratio': outlier_ratio,
                        'Method': method_info['name'],
                        'Method_Key': method_key,
                        'Method_Type': method_info['type'],
                        'Description': method_info.get('description', ''),
                        'Accuracy': metrics['accuracy'],
                        'Precision_Macro': metrics['precision_macro'],
                        'Recall_Macro': metrics['recall_macro'],
                        'F1_Macro': metrics['f1_macro'],
                        'F1_Weighted': metrics['f1_weighted'],
                        'Training_Time': metrics['training_time']
                    })
                    
                    print(f"      âœ… å‡†ç¡®ç‡: {metrics['accuracy']:.4f}, F1: {metrics['f1_macro']:.4f}")
                    
                except Exception as e:
                    print(f"      âŒ é”™è¯¯: {str(e)}")
                    continue
    
    return pd.DataFrame(results)


def create_parameterized_comparison_methods(
    representation_dim,
    feature_hidden_dims,
    abduction_hidden_dims,
    batch_size,
    epochs,
    learning_rate,
    early_stopping_patience
):
    """åˆ›å»ºå‚æ•°åŒ–çš„ç”¨äºé²æ£’æ€§æ¯”è¾ƒçš„æ–¹æ³•"""
    from src.models.caac_ovr_model import (
        CAACOvRModel, 
        SoftmaxMLPModel,
        OvRCrossEntropyMLPModel,
        CAACOvRGaussianModel,
        CrammerSingerMLPModel
    )
    
    # å‚æ•°åŒ–çš„åŸºç¡€ç½‘ç»œæ¶æ„å‚æ•°
    base_params = {
        'representation_dim': representation_dim,
        'latent_dim': None,  # é»˜è®¤ç­‰äºrepresentation_dim
        'feature_hidden_dims': feature_hidden_dims,
        'abduction_hidden_dims': abduction_hidden_dims,
        'lr': learning_rate,
        'batch_size': batch_size,
        'epochs': epochs,
        'device': None,
        'early_stopping_patience': early_stopping_patience,
        'early_stopping_min_delta': 0.0001
    }
    
    # CAACæ¨¡å‹ä¸“ç”¨å‚æ•°ï¼ˆåŒ…å«é¢å¤–çš„é²æ£’æ€§å‚æ•°ï¼‰
    caac_params = {
        **base_params,
        'learnable_thresholds': False,
        'uniqueness_constraint': False
    }
    
    # æ ‡å‡†MLPæ¨¡å‹å‚æ•°ï¼ˆä¸åŒ…å«CAACç‰¹æœ‰å‚æ•°ï¼‰
    mlp_params = base_params.copy()
    
    methods = {
        # æ ¸å¿ƒæ–¹æ³•å¯¹æ¯” - æ ¹æ®ç”¨æˆ·è¦æ±‚é€‰æ‹©çš„5ç§æ–¹æ³•
        'CAAC_Cauchy': {
            'name': 'CAAC OvR (Cauchy)',
            'type': 'unified',
            'model_class': CAACOvRModel,
            'params': caac_params,
            'description': f'æŸ¯è¥¿åˆ†å¸ƒ + å›ºå®šé˜ˆå€¼ (è¡¨å¾ç»´åº¦:{representation_dim})'
        },
        'CAAC_Gaussian': {
            'name': 'CAAC OvR (Gaussian)',
            'type': 'unified',
            'model_class': CAACOvRGaussianModel,
            'params': caac_params,
            'description': f'é«˜æ–¯åˆ†å¸ƒ + å›ºå®šé˜ˆå€¼ (è¡¨å¾ç»´åº¦:{representation_dim})'
        },
        'MLP_Softmax': {
            'name': 'MLP (Softmax)',
            'type': 'unified',
            'model_class': SoftmaxMLPModel,
            'params': mlp_params,
            'description': f'æ ‡å‡†å¤šå±‚æ„ŸçŸ¥æœº (è¡¨å¾ç»´åº¦:{representation_dim})'
        },
        'MLP_OvR_CE': {
            'name': 'MLP (OvR Cross Entropy)',
            'type': 'unified',
            'model_class': OvRCrossEntropyMLPModel,
            'params': mlp_params,
            'description': f'OvRç­–ç•¥ (è¡¨å¾ç»´åº¦:{representation_dim})'
        },
        'MLP_Hinge': {
            'name': 'MLP (Crammer & Singer Hinge)',
            'type': 'unified',
            'model_class': CrammerSingerMLPModel,
            'params': mlp_params,
            'description': f'é“°é“¾æŸå¤± (è¡¨å¾ç»´åº¦:{representation_dim})'
        }
    }
    return methods


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description='CAACé²æ£’æ€§å‚æ•°åŒ–æµ‹è¯•')
    
    # å™ªå£°æ°´å¹³å‚æ•°
    parser.add_argument('--noise-levels', nargs='+', type=float, 
                       default=[0.0, 0.05, 0.10, 0.15, 0.20],
                       help='å™ªå£°æ°´å¹³åˆ—è¡¨ (é»˜è®¤: 0.0 0.05 0.10 0.15 0.20)')
    
    # ç½‘ç»œç»“æ„å‚æ•°
    parser.add_argument('--representation-dim', type=int, default=128,
                       help='è¡¨å¾ç»´åº¦ (é»˜è®¤: 128)')
    parser.add_argument('--feature-hidden-dims', nargs='+', type=int, default=[64],
                       help='ç‰¹å¾ç½‘ç»œéšè—å±‚ç»´åº¦ (é»˜è®¤: 64)')
    parser.add_argument('--abduction-hidden-dims', nargs='+', type=int, default=[128, 64],
                       help='æ¨æ–­ç½‘ç»œéšè—å±‚ç»´åº¦ (é»˜è®¤: 128 64)')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch-size', type=int, default=64,
                       help='æ‰¹é‡å¤§å° (é»˜è®¤: 64)')
    parser.add_argument('--epochs', type=int, default=150,
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤: 150)')
    parser.add_argument('--learning-rate', type=float, default=0.001,
                       help='å­¦ä¹ ç‡ (é»˜è®¤: 0.001)')
    parser.add_argument('--early-stopping-patience', type=int, default=15,
                       help='æ—©åœè€å¿ƒå€¼ (é»˜è®¤: 15)')
    
    # æ•°æ®é›†é€‰æ‹©
    parser.add_argument('--datasets', nargs='+', 
                       default=['breast_cancer', 'optical_digits', 'digits', 'synthetic_imbalanced', 'covertype', 'letter'],
                       help='æ•°æ®é›†åˆ—è¡¨ (é»˜è®¤: ä½¿ç”¨æ ‡å‡†æ•°æ®é›†)')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    for noise in args.noise_levels:
        if not (0.0 <= noise <= 1.0):
            print(f"âŒ é”™è¯¯: å™ªå£°æ°´å¹³ {noise} å¿…é¡»åœ¨ [0.0, 1.0] èŒƒå›´å†…")
            sys.exit(1)
    
    if args.representation_dim <= 0:
        print("âŒ é”™è¯¯: è¡¨å¾ç»´åº¦å¿…é¡»ä¸ºæ­£æ•´æ•°")
        sys.exit(1)
    
    if any(dim <= 0 for dim in args.feature_hidden_dims):
        print("âŒ é”™è¯¯: ç‰¹å¾ç½‘ç»œéšè—å±‚ç»´åº¦å¿…é¡»ä¸ºæ­£æ•´æ•°")
        sys.exit(1)
    
    if any(dim <= 0 for dim in args.abduction_hidden_dims):
        print("âŒ é”™è¯¯: æ¨æ–­ç½‘ç»œéšè—å±‚ç»´åº¦å¿…é¡»ä¸ºæ­£æ•´æ•°")
        sys.exit(1)
    
    print("ğŸ¯ ä½¿ç”¨å‚æ•°:")
    print(f"  â€¢ å™ªå£°æ°´å¹³: {args.noise_levels}")
    print(f"  â€¢ è¡¨å¾ç»´åº¦: {args.representation_dim}")
    print(f"  â€¢ ç‰¹å¾ç½‘ç»œéšè—å±‚: {args.feature_hidden_dims}")
    print(f"  â€¢ æ¨æ–­ç½‘ç»œéšè—å±‚: {args.abduction_hidden_dims}")
    print(f"  â€¢ æ‰¹é‡å¤§å°: {args.batch_size}")
    print(f"  â€¢ è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"  â€¢ å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"  â€¢ æ—©åœè€å¿ƒå€¼: {args.early_stopping_patience}")
    print(f"  â€¢ æ•°æ®é›†: {args.datasets}")
    print()
    
    success = run_standard_robustness_test(
        noise_levels=args.noise_levels,
        representation_dim=args.representation_dim,
        feature_hidden_dims=args.feature_hidden_dims,
        abduction_hidden_dims=args.abduction_hidden_dims,
        batch_size=args.batch_size,
        epochs=args.epochs,
        learning_rate=args.learning_rate,
        early_stopping_patience=args.early_stopping_patience,
        datasets=args.datasets
    )
    
    if not success:
        print("\nğŸ’¡ å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥:")
        print("  1. æ˜¯å¦åœ¨æ­£ç¡®çš„condaç¯å¢ƒ (conda activate base)")
        print("  2. æ˜¯å¦å®‰è£…äº†æ‰€æœ‰ä¾èµ–åŒ…")
        print("  3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ (ç”¨äºä¸‹è½½æŸäº›æ•°æ®é›†)")
        print("  4. å‚æ•°è®¾ç½®æ˜¯å¦åˆç†")
        sys.exit(1)


if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°è¿è¡Œ
    if len(sys.argv) == 1:
        print("ğŸš€ ä½¿ç”¨é»˜è®¤å‚æ•°è¿è¡Œ...")
        success = run_standard_robustness_test()
        if not success:
            print("\nğŸ’¡ å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥:")
            print("  1. æ˜¯å¦åœ¨æ­£ç¡®çš„condaç¯å¢ƒ (conda activate base)")
            print("  2. æ˜¯å¦å®‰è£…äº†æ‰€æœ‰ä¾èµ–åŒ…")
            print("  3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ (ç”¨äºä¸‹è½½æŸäº›æ•°æ®é›†)")
            sys.exit(1)
    else:
        main() 