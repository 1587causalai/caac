#!/usr/bin/env python3
"""
CAACæ–¹æ³•é²æ£’æ€§å¯¹æ¯”å®éªŒè„šæœ¬

ä¸“é—¨æµ‹è¯•CAACæ–¹æ³•åœ¨å«æœ‰æ ‡ç­¾å™ªå£°(outliers)çš„åˆ†ç±»æ•°æ®ä¸Šçš„é²æ£’æ€§è¡¨ç°ã€‚

æµ‹è¯•è®¾ç½®ï¼š
- æ•°æ®åˆ†å‰²ï¼š70% train / 15% val / 15% test
- Train+Val å«æœ‰æ ‡ç­¾å™ªå£°ï¼ŒTestä¿æŒå¹²å‡€
- ä½¿ç”¨proportionalç­–ç•¥æ·»åŠ outliers
- ä¸è€ƒè™‘å¯å­¦ä¹ é˜ˆå€¼å’Œå”¯ä¸€æ€§çº¦æŸï¼Œä¸“æ³¨äºæ ¸å¿ƒæ–¹æ³•å¯¹æ¯”

æ ¸å¿ƒå¯¹æ¯”æ–¹æ³•ï¼ˆæ ¹æ®ç”¨æˆ·è¦æ±‚ç²¾é€‰ï¼‰ï¼š
1. CAAC OvR (Cauchy) - æŸ¯è¥¿åˆ†å¸ƒ + å›ºå®šé˜ˆå€¼ (æˆ‘ä»¬çš„æ ¸å¿ƒæ–¹æ³•)
3. CAAC OvR (Gaussian) - é«˜æ–¯åˆ†å¸ƒ + å›ºå®šé˜ˆå€¼  
5. MLP (Softmax) - æ ‡å‡†å¤šå±‚æ„ŸçŸ¥æœº
6. MLP (OvR Cross Entropy) - OvRç­–ç•¥
7. MLP (Crammer & Singer Hinge) - é“°é“¾æŸå¤±

è¿è¡Œæ–¹å¼ï¼š
python compare_methods_outlier_robustness.py
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris, load_wine, load_breast_cancer, load_digits
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import time
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®å¤„ç†å™¨
from src.models.caac_ovr_model import (
    CAACOvRModel, 
    SoftmaxMLPModel,
    OvRCrossEntropyMLPModel,
    CAACOvRGaussianModel,
    CrammerSingerMLPModel
)
from src.data.data_processor import DataProcessor

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
np.random.seed(42)

def load_datasets():
    """åŠ è½½æµ‹è¯•æ•°æ®é›† - æ‰©å±•åˆ°æ›´å¤šçœŸå®æ•°æ®é›†"""
    from sklearn.datasets import fetch_openml, make_classification
    import numpy as np
    datasets = {}
    
    print("ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®é›†...")
    
    # === ç»å…¸å°è§„æ¨¡æ•°æ®é›† (åŸºç¡€éªŒè¯) ===
    print("  åŠ è½½ç»å…¸æ•°æ®é›†...")
    
    # Irisæ•°æ®é›† - 3ç±»å¹³è¡¡
    iris = load_iris()
    datasets['iris'] = {
        'data': iris.data,
        'target': iris.target,
        'target_names': iris.target_names,
        'name': 'Iris (3-class, balanced)',
        'size': 'small'
    }
    
    # Wineæ•°æ®é›† - 3ç±»ç¨ä¸å¹³è¡¡
    wine = load_wine()
    datasets['wine'] = {
        'data': wine.data,
        'target': wine.target,
        'target_names': wine.target_names,
        'name': 'Wine (3-class, slight imbalance)',
        'size': 'small'
    }
    
    # Breast Canceræ•°æ®é›† - 2ç±»ä¸å¹³è¡¡
    bc = load_breast_cancer()
    datasets['breast_cancer'] = {
        'data': bc.data,
        'target': bc.target,
        'target_names': bc.target_names,
        'name': 'Breast Cancer (2-class, imbalanced)',
        'size': 'small'
    }
    
    # === ä¸­ç­‰è§„æ¨¡æ•°æ®é›† ===
    print("  åŠ è½½ä¸­ç­‰è§„æ¨¡æ•°æ®é›†...")
    
    try:
        # Digitsæ•°æ®é›† - 10ç±»ï¼Œ1797æ ·æœ¬ï¼Œ64ç‰¹å¾
        digits = load_digits()
        datasets['digits'] = {
            'data': digits.data,
            'target': digits.target,
            'target_names': digits.target_names,
            'name': 'Digits (10-class, balanced)',
            'size': 'medium'
        }
        print("    âœ… Digitsæ•°æ®é›†åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"    âŒ Digitsæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    
    try:
        # Fetch Covertypeæ•°æ®é›† - æ£®æ—è¦†ç›–ç±»å‹é¢„æµ‹ï¼Œ7ç±»ï¼Œ581kæ ·æœ¬ (é‡‡æ ·åˆ°10k)
        covertype = fetch_openml('covertype', version=3, parser='auto')
        # éšæœºé‡‡æ ·10000ä¸ªæ ·æœ¬ä»¥æé«˜å®éªŒé€Ÿåº¦
        np.random.seed(42)
        sample_indices = np.random.choice(len(covertype.data), 10000, replace=False)
        
        datasets['covertype'] = {
            'data': covertype.data.iloc[sample_indices].values,
            'target': covertype.target.iloc[sample_indices].values.astype(int) - 1,  # è½¬æ¢ä¸º0-based
            'target_names': [f'Type_{i}' for i in range(7)],
            'name': 'Forest Covertype (7-class, sampled 10k)',
            'size': 'medium'
        }
        print("    âœ… Forest Covertypeæ•°æ®é›†åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"    âŒ Forest Covertypeæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    
    try:
        # Letter Recognitionæ•°æ®é›† - 26ç±»å­—æ¯è¯†åˆ«ï¼Œ20000æ ·æœ¬
        letter = fetch_openml('letter', version=1, parser='auto')
        datasets['letter'] = {
            'data': letter.data.values,
            'target': letter.target.values,
            'target_names': [chr(ord('A') + i) for i in range(26)],
            'name': 'Letter Recognition (26-class, 20k samples)',
            'size': 'medium'
        }
        print("    âœ… Letter Recognitionæ•°æ®é›†åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"    âŒ Letter Recognitionæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    
    # === å¤§è§„æ¨¡æ•°æ®é›† ===
    print("  åŠ è½½å¤§è§„æ¨¡æ•°æ®é›†...")
    
    try:
        # Fashion-MNISTæ•°æ®é›† - 10ç±»æœè£…å›¾åƒï¼Œ70kæ ·æœ¬ (é‡‡æ ·åˆ°20k)
        fashion_mnist = fetch_openml('Fashion-MNIST', version=1, parser='auto')
        # éšæœºé‡‡æ ·20000ä¸ªæ ·æœ¬
        np.random.seed(42)
        sample_indices = np.random.choice(len(fashion_mnist.data), 20000, replace=False)
        
        datasets['fashion_mnist'] = {
            'data': fashion_mnist.data.iloc[sample_indices].values / 255.0,  # å½’ä¸€åŒ–åˆ°[0,1]
            'target': fashion_mnist.target.iloc[sample_indices].values.astype(int),
            'target_names': ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
                           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'],
            'name': 'Fashion-MNIST (10-class, sampled 20k)',
            'size': 'large'
        }
        print("    âœ… Fashion-MNISTæ•°æ®é›†åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"    âŒ Fashion-MNISTæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    
    try:
        # MNISTæ•°æ®é›† - 10ç±»æ‰‹å†™æ•°å­—ï¼Œ70kæ ·æœ¬ (é‡‡æ ·åˆ°15k)
        mnist = fetch_openml('mnist_784', version=1, parser='auto')
        # éšæœºé‡‡æ ·15000ä¸ªæ ·æœ¬
        np.random.seed(42)
        sample_indices = np.random.choice(len(mnist.data), 15000, replace=False)
        
        datasets['mnist'] = {
            'data': mnist.data.iloc[sample_indices].values / 255.0,  # å½’ä¸€åŒ–åˆ°[0,1]
            'target': mnist.target.iloc[sample_indices].values.astype(int),
            'target_names': [str(i) for i in range(10)],
            'name': 'MNIST (10-class, sampled 15k)',
            'size': 'large'
        }
        print("    âœ… MNISTæ•°æ®é›†åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"    âŒ MNISTæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    
    # === åˆæˆæ•°æ®é›† (ç”¨äºå‹åŠ›æµ‹è¯•) ===
    print("  ç”Ÿæˆåˆæˆæ•°æ®é›†...")
    
    try:
        # å¤šç±»ä¸å¹³è¡¡æ•°æ®é›†
        X_synthetic, y_synthetic = make_classification(
            n_samples=5000, n_features=20, n_informative=15, n_redundant=5,
            n_classes=5, n_clusters_per_class=1, weights=[0.4, 0.25, 0.15, 0.15, 0.05],
            random_state=42
        )
        datasets['synthetic_imbalanced'] = {
            'data': X_synthetic,
            'target': y_synthetic,
            'target_names': [f'Class_{i}' for i in range(5)],
            'name': 'Synthetic Imbalanced (5-class, 5k samples)',
            'size': 'medium'
        }
        print("    âœ… åˆæˆä¸å¹³è¡¡æ•°æ®é›†ç”ŸæˆæˆåŠŸ")
    except Exception as e:
        print(f"    âŒ åˆæˆæ•°æ®é›†ç”Ÿæˆå¤±è´¥: {e}")
    
    # === çœŸå®ä¸–ç•ŒæŒ‘æˆ˜æ•°æ®é›† ===
    print("  åŠ è½½çœŸå®ä¸–ç•ŒæŒ‘æˆ˜æ•°æ®é›†...")
    
    try:
        # Digitsæ•°æ®é›† - ç›´æ¥ä½¿ç”¨ä¸Šé¢å·²åŠ è½½çš„digitså˜é‡
        if 'digits' not in datasets:  # å¦‚æœä¸Šé¢æ²¡æœ‰æˆåŠŸåŠ è½½
            digits = load_digits()
            datasets['digits'] = {
                'data': digits.data,
                'target': digits.target,
                'target_names': digits.target_names,
                'name': 'Digits (10-class, balanced)',
                'size': 'medium'
            }
        print("    âœ… Digitsæ•°æ®é›†æ£€æŸ¥å®Œæˆ")
            
        # Optical Recognition of Handwritten Digitsæ•°æ®é›†
        optical_digits = load_digits()
        datasets['optical_digits'] = {
            'data': optical_digits.data,
            'target': optical_digits.target,
            'target_names': [str(i) for i in range(10)],
            'name': 'Optical Digits (10-class, 1.8k samples)',
            'size': 'small'
        }
        print("    âœ… Optical Digitsæ•°æ®é›†åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"    âŒ Optical Digitsæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    
    print(f"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±{len(datasets)}ä¸ªæ•°æ®é›†")
    
    # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
    print("-" * 80)
    for key, dataset in datasets.items():
        n_samples, n_features = dataset['data'].shape
        n_classes = len(np.unique(dataset['target']))
        size_label = dataset.get('size', 'unknown')
        print(f"  {dataset['name']:<40} | {n_samples:>6}æ ·æœ¬ | {n_features:>3}ç‰¹å¾ | {n_classes:>2}ç±» | {size_label}")
    
    return datasets

def create_robust_comparison_methods():
    """åˆ›å»ºç”¨äºé²æ£’æ€§æ¯”è¾ƒçš„æ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸è€ƒè™‘å¤æ‚å‚æ•°ï¼‰"""
    # åŸºç¡€ç½‘ç»œæ¶æ„å‚æ•°
    base_params = {
        'representation_dim': 128,
        'latent_dim': None,  # é»˜è®¤ç­‰äºrepresentation_dim
        'feature_hidden_dims': [64],
        'abduction_hidden_dims': [128, 64],
        'lr': 0.001,
        'batch_size': 64,  # å¢åŠ batch sizeä»¥å¤„ç†æ›´å¤§æ•°æ®é›†
        'epochs': 150,     # å¢åŠ epochsä»¥ç¡®ä¿å……åˆ†è®­ç»ƒ
        'device': None,
        'early_stopping_patience': 15,  # å¢åŠ patienceä»¥é€‚åº”å¤§æ•°æ®é›†
        'early_stopping_min_delta': 0.0001
    }
    
    # CAACæ¨¡å‹ä¸“ç”¨å‚æ•°ï¼ˆåŒ…å«é¢å¤–çš„é²æ£’æ€§å‚æ•°ï¼‰
    caac_params = {
        **base_params,
        'learnable_thresholds': False,
        'uniqueness_constraint': False
    }
    
    # æ ‡å‡†MLPæ¨¡å‹å‚æ•°ï¼ˆä¸åŒ…å«CAACç‰¹æœ‰å‚æ•°ï¼‰
    mlp_params = base_params.copy()
    
    methods = {
        # æ ¸å¿ƒæ–¹æ³•å¯¹æ¯” - æ ¹æ®ç”¨æˆ·è¦æ±‚é€‰æ‹©ç¬¬1ã€3ã€5ã€6ã€7ç§æ–¹æ³•
        'CAAC_Cauchy': {
            'name': 'CAAC OvR (Cauchy)',
            'type': 'unified',
            'model_class': CAACOvRModel,
            'params': caac_params,
            'description': 'ç¬¬1ç§ï¼šæŸ¯è¥¿åˆ†å¸ƒ + å›ºå®šé˜ˆå€¼ (æˆ‘ä»¬çš„æ ¸å¿ƒæ–¹æ³•)'
        },
        'CAAC_Gaussian': {
            'name': 'CAAC OvR (Gaussian)',
            'type': 'unified',
            'model_class': CAACOvRGaussianModel,
            'params': caac_params,
            'description': 'ç¬¬3ç§ï¼šé«˜æ–¯åˆ†å¸ƒ + å›ºå®šé˜ˆå€¼'
        },
        'MLP_Softmax': {
            'name': 'MLP (Softmax)',
            'type': 'unified',
            'model_class': SoftmaxMLPModel,
            'params': mlp_params,
            'description': 'ç¬¬5ç§ï¼šæ ‡å‡†å¤šå±‚æ„ŸçŸ¥æœº'
        },
        'MLP_OvR_CE': {
            'name': 'MLP (OvR Cross Entropy)',
            'type': 'unified',
            'model_class': OvRCrossEntropyMLPModel,
            'params': mlp_params,
            'description': 'ç¬¬6ç§ï¼šOvRç­–ç•¥'
        },
        'MLP_Hinge': {
            'name': 'MLP (Crammer & Singer Hinge)',
            'type': 'unified',
            'model_class': CrammerSingerMLPModel,
            'params': mlp_params,
            'description': 'ç¬¬7ç§ï¼šé“°é“¾æŸå¤±'
        }
    }
    return methods

def evaluate_method_with_outliers(method_info, X_train, X_val, X_test, y_train, y_val, y_test):
    """è¯„ä¼°å•ä¸ªæ–¹æ³•åœ¨å«outliersæ•°æ®ä¸Šçš„æ€§èƒ½"""
    start_time = time.time()
    
    if method_info['type'] == 'unified':
        # ä½¿ç”¨æˆ‘ä»¬çš„ç»Ÿä¸€ç½‘ç»œæ¶æ„
        input_dim = X_train.shape[1]
        n_classes = len(np.unique(np.concatenate([y_train, y_val, y_test])))
        
        model = method_info['model_class'](
            input_dim=input_dim, 
            n_classes=n_classes,
            **method_info['params']
        )
        
        # è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨å«outliersçš„trainå’Œvalæ•°æ®
        model.fit(X_train, y_train, X_val, y_val, verbose=0)
        
        # é¢„æµ‹ï¼šåœ¨å¹²å‡€çš„testæ•°æ®ä¸Šè¯„ä¼°
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)
        
    else:
        # ä½¿ç”¨sklearnæ¨¡å‹
        model = method_info['model']
        
        # åˆå¹¶trainå’Œvalæ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆsklearnæ¨¡å‹ä¸æ”¯æŒéªŒè¯é›†ï¼‰
        X_train_val_combined = np.vstack([X_train, X_val])
        y_train_val_combined = np.concatenate([y_train, y_val])
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train_val_combined, y_train_val_combined)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)
    
    training_time = time.time() - start_time
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)
    recall_macro = recall_score(y_test, y_pred, average='macro')
    f1_macro = f1_score(y_test, y_pred, average='macro')
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    
    return {
        'accuracy': accuracy,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'training_time': training_time
    }

def run_outlier_robustness_experiments(datasets=None):
    """è¿è¡Œoutlieré²æ£’æ€§å¯¹æ¯”å®éªŒ"""
    print("ğŸ”¬ å¼€å§‹è¿è¡Œæ ¸å¿ƒæ–¹æ³•outlieré²æ£’æ€§å¯¹æ¯”å®éªŒ")
    print("åŒ…å«æ–¹æ³•: CAAC(Cauchy), CAAC(Gaussian), MLP(Softmax), MLP(OvR), MLP(Hinge)")
    print("=" * 80)
    
    if datasets is None:
        datasets = load_datasets()
    
    methods = create_robust_comparison_methods()
    
    # æµ‹è¯•ä¸åŒçš„outlieræ¯”ä¾‹
    outlier_ratios = [0.0, 0.05, 0.10, 0.15, 0.20]
    
    results = []
    
    for dataset_name, dataset in datasets.items():
        print(f"\nğŸ“Š æ­£åœ¨æµ‹è¯•æ•°æ®é›†: {dataset['name']}")
        print("-" * 50)
        
        # æ•°æ®é¢„å¤„ç†
        X = dataset['data']
        y = dataset['target']
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # æ˜¾ç¤ºåŸå§‹ç±»åˆ«åˆ†å¸ƒ
        unique, counts = np.unique(y, return_counts=True)
        print(f"åŸå§‹ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique, counts))}")
        
        for outlier_ratio in outlier_ratios:
            print(f"\n  ğŸ¯ æµ‹è¯•outlieræ¯”ä¾‹: {outlier_ratio:.1%}")
            
            # ä½¿ç”¨æ–°çš„æ•°æ®åˆ†å‰²ç­–ç•¥
            if outlier_ratio > 0:
                result = DataProcessor.split_classification_data_with_outliers(
                    X_scaled, y,
                    train_size=0.7, val_size=0.15, test_size=0.15,
                    outlier_ratio=outlier_ratio, balance_strategy='proportional',
                    random_state=42
                )
                X_train, X_val, X_test, y_train, y_val, y_test, outlier_info = result
                
                print(f"    Outliersæ·»åŠ : Train={outlier_info['outliers_in_train']}, Val={outlier_info['outliers_in_val']}")
            else:
                # æ— outliersçš„åŸºçº¿
                X_train, X_val, X_test, y_train, y_val, y_test = DataProcessor.split_data(
                    X_scaled, y, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42
                )
                print(f"    åŸºçº¿å®éªŒ (æ— outliers)")
            
            # æµ‹è¯•æ¯ç§æ–¹æ³•
            for method_key, method_info in methods.items():
                print(f"    ğŸ§ª æµ‹è¯•æ–¹æ³•: {method_info['name']}")
                
                try:
                    metrics = evaluate_method_with_outliers(
                        method_info, 
                        X_train, X_val, X_test, 
                        y_train, y_val, y_test
                    )
                    
                    results.append({
                        'Dataset': dataset['name'],
                        'Dataset_Key': dataset_name,
                        'Outlier_Ratio': outlier_ratio,
                        'Method': method_info['name'],
                        'Method_Key': method_key,
                        'Method_Type': method_info['type'],
                        'Description': method_info.get('description', ''),
                        'Accuracy': metrics['accuracy'],
                        'Precision_Macro': metrics['precision_macro'],
                        'Recall_Macro': metrics['recall_macro'],
                        'F1_Macro': metrics['f1_macro'],
                        'F1_Weighted': metrics['f1_weighted'],
                        'Training_Time': metrics['training_time']
                    })
                    
                    print(f"      âœ… å‡†ç¡®ç‡: {metrics['accuracy']:.4f}, F1: {metrics['f1_macro']:.4f}")
                    
                except Exception as e:
                    print(f"      âŒ é”™è¯¯: {str(e)}")
                    continue
    
    return pd.DataFrame(results)

def create_robustness_visualizations(results_df):
    """åˆ›å»ºé²æ£’æ€§å¯è§†åŒ–å›¾è¡¨"""
    plt.style.use('default')
    
    # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºé²æ£’æ€§æ›²çº¿
    datasets = results_df['Dataset_Key'].unique()
    
    fig, axes = plt.subplots(1, len(datasets), figsize=(6*len(datasets), 5))
    if len(datasets) == 1:
        axes = [axes]
    
    # é¢œè‰²æ˜ å°„ - é’ˆå¯¹é€‰å®šçš„5ç§æ ¸å¿ƒæ–¹æ³•
    method_colors = {
        'CAAC_Cauchy': '#d62728',      # çº¢è‰² - æˆ‘ä»¬çš„ä¸»è¦æ–¹æ³•
        'CAAC_Gaussian': '#ff7f0e',    # æ©™è‰² - é«˜æ–¯ç‰ˆæœ¬
        'MLP_Softmax': '#2ca02c',      # ç»¿è‰² - æ ‡å‡†MLP
        'MLP_OvR_CE': '#1f77b4',       # è“è‰² - OvR MLP
        'MLP_Hinge': '#9467bd'         # ç´«è‰² - HingeæŸå¤±MLP
    }
    for i, dataset_key in enumerate(datasets):
        ax = axes[i]
        dataset_data = results_df[results_df['Dataset_Key'] == dataset_key]
        dataset_name = dataset_data['Dataset'].iloc[0]
        
        # ä¸ºæ¯ä¸ªæ–¹æ³•ç»˜åˆ¶é²æ£’æ€§æ›²çº¿
        for method_key in dataset_data['Method_Key'].unique():
            method_data = dataset_data[dataset_data['Method_Key'] == method_key]
            method_name = method_data['Method'].iloc[0]
            
            # æŒ‰outlieræ¯”ä¾‹æ’åº
            method_data_sorted = method_data.sort_values('Outlier_Ratio')
            
            color = method_colors.get(method_key, '#000000')
            linestyle = '--' if 'Logistic' in method_key or 'SVM' in method_key or 'Random_Forest' in method_key else '-'
            linewidth = 3 if 'CAAC' in method_key else 2
            
            ax.plot(method_data_sorted['Outlier_Ratio'] * 100, 
                   method_data_sorted['Accuracy'],
                   marker='o', linewidth=linewidth, linestyle=linestyle,
                   color=color, label=method_name, markersize=6)
        
        ax.set_xlabel('Outlier Ratio (%)', fontsize=12)
        ax.set_ylabel('Accuracy', fontsize=12)
        ax.set_title(f'{dataset_name}\nRobustness to Label Noise', fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=9, loc='lower left')
        ax.set_ylim(0.5, 1.05)
    
    plt.tight_layout()
    
    # ç¡®ä¿resultsç›®å½•å­˜åœ¨å¹¶ä¿å­˜æ–‡ä»¶
    import os
    results_dir = 'results'
    os.makedirs(results_dir, exist_ok=True)
    
    curves_file = os.path.join(results_dir, 'caac_outlier_robustness_curves.png')
    plt.savefig(curves_file, dpi=300, bbox_inches='tight')
    plt.close()  # å…³é—­å›¾ç‰‡è€Œä¸æ˜¾ç¤º
    print(f"ğŸ“ˆ é²æ£’æ€§æ›²çº¿å›¾å·²ä¿å­˜ä¸º: {curves_file}")

def create_robustness_heatmap(results_df):
    """åˆ›å»ºé²æ£’æ€§çƒ­åŠ›å›¾"""
    # è®¡ç®—ç›¸å¯¹äºæ— outliersåŸºçº¿çš„æ€§èƒ½è¡°å‡
    results_degradation = []
    
    for dataset_key in results_df['Dataset_Key'].unique():
        dataset_data = results_df[results_df['Dataset_Key'] == dataset_key]
        
        for method_key in dataset_data['Method_Key'].unique():
            method_data = dataset_data[dataset_data['Method_Key'] == method_key]
            
            # è·å–åŸºçº¿æ€§èƒ½ï¼ˆoutlier_ratio = 0.0ï¼‰
            baseline_acc = method_data[method_data['Outlier_Ratio'] == 0.0]['Accuracy'].iloc[0]
            
            for _, row in method_data.iterrows():
                if row['Outlier_Ratio'] > 0:
                    degradation = (baseline_acc - row['Accuracy']) / baseline_acc * 100
                    results_degradation.append({
                        'Dataset': row['Dataset_Key'],
                        'Method': row['Method_Key'], 
                        'Outlier_Ratio': f"{row['Outlier_Ratio']:.1%}",
                        'Performance_Degradation': degradation
                    })
    
    degradation_df = pd.DataFrame(results_degradation)
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    plt.figure(figsize=(12, 8))
    
    # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºå­çƒ­åŠ›å›¾
    datasets = degradation_df['Dataset'].unique()
    fig, axes = plt.subplots(1, len(datasets), figsize=(5*len(datasets), 6))
    if len(datasets) == 1:
        axes = [axes]
    
    for i, dataset in enumerate(datasets):
        dataset_data = degradation_df[degradation_df['Dataset'] == dataset]
        pivot_data = dataset_data.pivot(index='Method', columns='Outlier_Ratio', values='Performance_Degradation')
        
        method_order = ['CAAC_Cauchy', 'CAAC_Gaussian', 'MLP_Softmax', 'MLP_OvR_CE', 'MLP_Hinge']
        pivot_data = pivot_data.reindex([m for m in method_order if m in pivot_data.index])
        
        sns.heatmap(pivot_data, annot=True, cmap='Reds', fmt='.1f', 
                   cbar_kws={'label': 'Performance Degradation (%)'}, ax=axes[i])
        axes[i].set_title(f'{dataset.replace("_", " ").title()}\nPerformance Degradation', fontweight='bold')
        axes[i].set_xlabel('Outlier Ratio')
        axes[i].set_ylabel('Method')
    
    plt.tight_layout()
    
    # ç¡®ä¿resultsç›®å½•å­˜åœ¨å¹¶ä¿å­˜æ–‡ä»¶
    import os
    results_dir = 'results'
    os.makedirs(results_dir, exist_ok=True)
    
    heatmap_file = os.path.join(results_dir, 'caac_outlier_robustness_heatmap.png')
    plt.savefig(heatmap_file, dpi=300, bbox_inches='tight')
    plt.close()  # å…³é—­å›¾ç‰‡è€Œä¸æ˜¾ç¤º
    print(f"ğŸ“ˆ é²æ£’æ€§çƒ­åŠ›å›¾å·²ä¿å­˜ä¸º: {heatmap_file}")

def analyze_robustness_results(results_df):
    """åˆ†æé²æ£’æ€§å®éªŒç»“æœ"""
    print("\n" + "=" * 70)
    print("ğŸ” CAACæ–¹æ³•outlieré²æ£’æ€§åˆ†æ")
    print("=" * 70)
    
    # è®¡ç®—å¹³å‡é²æ£’æ€§å¾—åˆ†ï¼ˆåœ¨ä¸åŒoutlieræ¯”ä¾‹ä¸‹çš„å¹³å‡æ€§èƒ½ï¼‰
    robustness_scores = []
    
    for method_key in results_df['Method_Key'].unique():
        method_data = results_df[results_df['Method_Key'] == method_key]
        method_name = method_data['Method'].iloc[0]
        
        # è®¡ç®—åœ¨ä¸åŒoutlieræ¯”ä¾‹ä¸‹çš„å¹³å‡æ€§èƒ½
        avg_accuracy = method_data.groupby('Outlier_Ratio')['Accuracy'].mean()
        
        # è®¡ç®—é²æ£’æ€§åˆ†æ•°ï¼ˆæ€§èƒ½åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„å‡å€¼ï¼‰
        overall_robustness = avg_accuracy.mean()
        
        # è®¡ç®—æ€§èƒ½è¡°å‡ï¼ˆä»0%åˆ°20% outliersçš„è¡°å‡ç¨‹åº¦ï¼‰
        baseline_acc = avg_accuracy[0.0]
        worst_acc = avg_accuracy[0.2]
        performance_drop = (baseline_acc - worst_acc) / baseline_acc * 100
        
        robustness_scores.append({
            'Method': method_name,
            'Method_Key': method_key,
            'Baseline_Accuracy': baseline_acc,
            'Worst_Accuracy': worst_acc,
            'Performance_Drop': performance_drop,
            'Overall_Robustness': overall_robustness
        })
    
    robustness_df = pd.DataFrame(robustness_scores)
    robustness_df = robustness_df.sort_values('Overall_Robustness', ascending=False)
    
    print("\nğŸ“Š æ–¹æ³•é²æ£’æ€§æ’å (æŒ‰æ€»ä½“é²æ£’æ€§è¯„åˆ†):")
    print("-" * 50)
    for i, row in robustness_df.iterrows():
        print(f"{robustness_df.index.get_loc(i)+1:2d}. {row['Method']:<30} "
              f"é²æ£’æ€§: {row['Overall_Robustness']:.4f} "
              f"(è¡°å‡: {row['Performance_Drop']:.1f}%)")
    
    # ä¸“é—¨åˆ†æCAACæ–¹æ³•
    caac_methods = robustness_df[robustness_df['Method_Key'].str.contains('CAAC')]
    if len(caac_methods) > 0:
        print(f"\nğŸ¯ CAACæ–¹æ³•ä¸“é¡¹åˆ†æ:")
        print("-" * 30)
        for _, row in caac_methods.iterrows():
            print(f"â€¢ {row['Method']}: åŸºçº¿å‡†ç¡®ç‡ {row['Baseline_Accuracy']:.4f}, "
                  f"æœ€å·®å‡†ç¡®ç‡ {row['Worst_Accuracy']:.4f}, "
                  f"æ€§èƒ½è¡°å‡ {row['Performance_Drop']:.1f}%")
    
    return robustness_df

def generate_robustness_report(results_df, robustness_df):
    """ç”Ÿæˆè¯¦ç»†çš„é²æ£’æ€§å®éªŒæŠ¥å‘Š (å‚è€ƒcompare_methods.pyçš„ç»“æ„)"""
    from datetime import datetime
    import os
    
    print("\nğŸ“„ ç”Ÿæˆé²æ£’æ€§å®éªŒæŠ¥å‘Š")
    print("=" * 40)
    
    # ç¡®ä¿resultsç›®å½•å­˜åœ¨
    results_dir = 'results'
    os.makedirs(results_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = os.path.join(results_dir, f"caac_outlier_robustness_report_{timestamp}.md")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"""# CAACæ–¹æ³•Outlieré²æ£’æ€§å®éªŒæŠ¥å‘Š

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## å®éªŒæ¦‚è¿°

æœ¬æŠ¥å‘Šå±•ç¤ºäº†**CAACåˆ†ç±»æ–¹æ³•**åœ¨å«æœ‰æ ‡ç­¾å™ªå£°(outliers)çš„æ•°æ®ä¸Šçš„é²æ£’æ€§è¡¨ç°ã€‚å®éªŒé‡‡ç”¨åˆ›æ–°çš„æ•°æ®åˆ†å‰²ç­–ç•¥ï¼š**70% train / 15% val / 15% test**ï¼Œåœ¨train+valæ•°æ®ä¸­æ³¨å…¥ä¸åŒæ¯”ä¾‹çš„æ ‡ç­¾å™ªå£°ï¼Œä¿æŒtestæ•°æ®å¹²å‡€ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨çœŸå®å™ªå£°ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚

### æ ¸å¿ƒç ”ç©¶é—®é¢˜
**CAACæ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯ä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒçš„ç‰ˆæœ¬ï¼‰æ˜¯å¦åœ¨å«æœ‰æ ‡ç­¾å™ªå£°çš„æ•°æ®ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§ï¼Ÿ**

### å®éªŒåˆ›æ–°ç‚¹

1. **æ–°æ•°æ®åˆ†å‰²ç­–ç•¥**: 70/15/15åˆ†å‰²çªç ´ä¼ ç»Ÿ80/20é™åˆ¶ï¼Œæ›´å¥½åœ°æ¨¡æ‹ŸçœŸå®åœºæ™¯
2. **Proportionalæ ‡ç­¾å™ªå£°**: æŒ‰ç±»åˆ«æ¯”ä¾‹æ³¨å…¥å™ªå£°ï¼Œä¿æŒæ•°æ®ç»Ÿè®¡ç‰¹æ€§
3. **æ±¡æŸ“éªŒè¯é›†**: éªŒè¯é›†ä¹ŸåŒ…å«å™ªå£°ï¼Œæ¨¡æ‹ŸçœŸå®æ—©åœç¯å¢ƒ
4. **å¹²å‡€æµ‹è¯•é›†**: ä¿æŒæµ‹è¯•é›†æ— å™ªå£°ï¼Œç¡®ä¿è¯„ä¼°å…¬æ­£æ€§
5. **æ¸è¿›å¼å™ªå£°æµ‹è¯•**: 5ä¸ªå™ªå£°æ°´å¹³(0%-20%)æä¾›å®Œæ•´é²æ£’æ€§æ›²çº¿

### æµ‹è¯•çš„æ–¹æ³•æ¶æ„

#### æ ¸å¿ƒCAACæ–¹æ³• (ç ”ç©¶ç„¦ç‚¹)
1. **CAAC OvR (Cauchy)** - ä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒçš„å› æœè¡¨å¾å­¦ä¹ æ–¹æ³•
2. **CAAC OvR (Gaussian)** - ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒçš„å¯¹ç…§ç‰ˆæœ¬

#### ç¥ç»ç½‘ç»œåŸºçº¿æ–¹æ³•  
3. **MLP (Softmax)** - æ ‡å‡†å¤šå±‚æ„ŸçŸ¥æœºï¼ŒSoftmaxæŸå¤±
4. **MLP (OvR Cross Entropy)** - ä¸€å¯¹å…¶ä½™ç­–ç•¥çš„MLP
5. **MLP (Crammer & Singer Hinge)** - é“°é“¾æŸå¤±å¤šåˆ†ç±»æ–¹æ³•

#### ç»å…¸æœºå™¨å­¦ä¹ åŸºçº¿æ–¹æ³•
6. **Logistic Regression (Softmax)** - å¤šé¡¹å¼é€»è¾‘å›å½’
7. **Logistic Regression (OvR)** - ä¸€å¯¹å…¶ä½™é€»è¾‘å›å½’  
8. **SVM (RBF)** - å¾„å‘åŸºå‡½æ•°æ”¯æŒå‘é‡æœº
9. **Random Forest** - éšæœºæ£®æ—é›†æˆæ–¹æ³•

**ç½‘ç»œæ¶æ„ç»Ÿä¸€æ€§**: æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•é‡‡ç”¨ç›¸åŒæ¶æ„ç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼š
- **FeatureNet**: è¾“å…¥ç»´åº¦ â†’ 64ç»´ç‰¹å¾è¡¨å¾
- **AbductionNet**: 64ç»´ â†’ 64ç»´å› æœè¡¨å¾å‚æ•°
- **ActionNet**: 64ç»´ â†’ ç±»åˆ«æ•°é‡å¾—åˆ†

### å®éªŒæ–¹æ³•

#### æ•°æ®åˆ†å‰²ç­–ç•¥ (70/15/15)
```
åŸå§‹æ•°æ®é›†
    â†“
è®­ç»ƒé›† (70%) + éªŒè¯é›† (15%) â† æ³¨å…¥proportionalæ ‡ç­¾å™ªå£°
æµ‹è¯•é›† (15%) â† ä¿æŒå®Œå…¨å¹²å‡€
    â†“
è®­ç»ƒ: åœ¨æ±¡æŸ“çš„trainä¸Šè®­ç»ƒï¼Œåœ¨æ±¡æŸ“çš„valä¸Šæ—©åœ
è¯„ä¼°: åœ¨å¹²å‡€çš„testä¸Šæœ€ç»ˆè¯„ä¼°
```

#### æ ‡ç­¾å™ªå£°æ³¨å…¥ç­–ç•¥ (Proportional)
**æœ€realisticçš„proportionalç­–ç•¥ä¼˜åŠ¿**:
- é”™è¯¯æ ‡ç­¾æŒ‰åŸå§‹ç±»åˆ«åˆ†å¸ƒæ¯”ä¾‹åˆ†é…
- é¿å…éšæœºç­–ç•¥çš„ä¸ç°å®æ€§(å¦‚å°†æ‰€æœ‰é”™è¯¯éƒ½åˆ†é…ç»™æŸä¸€ç±»)
- ä¿æŒæ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§å’Œç±»åˆ«å¹³è¡¡
- æ›´æ¥è¿‘çœŸå®ä¸–ç•Œä¸­çš„æ ‡ç­¾é”™è¯¯æ¨¡å¼

**å™ªå£°æ¯”ä¾‹æµ‹è¯•**:
- 0% (åŸºçº¿): æ— å™ªå£°ï¼Œå»ºç«‹æ€§èƒ½åŸºå‡†
- 5% (è½»åº¦): æ¨¡æ‹Ÿé«˜è´¨é‡æ ‡æ³¨ä¸­çš„å°‘é‡é”™è¯¯
- 10% (ä¸­åº¦): æ¨¡æ‹Ÿä¸€èˆ¬è´¨é‡æ ‡æ³¨çš„é”™è¯¯ç‡
- 15% (é‡åº¦): æ¨¡æ‹Ÿä½è´¨é‡æ ‡æ³¨æˆ–å›°éš¾æ ·æœ¬æ ‡æ³¨
- 20% (æé‡): æ¨¡æ‹Ÿæå…·æŒ‘æˆ˜æ€§çš„å™ªå£°ç¯å¢ƒ

#### é²æ£’æ€§è¯„ä¼°æµç¨‹
1. **å™ªå£°æ³¨å…¥**: åœ¨train+valä¸­æŒ‰proportionalç­–ç•¥æ³¨å…¥æ ‡ç­¾å™ªå£°
2. **æ¨¡å‹è®­ç»ƒ**: åœ¨æ±¡æŸ“çš„è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹
3. **æ—©åœç­–ç•¥**: åŸºäºæ±¡æŸ“çš„éªŒè¯é›†è¡¨ç°è¿›è¡Œæ—©åœ(æ¨¡æ‹ŸçœŸå®åœºæ™¯)
4. **æœ€ç»ˆè¯„ä¼°**: åœ¨å¹²å‡€çš„æµ‹è¯•é›†ä¸Šè¯„ä¼°çœŸå®æ³›åŒ–æ€§èƒ½
5. **é²æ£’æ€§è®¡ç®—**: æ¯”è¾ƒä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„æ€§èƒ½è¡°å‡

### æµ‹è¯•æ•°æ®é›†

""")
        
        # æ·»åŠ æ•°æ®é›†è¯¦ç»†ä¿¡æ¯
        datasets_info = results_df.groupby('Dataset').agg({
            'Dataset_Key': 'first'
        }).reset_index()
        
        dataset_descriptions = {
            'Iris': '3-class, balanced',
            'Wine': '3-class, slight imbalance', 
            'Breast Cancer': '2-class, imbalanced',
            'Digits': '10-class, balanced',
            'Forest Covertype': '7-class, sampled 10k',
            'Letter Recognition': '26-class, 20k samples',
            'Fashion-MNIST': '10-class, sampled 20k',
            'MNIST': '10-class, sampled 15k',
            'Synthetic Imbalanced': '5-class, 5k samples',
            'Optical Digits': '10-class, 1.8k samples'
        }
        
        for _, row in datasets_info.iterrows():
            desc = dataset_descriptions.get(row['Dataset'], 'è¯¦ç»†ä¿¡æ¯å¾…è¡¥å……')
            f.write(f"- **{row['Dataset']}æ•°æ®é›†**: {desc}\n")
        
        f.write(f"""

## è¯¦ç»†å®éªŒç»“æœ

### é²æ£’æ€§æ€§èƒ½å¯¹æ¯”

""")
        
        # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºè¯¦ç»†çš„å¯¹æ¯”è¡¨
        for dataset in results_df['Dataset'].unique():
            dataset_data = results_df[results_df['Dataset'] == dataset]
            
            f.write(f"\n#### {dataset} æ•°æ®é›†é²æ£’æ€§è¡¨ç°\n\n")
            
            # åˆ›å»ºå‡†ç¡®ç‡å¯¹æ¯”è¡¨
            pivot_acc = dataset_data.pivot(index='Method', columns='Outlier_Ratio', values='Accuracy')
            # å°†åˆ—åä»æ¯”ä¾‹è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            pivot_acc.columns = [f'{col*100:.1f}%' for col in pivot_acc.columns]
            f.write("**å‡†ç¡®ç‡éšå™ªå£°æ¯”ä¾‹å˜åŒ–:**\n\n")
            f.write(pivot_acc.round(4).to_markdown())
            f.write("\n\n")
            
            # åˆ›å»ºF1åˆ†æ•°å¯¹æ¯”è¡¨
            pivot_f1 = dataset_data.pivot(index='Method', columns='Outlier_Ratio', values='F1_Macro')
            pivot_f1.columns = [f'{col*100:.1f}%' for col in pivot_f1.columns]
            f.write("**F1åˆ†æ•°éšå™ªå£°æ¯”ä¾‹å˜åŒ–:**\n\n")
            f.write(pivot_f1.round(4).to_markdown())
            f.write("\n\n")
        
        f.write(f"""## æ–¹æ³•é²æ£’æ€§ç»Ÿè®¡

### æ•´ä½“é²æ£’æ€§æ’å (ç»¼åˆæ‰€æœ‰æ•°æ®é›†)

""")
        
        # é²æ£’æ€§æ’åè¡¨
        f.write(robustness_df.round(4).to_markdown(index=False))
        
        f.write(f"""

### é²æ£’æ€§æ’ååˆ†æ

""")
        
        # æŒ‰é²æ£’æ€§å¾—åˆ†æ’åºåˆ†æ
        for i, (_, row) in enumerate(robustness_df.iterrows(), 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            f.write(f"{emoji} **{row['Method']}**:\n")
            f.write(f"   - æ€»ä½“é²æ£’æ€§å¾—åˆ†: {row['Overall_Robustness']:.4f}\n")
            f.write(f"   - åŸºçº¿å‡†ç¡®ç‡: {row['Baseline_Accuracy']:.4f}\n")
            f.write(f"   - æœ€å·®å‡†ç¡®ç‡: {row['Worst_Accuracy']:.4f}\n")
            f.write(f"   - æ€§èƒ½è¡°å‡: {row['Performance_Drop']:.1f}%\n\n")
        
        # CAACæ–¹æ³•ä¸“é¡¹åˆ†æ
        caac_methods = robustness_df[robustness_df['Method_Key'].str.contains('CAAC')]
        if len(caac_methods) > 0:
            f.write(f"""### CAACæ–¹æ³•ä¸“é¡¹é²æ£’æ€§åˆ†æ

#### æŸ¯è¥¿åˆ†å¸ƒ vs é«˜æ–¯åˆ†å¸ƒé²æ£’æ€§å¯¹æ¯”

""")
            
            caac_cauchy = caac_methods[caac_methods['Method_Key'].str.contains('Cauchy')]
            caac_gaussian = caac_methods[caac_methods['Method_Key'].str.contains('Gaussian')]
            
            if len(caac_cauchy) > 0:
                cauchy_row = caac_cauchy.iloc[0]
                cauchy_rank = robustness_df.index[robustness_df['Method_Key'] == cauchy_row['Method_Key']].tolist()[0] + 1
                f.write(f"**æŸ¯è¥¿åˆ†å¸ƒCAACæ–¹æ³•è¡¨ç°:**\n")
                f.write(f"- æ’å: ç¬¬{cauchy_rank}å\n")
                f.write(f"- é²æ£’æ€§å¾—åˆ†: {cauchy_row['Overall_Robustness']:.4f}\n")
                f.write(f"- åŸºçº¿å‡†ç¡®ç‡: {cauchy_row['Baseline_Accuracy']:.4f}\n")
                f.write(f"- æœ€å·®å‡†ç¡®ç‡: {cauchy_row['Worst_Accuracy']:.4f}\n")
                f.write(f"- æ€§èƒ½è¡°å‡: {cauchy_row['Performance_Drop']:.1f}%\n\n")
            
            if len(caac_gaussian) > 0:
                gaussian_row = caac_gaussian.iloc[0]
                gaussian_rank = robustness_df.index[robustness_df['Method_Key'] == gaussian_row['Method_Key']].tolist()[0] + 1
                f.write(f"**é«˜æ–¯åˆ†å¸ƒCAACæ–¹æ³•è¡¨ç°:**\n")
                f.write(f"- æ’å: ç¬¬{gaussian_rank}å\n")
                f.write(f"- é²æ£’æ€§å¾—åˆ†: {gaussian_row['Overall_Robustness']:.4f}\n")
                f.write(f"- åŸºçº¿å‡†ç¡®ç‡: {gaussian_row['Baseline_Accuracy']:.4f}\n")
                f.write(f"- æœ€å·®å‡†ç¡®ç‡: {gaussian_row['Worst_Accuracy']:.4f}\n")
                f.write(f"- æ€§èƒ½è¡°å‡: {gaussian_row['Performance_Drop']:.1f}%\n\n")
            
            # å¯¹æ¯”åˆ†æ
            if len(caac_cauchy) > 0 and len(caac_gaussian) > 0:
                cauchy_robust = cauchy_row['Overall_Robustness']
                gaussian_robust = gaussian_row['Overall_Robustness']
                cauchy_drop = cauchy_row['Performance_Drop']
                gaussian_drop = gaussian_row['Performance_Drop']
                
                winner = "æŸ¯è¥¿åˆ†å¸ƒ" if cauchy_robust > gaussian_robust else "é«˜æ–¯åˆ†å¸ƒ"
                robust_diff = abs(cauchy_robust - gaussian_robust)
                drop_diff = abs(cauchy_drop - gaussian_drop)
                
                f.write(f"""#### åˆ†å¸ƒé€‰æ‹©å½±å“åˆ†æ

**é²æ£’æ€§å¯¹æ¯”ç»“æœ:**
- æ›´é²æ£’çš„åˆ†å¸ƒ: **{winner}**
- é²æ£’æ€§å¾—åˆ†å·®å¼‚: {robust_diff:.4f}
- æ€§èƒ½è¡°å‡å·®å¼‚: {drop_diff:.1f}%

**åˆ†å¸ƒé€‰æ‹©å»ºè®®:**
""")
                if cauchy_robust > gaussian_robust:
                    f.write(f"âœ… **æ¨èä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒ**, åœ¨æ ‡ç­¾å™ªå£°ç¯å¢ƒä¸‹è¡¨ç°æ›´ç¨³å®š\n")
                else:
                    f.write(f"âš ï¸ **é«˜æ–¯åˆ†å¸ƒåœ¨æ­¤å®éªŒä¸­è¡¨ç°æ›´å¥½**, éœ€è¦è¿›ä¸€æ­¥åˆ†æåŸå› \n")
                f.write(f"\n")
        
        # ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”
        f.write(f"""### ä¸åŸºçº¿æ–¹æ³•é²æ£’æ€§å¯¹æ¯”

#### CAAC vs ç¥ç»ç½‘ç»œåŸºçº¿

""")
        
        # åˆ†æç¥ç»ç½‘ç»œæ–¹æ³•å¯¹æ¯”
        nn_methods = results_df[results_df['Method'].str.contains('CAAC|MLP')]
        if not nn_methods.empty:
            nn_summary = nn_methods.groupby('Method').agg({
                'Accuracy': 'mean',
                'F1_Macro': 'mean'
            }).round(4)
            
            f.write("**ç¥ç»ç½‘ç»œæ–¹æ³•å¹³å‡æ€§èƒ½å¯¹æ¯”:**\n\n")
            f.write(nn_summary.to_markdown())
            f.write("\n\n")
        
        # æ ¸å¿ƒå‘ç°å’Œç»“è®º
        most_robust = robustness_df.iloc[0]
        least_degraded = robustness_df.loc[robustness_df['Performance_Drop'].idxmin()]
        
        f.write(f"""## æ ¸å¿ƒå‘ç°ï¼šæ ‡ç­¾å™ªå£°ç¯å¢ƒä¸‹çš„æ–¹æ³•é²æ£’æ€§

### ä¸»è¦å‘ç°

1. **æœ€é²æ£’æ–¹æ³•**: {most_robust['Method']} 
   - æ€»ä½“é²æ£’æ€§å¾—åˆ†: {most_robust['Overall_Robustness']:.4f}
   - åœ¨æ‰€æœ‰å™ªå£°æ°´å¹³ä¸‹ä¿æŒæœ€ç¨³å®šçš„æ€§èƒ½

2. **æ€§èƒ½è¡°å‡æœ€å°**: {least_degraded['Method']}
   - ä»0%åˆ°20%å™ªå£°ä»…è¡°å‡: {least_degraded['Performance_Drop']:.1f}%
   - å±•ç°å‡ºæœ€å¼ºçš„æŠ—å™ªå£°èƒ½åŠ›

3. **CAACæ–¹æ³•è¡¨ç°**: 
""")
        
        # åˆ†æCAACæ–¹æ³•çš„æ•´ä½“è¡¨ç°
        caac_ranks = []
        for _, caac_method in caac_methods.iterrows():
            rank = robustness_df.index[robustness_df['Method_Key'] == caac_method['Method_Key']].tolist()[0] + 1
            caac_ranks.append(rank)
        
        if caac_ranks:
            avg_caac_rank = sum(caac_ranks) / len(caac_ranks)
            f.write(f"   - CAACæ–¹æ³•å¹³å‡æ’å: {avg_caac_rank:.1f}\n")
            f.write(f"   - åœ¨{len(robustness_df)}ä¸ªæ–¹æ³•ä¸­å¤„äº{'å‰åˆ—' if avg_caac_rank <= 3 else 'ä¸­ç­‰' if avg_caac_rank <= 6 else 'ååˆ—'}ä½ç½®\n")
        
        f.write(f"""

### æ•°æ®åˆ†å‰²ç­–ç•¥éªŒè¯

#### 70/15/15åˆ†å‰²ç­–ç•¥çš„ä¼˜åŠ¿éªŒè¯

1. **æ›´çœŸå®çš„éªŒè¯**: éªŒè¯é›†åŒ…å«å™ªå£°ï¼ŒçœŸå®æ¨¡æ‹Ÿå®é™…éƒ¨ç½²ä¸­çš„æ—©åœåœºæ™¯
2. **å…¬æ­£çš„æµ‹è¯•**: æµ‹è¯•é›†ä¿æŒå¹²å‡€ï¼Œç¡®ä¿æœ€ç»ˆè¯„ä¼°çš„å…¬æ­£æ€§  
3. **å……è¶³çš„è®­ç»ƒæ•°æ®**: 70%è®­ç»ƒæ•°æ®ä¸ºæ¨¡å‹æä¾›å……åˆ†çš„å­¦ä¹ æœºä¼š
4. **åˆç†çš„éªŒè¯è§„æ¨¡**: 15%éªŒè¯æ•°æ®è¶³ä»¥è¿›è¡Œå¯é çš„æ—©åœåˆ¤æ–­

#### Proportionalå™ªå£°ç­–ç•¥çš„æœ‰æ•ˆæ€§

1. **ç»Ÿè®¡ç‰¹æ€§ä¿æŒ**: ä¸ç ´ååŸå§‹æ•°æ®çš„ç±»åˆ«åˆ†å¸ƒç‰¹å¾
2. **çœŸå®åœºæ™¯æ¨¡æ‹Ÿ**: æ›´è´´è¿‘å®é™…æ ‡æ³¨é”™è¯¯çš„åˆ†å¸ƒæ¨¡å¼
3. **å¯æ§å™ªå£°å¼ºåº¦**: æ¸è¿›å¼å™ªå£°æ¯”ä¾‹æä¾›å®Œæ•´çš„é²æ£’æ€§è¯„ä¼°
4. **æ–¹æ³•å…¬å¹³æ¯”è¾ƒ**: ç¡®ä¿æ‰€æœ‰æ–¹æ³•é¢ä¸´ç›¸åŒçš„å™ªå£°æŒ‘æˆ˜

### åˆ†å¸ƒé€‰æ‹©çš„å½±å“åˆ†æ

""")
        
        if len(caac_cauchy) > 0 and len(caac_gaussian) > 0:
            f.write(f"""åŸºäºæŸ¯è¥¿åˆ†å¸ƒä¸é«˜æ–¯åˆ†å¸ƒCAACæ–¹æ³•çš„å¯¹æ¯”ï¼š

1. **ç†è®ºå‡è®¾éªŒè¯**: {"âœ… å®éªŒæ”¯æŒ" if cauchy_robust > gaussian_robust else "âŒ å®éªŒä¸æ”¯æŒ"}æŸ¯è¥¿åˆ†å¸ƒåœ¨é²æ£’æ€§æ–¹é¢çš„ç†è®ºä¼˜åŠ¿
2. **å®é™…åº”ç”¨å»ºè®®**: {"æ¨èæŸ¯è¥¿åˆ†å¸ƒ" if cauchy_robust > gaussian_robust else "æ¨èé«˜æ–¯åˆ†å¸ƒ"}ç”¨äºå®é™…çš„å™ªå£°æ•æ„Ÿåº”ç”¨
3. **è¿›ä¸€æ­¥ç ”ç©¶æ–¹å‘**: {"æ¢ç´¢æŸ¯è¥¿åˆ†å¸ƒçš„ä¼˜åŠ¿æœºåˆ¶" if cauchy_robust > gaussian_robust else "åˆ†æé«˜æ–¯åˆ†å¸ƒè¡¨ç°æ›´å¥½çš„åŸå› "}

""")
        
        f.write(f"""## å®éªŒç»“è®º

### é²æ£’æ€§ç»“è®º

1. **æ–¹æ³•é²æ£’æ€§æ’åº**: 
   - å† å†›: {robustness_df.iloc[0]['Method']} (é²æ£’æ€§å¾—åˆ†: {robustness_df.iloc[0]['Overall_Robustness']:.4f})
   - äºšå†›: {robustness_df.iloc[1]['Method']} (é²æ£’æ€§å¾—åˆ†: {robustness_df.iloc[1]['Overall_Robustness']:.4f})
   - å­£å†›: {robustness_df.iloc[2]['Method']} (é²æ£’æ€§å¾—åˆ†: {robustness_df.iloc[2]['Overall_Robustness']:.4f})

2. **CAACæ–¹æ³•ç‰¹ç‚¹**:
   - åœ¨æ ‡ç­¾å™ªå£°ç¯å¢ƒä¸‹å±•ç°å‡ºç‹¬ç‰¹çš„è¡Œä¸ºæ¨¡å¼
   - åˆ†å¸ƒé€‰æ‹©å¯¹é²æ£’æ€§æœ‰æ˜¾è‘—å½±å“
   - é€‚ç”¨äºéœ€è¦ä¸ç¡®å®šæ€§é‡åŒ–å’Œé²æ£’æ€§çš„åœºæ™¯

3. **ä¼ ç»Ÿæ–¹æ³•è¡¨ç°**:
   - ç»å…¸æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å™ªå£°ç¯å¢ƒä¸‹è¡¨ç°ç¨³å®š
   - æŸäº›ä¼ ç»Ÿæ–¹æ³•åœ¨ç‰¹å®šåœºæ™¯ä¸‹ä¼˜äºæ·±åº¦å­¦ä¹ æ–¹æ³•
   - è®¡ç®—æ•ˆç‡ä¸é²æ£’æ€§å­˜åœ¨æƒè¡¡å…³ç³»

### æ–¹æ³•é€‰æ‹©å»ºè®®

**æ¨èä½¿ç”¨CAACæ–¹æ³•çš„åœºæ™¯**:
- éœ€è¦ä¸ç¡®å®šæ€§é‡åŒ–çš„é«˜é£é™©å†³ç­–åœºæ™¯
- æ ‡ç­¾è´¨é‡ä¸ç¡®å®šæˆ–å­˜åœ¨ç³»ç»Ÿæ€§æ ‡æ³¨é”™è¯¯çš„æ•°æ®é›†
- éœ€è¦ç†è§£æ¨¡å‹ç½®ä¿¡åº¦å’Œå†³ç­–è¾¹ç•Œçš„ç ”ç©¶åº”ç”¨
- åŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ç­‰å®¹é”™æ€§è¦æ±‚é«˜çš„é¢†åŸŸ

**æ¨èä½¿ç”¨ä¼ ç»Ÿé²æ£’æ–¹æ³•çš„åœºæ™¯**:
- è¿½æ±‚æœ€é«˜é²æ£’æ€§çš„å…³é”®ä»»åŠ¡åº”ç”¨
- è®¡ç®—èµ„æºå—é™ä½†éœ€è¦é²æ£’æ€§çš„è¾¹ç¼˜éƒ¨ç½²
- å¿«é€ŸåŸå‹å¼€å‘å’Œbaselineå»ºç«‹
- å¯¹è§£é‡Šæ€§è¦æ±‚å¾ˆé«˜çš„ä¸šåŠ¡åœºæ™¯

### å®éªŒæ–¹æ³•è®ºä»·å€¼

**æ•°æ®åˆ†å‰²ç­–ç•¥åˆ›æ–°**:
- 70/15/15åˆ†å‰²ä¸ºæ ‡ç­¾å™ªå£°ç ”ç©¶æä¾›äº†æ–°çš„å®éªŒèŒƒå¼
- éªŒè¯é›†åŒ…å«å™ªå£°çš„è®¾è®¡æ›´æ¥è¿‘çœŸå®éƒ¨ç½²åœºæ™¯
- ä¸ºå…¶ä»–é²æ£’æ€§ç ”ç©¶æä¾›äº†å¯å¤ç”¨çš„å®éªŒæ¡†æ¶

**å™ªå£°æ³¨å…¥ç­–ç•¥ä¼˜åŒ–**:
- Proportionalç­–ç•¥ç›¸æ¯”éšæœºç­–ç•¥æ›´å…·ç°å®æ„ä¹‰
- æ¸è¿›å¼å™ªå£°æ¯”ä¾‹è®¾è®¡æä¾›äº†å®Œæ•´çš„é²æ£’æ€§æ›²çº¿
- ä¸ºæ ‡ç­¾å™ªå£°ç ”ç©¶å»ºç«‹äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®

### æœªæ¥ç ”ç©¶æ–¹å‘

**çŸ­æœŸæ”¹è¿›æ–¹å‘**:
1. æ‰©å±•åˆ°æ›´å¤šæ•°æ®é›†å’Œä»»åŠ¡ç±»å‹éªŒè¯æ–¹æ³•æ™®é€‚æ€§
2. æ¢ç´¢è‡ªé€‚åº”å™ªå£°æ£€æµ‹å’Œçº æ­£æœºåˆ¶
3. ç ”ç©¶ä¸åŒç±»å‹æ ‡ç­¾å™ªå£°(å¯¹ç§°vséå¯¹ç§°)çš„å½±å“

**é•¿æœŸç ”ç©¶æ–¹å‘**:
1. å¼€å‘è‡ªé€‚åº”åˆ†å¸ƒé€‰æ‹©æœºåˆ¶ï¼Œæ ¹æ®æ•°æ®ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åˆ†å¸ƒ
2. é›†æˆå¤šç§é²æ£’æ€§ç­–ç•¥çš„æ··åˆæ–¹æ³•
3. åœ¨çº¿å­¦ä¹ ç¯å¢ƒä¸‹çš„å®æ—¶é²æ£’æ€§é€‚åº”
4. å¤§è§„æ¨¡æ•°æ®é›†å’ŒçœŸå®æ ‡æ³¨å™ªå£°ç¯å¢ƒä¸‹çš„éªŒè¯

**ç†è®ºå‘å±•æ–¹å‘**:
1. æ·±å…¥åˆ†ææŸ¯è¥¿åˆ†å¸ƒvsé«˜æ–¯åˆ†å¸ƒåœ¨å™ªå£°ç¯å¢ƒä¸‹çš„ç†è®ºä¼˜åŠ¿
2. å»ºç«‹æ ‡ç­¾å™ªå£°ç¯å¢ƒä¸‹çš„æ³›åŒ–ç†è®º
3. å¼€å‘é’ˆå¯¹ä¸åŒå™ªå£°æ¨¡å¼çš„æœ€ä¼˜ç­–ç•¥é€‰æ‹©ç†è®º

## å¯è§†åŒ–ç»“æœ

å®éªŒç”Ÿæˆçš„é²æ£’æ€§åˆ†æå›¾è¡¨å…¨é¢å±•ç¤ºäº†æ–¹æ³•æ€§èƒ½ï¼š

### é²æ£’æ€§æ›²çº¿å›¾
- **æ–‡ä»¶**: `caac_outlier_robustness_curves.png`
- **å†…å®¹**: å±•ç¤ºä¸åŒæ–¹æ³•åœ¨å„å™ªå£°æ°´å¹³ä¸‹çš„æ€§èƒ½å˜åŒ–è¶‹åŠ¿
- **ç”¨é€”**: ç›´è§‚æ¯”è¾ƒæ–¹æ³•çš„é²æ£’æ€§ä¸‹é™æ¨¡å¼

### é²æ£’æ€§çƒ­åŠ›å›¾
- **æ–‡ä»¶**: `caac_outlier_robustness_heatmap.png`  
- **å†…å®¹**: æ–¹æ³•Ã—å™ªå£°æ°´å¹³çš„æ€§èƒ½çŸ©é˜µå¯è§†åŒ–
- **ç”¨é€”**: å¿«é€Ÿè¯†åˆ«æœ€é²æ£’çš„æ–¹æ³•å’Œæœ€å…·æŒ‘æˆ˜æ€§çš„å™ªå£°æ°´å¹³

## æ•°æ®æ–‡ä»¶

### è¯¦ç»†å®éªŒæ•°æ®
- **è¯¦ç»†ç»“æœ**: `caac_outlier_robustness_detailed_{timestamp}.csv`
  - åŒ…å«æ¯ä¸ªæ–¹æ³•åœ¨æ¯ä¸ªæ•°æ®é›†å’Œå™ªå£°æ°´å¹³ä¸‹çš„å®Œæ•´æ€§èƒ½æŒ‡æ ‡
  - å¯ç”¨äºè¿›ä¸€æ­¥çš„ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–

- **é²æ£’æ€§æ±‡æ€»**: `caac_outlier_robustness_summary_{timestamp}.csv`
  - åŒ…å«æ¯ä¸ªæ–¹æ³•çš„é²æ£’æ€§å¾—åˆ†ã€åŸºçº¿æ€§èƒ½ã€æœ€å·®æ€§èƒ½å’Œæ€§èƒ½è¡°å‡
  - é€‚ç”¨äºå¿«é€Ÿæ–¹æ³•æ¯”è¾ƒå’Œæ’ååˆ†æ

### å¯è§†åŒ–æ–‡ä»¶
- **é²æ£’æ€§æ›²çº¿å›¾**: `caac_outlier_robustness_curves.png`
- **é²æ£’æ€§çƒ­åŠ›å›¾**: `caac_outlier_robustness_heatmap.png`

---

**å®éªŒé…ç½®è¯¦ç»†ä¿¡æ¯**:
- **Pythonç¯å¢ƒ**: base condaç¯å¢ƒ (ç¡®ä¿ä¾èµ–ä¸€è‡´æ€§)
- **æ•°æ®åˆ†å‰²**: 70%è®­ç»ƒ / 15%éªŒè¯ / 15%æµ‹è¯• (åˆ›æ–°çš„é²æ£’æ€§æµ‹è¯•åˆ†å‰²)
- **å™ªå£°ç­–ç•¥**: Proportionalæ ‡ç­¾å™ªå£° (0%, 5%, 10%, 15%, 20%)
- **ç‰¹å¾æ ‡å‡†åŒ–**: StandardScaler (ç¡®ä¿ç‰¹å¾å°ºåº¦ä¸€è‡´æ€§)
- **æ—©åœç­–ç•¥**: patience=10, åœ¨æ±¡æŸ“éªŒè¯é›†ä¸Šæ‰§è¡Œ (æ¨¡æ‹ŸçœŸå®éƒ¨ç½²)
- **æœ€ç»ˆè¯„ä¼°**: åœ¨å¹²å‡€æµ‹è¯•é›†ä¸Šè¿›è¡Œ (ç¡®ä¿è¯„ä¼°å…¬æ­£æ€§)
- **éšæœºç§å­**: 42 (ç¡®ä¿ç»“æœå®Œå…¨å¯é‡å¤)
- **ç½‘ç»œæ¶æ„**: ç»Ÿä¸€æ¶æ„ç¡®ä¿ç¥ç»ç½‘ç»œæ–¹æ³•çš„å…¬å¹³æ¯”è¾ƒ

**å®éªŒè´¨é‡ä¿è¯**:
- æ‰€æœ‰å®éªŒä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
- ç¥ç»ç½‘ç»œæ–¹æ³•é‡‡ç”¨ç»Ÿä¸€æ¶æ„ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
- å¤šæ¬¡è¿è¡ŒéªŒè¯ç»“æœç¨³å®šæ€§
- è¯¦ç»†è®°å½•æ‰€æœ‰è¶…å‚æ•°è®¾ç½®

*æœ¬æŠ¥å‘Šç”±CAACé²æ£’æ€§å®éªŒè„šæœ¬åŸºäº{len(results_df)}æ¡è¯¦ç»†å®éªŒè®°å½•è‡ªåŠ¨ç”Ÿæˆäº {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
""")
    
    print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # ä¿å­˜åŸå§‹ç»“æœ
    detailed_file = os.path.join(results_dir, f"caac_outlier_robustness_detailed_{timestamp}.csv")
    summary_file = os.path.join(results_dir, f"caac_outlier_robustness_summary_{timestamp}.csv")
    
    results_df.to_csv(detailed_file, index=False)
    robustness_df.to_csv(summary_file, index=False)
    print(f"ğŸ“Š è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {detailed_file}")
    print(f"ğŸ“Š æ±‡æ€»æ•°æ®å·²ä¿å­˜åˆ°: {summary_file}")
    
    return report_file

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª CAACæ–¹æ³•Outlieré²æ£’æ€§å¯¹æ¯”å®éªŒ")
    print("ä¸“æ³¨äºæµ‹è¯•CAACæ–¹æ³•åœ¨æ ‡ç­¾å™ªå£°ä¸‹çš„è¡¨ç°")
    print("ä½¿ç”¨70/15/15æ•°æ®åˆ†å‰² + proportionalæ ‡ç­¾å™ªå£°ç­–ç•¥")
    print("=" * 70)
    
    # åŠ è½½æ‰€æœ‰å¯ç”¨æ•°æ®é›†ï¼ˆè¿™ä¼šæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼‰
    all_datasets = load_datasets()
    
    print("\nğŸ“ æ•°æ®é›†é€‰æ‹©é€‰é¡¹:")
    print("1. å¿«é€Ÿæµ‹è¯• (åªç”¨ç»å…¸å°æ•°æ®é›†: Iris, Wine, Breast Cancer)")
    print("2. æ ‡å‡†æµ‹è¯• (å°+ä¸­ç­‰æ•°æ®é›†: åŒ…å«Digits, Covertypeç­‰)")
    print("3. å®Œæ•´æµ‹è¯• (æ‰€æœ‰æ•°æ®é›†: åŒ…å«MNIST, Fashion-MNISTç­‰å¤§æ•°æ®é›†)")
    print("4. è‡ªå®šä¹‰é€‰æ‹©")
    
    while True:
        try:
            choice = input("\nè¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼ (1-4): ").strip()
            
            if choice == '1':
                # å¿«é€Ÿæµ‹è¯• - åªç”¨ç»å…¸å°æ•°æ®é›†
                selected_datasets = ['iris', 'wine', 'breast_cancer']
                print("âœ… é€‰æ‹©å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
                break
            elif choice == '2':
                # æ ‡å‡†æµ‹è¯• - å°+ä¸­ç­‰æ•°æ®é›†
                selected_datasets = ['iris', 'wine', 'breast_cancer', 'digits', 'optical_digits', 'synthetic_imbalanced']
                print("âœ… é€‰æ‹©æ ‡å‡†æµ‹è¯•æ¨¡å¼")
                break
            elif choice == '3':
                # å®Œæ•´æµ‹è¯• - æ‰€æœ‰æ•°æ®é›†
                selected_datasets = list(all_datasets.keys())
                print("âœ… é€‰æ‹©å®Œæ•´æµ‹è¯•æ¨¡å¼ (è¿™ä¼šèŠ±è´¹è¾ƒé•¿æ—¶é—´)")
                break
            elif choice == '4':
                # è‡ªå®šä¹‰é€‰æ‹©
                print("\nå¯ç”¨æ•°æ®é›†:")
                for i, (key, dataset) in enumerate(all_datasets.items(), 1):
                    print(f"  {i}. {key} - {dataset['name']}")
                
                indices_input = input("è¯·è¾“å…¥è¦æµ‹è¯•çš„æ•°æ®é›†ç¼–å·ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼Œå¦‚ '1 2 3'ï¼‰: ").strip()
                try:
                    indices = [int(x) for x in indices_input.split()]
                    dataset_keys = list(all_datasets.keys())
                    selected_datasets = [dataset_keys[i-1] for i in indices if 1 <= i <= len(dataset_keys)]
                    if selected_datasets:
                        print(f"âœ… é€‰æ‹©äº†{len(selected_datasets)}ä¸ªæ•°æ®é›†: {[all_datasets[k]['name'] for k in selected_datasets]}")
                        break
                    else:
                        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
                except ValueError:
                    print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·é‡è¯•")
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-4")
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå®éªŒå–æ¶ˆ")
            return
        except EOFError:
            print("\n\nğŸ‘‹ è¾“å…¥ç»“æŸï¼Œå®éªŒå–æ¶ˆ")
            return
    
    # è¿‡æ»¤é€‰æ‹©çš„æ•°æ®é›†
    filtered_datasets = {k: v for k, v in all_datasets.items() if k in selected_datasets}
    
    print(f"\nğŸ¯ å°†åœ¨ä»¥ä¸‹{len(filtered_datasets)}ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒ:")
    for key, dataset in filtered_datasets.items():
        n_samples, n_features = dataset['data'].shape
        n_classes = len(np.unique(dataset['target']))
        print(f"  â€¢ {dataset['name']}: {n_samples}æ ·æœ¬, {n_features}ç‰¹å¾, {n_classes}ç±»")
    
    confirm = input(f"\nç¡®è®¤å¼€å§‹å®éªŒï¼Ÿ(y/N): ").strip().lower()
    if confirm not in ['y', 'yes']:
        print("ğŸ‘‹ å®éªŒå·²å–æ¶ˆ")
        return
    
    # è¿è¡Œå®éªŒ
    results_df = run_outlier_robustness_experiments(filtered_datasets)
    
    # åˆ›å»ºå¯è§†åŒ–
    create_robustness_visualizations(results_df)
    create_robustness_heatmap(results_df)
    
    # åˆ†æç»“æœ
    robustness_df = analyze_robustness_results(results_df)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_robustness_report(results_df, robustness_df)
    
    print(f"\n" + "=" * 70)
    print("ğŸ‰ CAAC outlieré²æ£’æ€§å®éªŒå®Œæˆï¼")
    print("âœ… å·²ç”Ÿæˆé²æ£’æ€§æ›²çº¿å›¾å’Œçƒ­åŠ›å›¾")
    print("âœ… å·²ç”Ÿæˆè¯¦ç»†çš„å®éªŒæŠ¥å‘Š")
    print("âœ… æ•°æ®åˆ†å‰²ç­–ç•¥å·²ä¼˜åŒ–ä¸º70/15/15")
    print("âœ… ä½¿ç”¨proportionalç­–ç•¥æ·»åŠ æ ‡ç­¾å™ªå£°")
    print("=" * 70)

if __name__ == '__main__':
    main() 