# ç»Ÿä¸€æ¶æ„åˆ†ç±»æ–¹æ³•ä»£ç è®¾è®¡æ–‡æ¡£

**æ–‡æ¡£ç‰ˆæœ¬**: v1.1  
**åˆ›å»ºæ—¥æœŸ**: 2024å¹´  
**æ›´æ–°æ—¥æœŸ**: 2024å¹´ï¼ˆæ–°å¢å¯å­¦ä¹ é˜ˆå€¼å˜ä½“ï¼‰  
**å¯¹åº”ä»£ç **: `src/models/caac_ovr_model.py`  
**æ•°å­¦åŸç†**: `docs/unified_methods_mathematical_principles.md`

## 1. æ•´ä½“æ¶æ„æ¦‚è§ˆ

åŸºäºå…±äº«æ½œåœ¨è¡¨å¾çš„ç»Ÿä¸€æ¶æ„è®¾è®¡ï¼Œæ”¯æŒ**ä¸ƒç§**ä¸åŒçš„åˆ†ç±»æ–¹æ³•ã€‚æ‰€æœ‰æ–¹æ³•é‡‡ç”¨ç›¸åŒçš„ç½‘ç»œç»“æ„ï¼Œä»…åœ¨æŸå¤±å‡½æ•°ã€æ¦‚ç‡å»ºæ¨¡ç­–ç•¥å’Œé˜ˆå€¼å‚æ•°è®¾å®šä¸Šæœ‰æ‰€å·®å¼‚ï¼Œç¡®ä¿å…¬å¹³çš„ç®—æ³•æ¯”è¾ƒã€‚

### 1.0 æ”¯æŒçš„æ–¹æ³•åˆ—è¡¨

**CAACç³»åˆ—æ–¹æ³•ï¼ˆ4ç§ï¼‰**ï¼š
1. `CAACOvRModel(learnable_thresholds=False)` - æŸ¯è¥¿åˆ†å¸ƒ + å›ºå®šé˜ˆå€¼
2. `CAACOvRModel(learnable_thresholds=True)` - æŸ¯è¥¿åˆ†å¸ƒ + å¯å­¦ä¹ é˜ˆå€¼
3. `CAACOvRGaussianModel(learnable_thresholds=False)` - é«˜æ–¯åˆ†å¸ƒ + å›ºå®šé˜ˆå€¼  
4. `CAACOvRGaussianModel(learnable_thresholds=True)` - é«˜æ–¯åˆ†å¸ƒ + å¯å­¦ä¹ é˜ˆå€¼

**MLPç³»åˆ—æ–¹æ³•ï¼ˆ3ç§ï¼‰**ï¼š
5. `SoftmaxMLPModel` - æ ‡å‡†Softmaxå¤šå±‚æ„ŸçŸ¥æœº
6. `OvRCrossEntropyMLPModel` - OvRäº¤å‰ç†µå¤šå±‚æ„ŸçŸ¥æœº
7. `CrammerSingerMLPModel` - Crammer & Singeré“°é“¾æŸå¤±

### 1.0.1 å”¯ä¸€æ€§çº¦æŸæ‰©å±•åŠŸèƒ½

**æ–°å¢åŠŸèƒ½**: CAACç³»åˆ—æ–¹æ³•æ”¯æŒå¯é€‰çš„**æ½œåœ¨å‘é‡é‡‡æ ·å”¯ä¸€æ€§çº¦æŸ**

**âš ï¸ å®éªŒå‘ç°**: å”¯ä¸€æ€§çº¦æŸåœ¨å®é™…åº”ç”¨ä¸­å€¾å‘äºé™ä½åˆ†ç±»å‡†ç¡®ç‡ï¼Œä¸»è¦ç”¨ä½œ**ç†è®ºå¯¹ç…§ç ”ç©¶**ã€‚

**åŠŸèƒ½å‚æ•°**:
- `uniqueness_constraint`: å¸ƒå°”å€¼ï¼Œæ˜¯å¦å¯ç”¨å”¯ä¸€æ€§çº¦æŸ (é»˜è®¤ `False`)
- `uniqueness_samples`: æ•´æ•°ï¼Œæ¯ä¸ªæ ·æœ¬çš„é‡‡æ ·æ¬¡æ•° (å»ºè®® `3`)
- `uniqueness_weight`: æµ®ç‚¹æ•°ï¼Œçº¦æŸæŸå¤±æƒé‡ (å»ºè®® `0.05`)

**å®ç°åŸç†**:
- å¯¹æ¯ä¸ªæ ·æœ¬ä»å…¶æ½œåœ¨åˆ†å¸ƒä¸­é‡‡æ ·å¤šä¸ªå®ä¾‹åŒ–å‘é‡
- ä½¿ç”¨æœ€å¤§-æ¬¡å¤§é—´éš”çº¦æŸç¡®ä¿æ¯ä¸ªé‡‡æ ·å®ä¾‹çš„å†³ç­–å”¯ä¸€æ€§
- é‡‡æ ·æ¬¡æ•°å’Œæƒé‡å…±åŒæ§åˆ¶çº¦æŸå¼ºåº¦

**æ¨èé…ç½®**:
```python
# ç†è®ºå¯¹ç…§ç ”ç©¶é…ç½®
model = CAACOvRModel(
    uniqueness_constraint=True,
    uniqueness_samples=3,        # æœ€å°é‡‡æ ·æ¬¡æ•°
    uniqueness_weight=0.05       # è¾ƒä½æƒé‡ï¼Œå‡å°‘å¯¹å‡†ç¡®ç‡çš„å½±å“
)
```

**ä½¿ç”¨å»ºè®®**:
- **ä¸»è¦ç”¨é€”**: ç†è®ºç ”ç©¶å’Œæ–¹æ³•è®ºæ¯”è¾ƒ
- **ä¸æ¨è**: è¿½æ±‚æœ€é«˜å‡†ç¡®ç‡çš„ç”Ÿäº§ç¯å¢ƒ
- **é€‚ç”¨**: éœ€è¦ä¸¥æ ¼å†³ç­–ä¸€è‡´æ€§çš„ç‰¹æ®Šåœºæ™¯

### 1.1 æ¶æ„è®¾è®¡åŸåˆ™

- **ç»Ÿä¸€æ€§**: æ‰€æœ‰æ–¹æ³•å…±äº«ç›¸åŒçš„ç½‘ç»œæ¶æ„
- **æ¨¡å—åŒ–**: æ¸…æ™°çš„æ¨¡å—è¾¹ç•Œï¼Œä¾¿äºæ‰©å±•å’Œç»´æŠ¤
- **å¯é…ç½®æ€§**: æ”¯æŒçµæ´»çš„å‚æ•°é…ç½®
- **å¯æ‰©å±•æ€§**: ä¸ºæœªæ¥æ”¹è¿›é¢„ç•™æ¥å£
- **æ¦‚å¿µå¯¹é½**: `d_latent = d_repr`ï¼Œä½“ç°å› æœè¡¨å¾æ¦‚å¿µ

### 1.2 æ ¸å¿ƒæ¨¡å—ç»„æˆ

```
è¾“å…¥ç‰¹å¾ x âˆˆ â„á´°
    â†“
FeatureNetwork (ç‰¹å¾æå–)
    â†“  
ç¡®å®šæ€§ç‰¹å¾è¡¨å¾ z âˆˆ â„áµˆÊ³áµ‰áµ–Ê³
    â†“
AbductionNetwork (æ¨ç†ç½‘ç»œ)
    â†“
å› æœè¡¨å¾éšæœºå˜é‡å‚æ•° (Î¼, Ïƒ) âˆˆ â„áµˆË¡áµƒáµ—áµ‰â¿áµ— Ã— â„áµˆË¡áµƒáµ—áµ‰â¿áµ—
    â†“
ActionNetwork (è¡ŒåŠ¨ç½‘ç»œ)
    â†“
ç±»åˆ«å¾—åˆ†/æ¦‚ç‡ âˆˆ â„á´º
    â†“
é¢„æµ‹ç±»åˆ« = argmax
```

**å…³é”®çº¦å®š**: `d_latent = d_repr`ï¼ˆå› æœè¡¨å¾ç»´åº¦ = ç‰¹å¾è¡¨å¾ç»´åº¦ï¼‰

---

## 2. æ ¸å¿ƒç½‘ç»œæ¨¡å—è®¾è®¡

### 2.1 FeatureNetwork - ç‰¹å¾æå–ç½‘ç»œ

**åŠŸèƒ½**: å°†åŸå§‹è¾“å…¥æ˜ å°„ä¸ºç¡®å®šæ€§é«˜ç»´ç‰¹å¾è¡¨å¾

```python
class FeatureNetwork(nn.Module):
    """
    ç‰¹å¾ç½‘ç»œ - ä¸å›å½’æ¨¡å‹å®Œå…¨ä¸€è‡´
    è¾“å…¥: x âˆˆ â„á´° (åŸå§‹ç‰¹å¾)
    è¾“å‡º: z âˆˆ â„áµˆÊ³áµ‰áµ–Ê³ (ç¡®å®šæ€§ç‰¹å¾è¡¨å¾)
    """
    def __init__(self, input_dim, representation_dim, hidden_dims=[64]):
        super(FeatureNetwork, self).__init__()
        layers = []
        prev_dim = input_dim
        for hidden_dim_i in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim_i))
            layers.append(nn.ReLU())
            prev_dim = hidden_dim_i
        layers.append(nn.Linear(prev_dim, representation_dim))
        self.network = nn.Sequential(*layers)
        
        # å­˜å‚¨æ¶æ„ä¿¡æ¯
        self.input_dim = input_dim
        self.representation_dim = representation_dim
        self.hidden_dims = hidden_dims 

    def forward(self, x):
        return self.network(x)
```

**è®¾è®¡ç‰¹ç‚¹**:
- å¤šå±‚æ„ŸçŸ¥æœºç»“æ„ï¼Œæ”¯æŒå¯é…ç½®éšè—å±‚
- ReLUæ¿€æ´»å‡½æ•°ï¼Œç¡®ä¿éçº¿æ€§å˜æ¢
- è¾“å‡ºç¡®å®šæ€§ç‰¹å¾è¡¨å¾å‘é‡

### 2.2 AbductionNetwork - æ¨ç†ç½‘ç»œ  

**åŠŸèƒ½**: ä»ç¡®å®šæ€§ç‰¹å¾è¡¨å¾æ¨ç†å› æœè¡¨å¾éšæœºå˜é‡çš„åˆ†å¸ƒå‚æ•°

```python
class AbductionNetwork(nn.Module):
    """
    ç»Ÿä¸€æ¨æ–­ç½‘ç»œ - è¾“å‡ºå› æœè¡¨å¾éšæœºå˜é‡çš„ä½ç½®å’Œå°ºåº¦å‚æ•°
    è¾“å…¥: z âˆˆ â„áµˆÊ³áµ‰áµ–Ê³ (ç¡®å®šæ€§ç‰¹å¾è¡¨å¾)
    è¾“å‡º: Î¼(z) âˆˆ â„áµˆË¡áµƒáµ—áµ‰â¿áµ—, Ïƒ(z) âˆˆ â„áµˆË¡áµƒáµ—áµ‰â¿áµ— (éšæœºå˜é‡å‚æ•°)
    """
    def __init__(self, representation_dim, latent_dim, hidden_dims=[64, 32]):
        super(AbductionNetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾æå–å±‚
        shared_layers_list = []
        prev_dim = representation_dim
        for hidden_dim_i in hidden_dims:
            shared_layers_list.append(nn.Linear(prev_dim, hidden_dim_i))
            shared_layers_list.append(nn.ReLU())
            prev_dim = hidden_dim_i
        shared_output_dim = prev_dim 
        
        # ä½ç½®å’Œå°ºåº¦å‚æ•°çš„ç‹¬ç«‹å¤´éƒ¨
        self.location_head = nn.Linear(shared_output_dim, latent_dim)
        self.scale_head = nn.Linear(shared_output_dim, latent_dim)
        self.shared_mlp = nn.Sequential(*shared_layers_list)
        
        # å­˜å‚¨æ¶æ„ä¿¡æ¯
        self.representation_dim = representation_dim
        self.latent_dim = latent_dim
        self.hidden_dims = hidden_dims
            
    def forward(self, representation):
        shared_features = self.shared_mlp(representation)
        location_param = self.location_head(shared_features)
        scale_param = F.softplus(self.scale_head(shared_features))  # ç¡®ä¿æ­£æ€§
        return location_param, scale_param
```

**è®¾è®¡ç‰¹ç‚¹**:
- å…±äº«MLPä¸»å¹² + åŒå¤´è®¾è®¡ï¼ˆä½ç½®/å°ºåº¦ï¼‰
- Softplusæ¿€æ´»ç¡®ä¿å°ºåº¦å‚æ•°æ­£æ€§
- æ”¯æŒæ¦‚å¿µå¯¹é½ï¼š`d_latent = d_repr`

### 2.3 ActionNetwork - è¡ŒåŠ¨ç½‘ç»œ

**åŠŸèƒ½**: å°†å› æœè¡¨å¾éšæœºå˜é‡çº¿æ€§å˜æ¢ä¸ºç±»åˆ«å¾—åˆ†éšæœºå˜é‡

```python
class ActionNetwork(nn.Module):
    """
    è¡ŒåŠ¨ç½‘ç»œ - å¤„ç†å› æœè¡¨å¾éšæœºå˜é‡ï¼Œè¾“å‡ºç±»åˆ«å¾—åˆ†åˆ†å¸ƒå‚æ•°
    æ¦‚å¿µ: è¾“å…¥éšæœºå˜é‡ U_jï¼Œé€šè¿‡çº¿æ€§å˜æ¢è¾“å‡ºç±»åˆ«å¾—åˆ†éšæœºå˜é‡ S_k çš„å‚æ•°
    """
    def __init__(self, latent_dim, n_classes):
        super(ActionNetwork, self).__init__()
        self.linear = nn.Linear(latent_dim, n_classes)
        self.latent_dim = latent_dim
        self.n_classes = n_classes
    
    def forward(self, location_param):
        # æ³¨ï¼šè¾“å…¥location_paramæ˜¯ä¸ºäº†å…¼å®¹ç°æœ‰æ¶æ„
        # æ¦‚å¿µä¸Šåº”è¯¥å¤„ç†éšæœºå˜é‡ï¼Œå®é™…é€šè¿‡æƒé‡çŸ©é˜µåœ¨æŸå¤±å‡½æ•°ä¸­è®¡ç®—åˆ†å¸ƒå‚æ•°
        return self.linear(location_param)
    
    def get_weights(self):
        """è·å–çº¿æ€§å˜æ¢å‚æ•°"""
        weight = self.linear.weight.data  # [n_classes, latent_dim] - çº¿æ€§å˜æ¢çŸ©é˜µA
        bias = self.linear.bias.data      # [n_classes] - åç½®B
        return weight, bias
    
    def compute_class_distribution_params(self, location_param, scale_param, distribution_type='cauchy'):
        """
        è®¡ç®—æ¯ä¸ªç±»åˆ«Scoreéšæœºå˜é‡çš„åˆ†å¸ƒå‚æ•°
        ä¸åŒåˆ†å¸ƒç±»å‹ä½¿ç”¨ä¸åŒçš„çº¿æ€§ç»„åˆè§„åˆ™
        
        Args:
            location_param: å› æœè¡¨å¾ä½ç½®å‚æ•° Î¼(z) âˆˆ â„áµˆË¡áµƒáµ—áµ‰â¿áµ—
            scale_param: å› æœè¡¨å¾å°ºåº¦å‚æ•° Ïƒ(z) âˆˆ â„áµˆË¡áµƒáµ—áµ‰â¿áµ—  
            distribution_type: åˆ†å¸ƒç±»å‹ ('cauchy' | 'gaussian')
            
        Returns:
            class_locations: ç±»åˆ«å¾—åˆ†ä½ç½®å‚æ•° âˆˆ â„á´º
            class_scales: ç±»åˆ«å¾—åˆ†å°ºåº¦å‚æ•° âˆˆ â„á´º
        """
        W, b = self.get_weights()
        batch_size = location_param.size(0)
        
        # ä½ç½®å‚æ•°ï¼šloc(S_k) = W_k @ Î¼(z) + b_k (æ‰€æœ‰åˆ†å¸ƒç›¸åŒ)
        class_locations = torch.matmul(location_param, W.T) + b.unsqueeze(0)
        
        if distribution_type == 'cauchy':
            # æŸ¯è¥¿åˆ†å¸ƒï¼šscale(S_k) = |W_k| @ Ïƒ(z)
            W_abs = torch.abs(W)
            class_scales = torch.matmul(scale_param, W_abs.T)
            return class_locations, torch.clamp(class_scales, min=1e-6)
            
        elif distribution_type == 'gaussian':
            # é«˜æ–¯åˆ†å¸ƒï¼švar(S_k) = W_k^2 @ Ïƒ(z)^2, std(S_k) = sqrt(var)
            W_squared = W ** 2
            class_variances = torch.matmul(scale_param ** 2, W_squared.T)
            class_stds = torch.sqrt(torch.clamp(class_variances, min=1e-6))
            return class_locations, class_stds
            
        else:
            raise ValueError(f"Unsupported distribution type: {distribution_type}")

    def compute_scores_from_samples(self, samples):
        """
        ç›´æ¥ä»é‡‡æ ·çš„æ½œåœ¨å‘é‡è®¡ç®—å¾—åˆ† (å”¯ä¸€æ€§çº¦æŸä¸“ç”¨)
        
        Args:
            samples: [batch_size, n_samples, latent_dim] - é‡‡æ ·çš„æ½œåœ¨å‘é‡å®ä¾‹
        
        Returns:
            scores: [batch_size, n_samples, n_classes] - æ¯ä¸ªé‡‡æ ·çš„ç¡®å®šæ€§å¾—åˆ†
        """
        W, b = self.get_weights()
        # æ‰¹é‡çŸ©é˜µè¿ç®—ï¼šsamples @ W.T + b
        scores = torch.matmul(samples, W.T) + b.unsqueeze(0).unsqueeze(0)
        return scores
```

**è®¾è®¡ç‰¹ç‚¹**:
- ç®€å•çº¿æ€§å˜æ¢å±‚ï¼Œä½“ç°çº¿æ€§ç»„åˆæ¦‚å¿µ
- æ”¯æŒä¸åŒåˆ†å¸ƒç±»å‹çš„å‚æ•°è®¡ç®—
- æ•°å€¼ç¨³å®šæ€§ä¿éšœï¼ˆclampæ“ä½œï¼‰

### 2.4 UnifiedClassificationNetwork - ç»Ÿä¸€åˆ†ç±»ç½‘ç»œ

**åŠŸèƒ½**: æ•´åˆä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼Œæ„æˆå®Œæ•´çš„åˆ†ç±»ç½‘ç»œ

```python
class UnifiedClassificationNetwork(nn.Module):
    """
    ç»Ÿä¸€åˆ†ç±»ç½‘ç»œ - æ•´åˆ FeatureNetwork â†’ AbductionNetwork â†’ ActionNetwork
    æ‰€æœ‰åˆ†ç±»æ–¹æ³•å…±äº«æ­¤æ¶æ„ï¼Œä»…æŸå¤±å‡½æ•°ä¸åŒ
    """
    def __init__(self, input_dim, representation_dim, latent_dim, n_classes,
                 feature_hidden_dims, abduction_hidden_dims):
        super(UnifiedClassificationNetwork, self).__init__()
        
        # ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—
        self.feature_net = FeatureNetwork(input_dim, representation_dim, feature_hidden_dims)
        self.abduction_net = AbductionNetwork(representation_dim, latent_dim, abduction_hidden_dims)
        self.action_net = ActionNetwork(latent_dim, n_classes)
        
        # å­˜å‚¨æ¶æ„ä¿¡æ¯
        self.input_dim = input_dim
        self.representation_dim = representation_dim
        self.latent_dim = latent_dim
        self.n_classes = n_classes
        self.feature_hidden_dims = feature_hidden_dims
        self.abduction_hidden_dims = abduction_hidden_dims

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Returns:
            logits: ç”¨äºä¼ ç»ŸMLPæ–¹æ³•çš„logits
            location_param: å› æœè¡¨å¾ä½ç½®å‚æ•°ï¼Œç”¨äºCAACæ–¹æ³•
            scale_param: å› æœè¡¨å¾å°ºåº¦å‚æ•°ï¼Œç”¨äºCAACæ–¹æ³•
        """
        representation = self.feature_net(x)
        location_param, scale_param = self.abduction_net(representation)
        logits = self.action_net(location_param)
        return logits, location_param, scale_param

    def predict_proba(self, x):
        """é¢„æµ‹æ¦‚ç‡ï¼ˆç”¨äºä¼ ç»ŸMLPæ–¹æ³•ï¼‰"""
        logits, _, _ = self.forward(x)
        return F.softmax(logits, dim=1)
```

**è®¾è®¡ç‰¹ç‚¹**:
- æ¨¡å—åŒ–ç»„è£…ï¼Œæ¸…æ™°çš„æ•°æ®æµ
- åŒæ—¶è¾“å‡ºlogitså’Œåˆ†å¸ƒå‚æ•°ï¼Œæ”¯æŒä¸åŒæ–¹æ³•
- å…¼å®¹ä¼ ç»Ÿsoftmaxé¢„æµ‹æ¥å£

---

## 3. ä¸ƒç§åˆ†ç±»æ–¹æ³•å®ç°

### 3.1 æ–¹æ³•å®ç°æ¨¡å¼

æ‰€æœ‰æ–¹æ³•é‡‡ç”¨ç›¸åŒçš„è®¾è®¡æ¨¡å¼ï¼š

```python
class MethodModel:
    """åˆ†ç±»æ–¹æ³•åŸºç¡€æ¨¡å¼"""
    
    def __init__(self, input_dim, representation_dim=64, latent_dim=None, ...):
        # æ¦‚å¿µå¯¹é½ï¼šd_latent = d_repr
        self.latent_dim = latent_dim if latent_dim is not None else representation_dim
        self._setup_model_optimizer()
    
    def _setup_model_optimizer(self):
        """è®¾ç½®ç»Ÿä¸€ç½‘ç»œå’Œä¼˜åŒ–å™¨"""
        self.model = UnifiedClassificationNetwork(...)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)
    
    def compute_loss(self, y_true, logits, location_param, scale_param):
        """æ ¸å¿ƒå·®å¼‚ï¼šä¸åŒçš„æŸå¤±å‡½æ•°å®ç°"""
        # å„æ–¹æ³•çš„å…·ä½“å®ç°ä¸åŒ
        pass
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, verbose=1):
        """ç»Ÿä¸€çš„è®­ç»ƒæµç¨‹"""
        # ç›¸åŒçš„è®­ç»ƒå¾ªç¯é€»è¾‘
        pass
```

### 3.2 CAAC OvR (æŸ¯è¥¿åˆ†å¸ƒ) - CAACOvRModel

**æ”¯æŒå˜ä½“**: å›ºå®šé˜ˆå€¼ç‰ˆæœ¬ & å¯å­¦ä¹ é˜ˆå€¼ç‰ˆæœ¬  
**ç‰¹è‰²**: ä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒCDFè®¡ç®—ç±»åˆ«æ¦‚ç‡

```python
class CAACOvRModel:
    def __init__(self, input_dim, n_classes, learnable_thresholds=False, ...):
        """
        Args:
            learnable_thresholds: æ˜¯å¦ä½¿ç”¨å¯å­¦ä¹ é˜ˆå€¼å‚æ•°
                - False: é˜ˆå€¼å›ºå®šä¸º0 (ä¼ ç»Ÿç‰ˆæœ¬)
                - True: é˜ˆå€¼ä¸ºå¯å­¦ä¹ å‚æ•° (æ–°å¢ç‰ˆæœ¬)
        """
        super().__init__()
        # ... ç½‘ç»œåˆå§‹åŒ– ...
        
        # é˜ˆå€¼å‚æ•°è®¾ç½®
        if learnable_thresholds:
            self.thresholds = nn.Parameter(torch.zeros(n_classes))
        else:
            self.register_buffer('thresholds', torch.zeros(n_classes))

def compute_loss(self, y_true, logits, location_param, scale_param):
    """
    CAACæŸ¯è¥¿æŸå¤±å‡½æ•°ï¼šä½“ç°å› æœè¡¨å¾éšæœºå˜é‡åˆ°Scoreéšæœºå˜é‡çš„åˆ†å¸ƒå˜æ¢
    æ ¸å¿ƒæ­¥éª¤ï¼š
    1. é€šè¿‡ActionNetworkè®¡ç®—æ¯ä¸ªç±»åˆ«Scoreçš„æŸ¯è¥¿åˆ†å¸ƒå‚æ•°
    2. ç”¨æŸ¯è¥¿CDFè®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ P_k  
    3. ç”¨P_kè®¡ç®—OvRäºŒå…ƒäº¤å‰ç†µæŸå¤±
    4. æ”¯æŒå›ºå®š/å¯å­¦ä¹ é˜ˆå€¼ä¸¤ç§æ¨¡å¼
    """
    batch_size = y_true.size(0)
    n_classes = self.n_classes
    device = y_true.device
    
    # è®¡ç®—ç±»åˆ«å¾—åˆ†çš„æŸ¯è¥¿åˆ†å¸ƒå‚æ•°
    class_locations, class_scales = self.model.action_net.compute_class_distribution_params(
        location_param, scale_param, distribution_type='cauchy'
    )
    
    # ä½¿ç”¨é˜ˆå€¼å‚æ•°ï¼ˆè‡ªåŠ¨æ”¯æŒå›ºå®š/å¯å­¦ä¹ ï¼‰
    thresholds = self.thresholds
    
    # æŸ¯è¥¿åˆ†å¸ƒCDFè®¡ç®—ç±»åˆ«æ¦‚ç‡
    pi = torch.tensor(np.pi, device=device)
    normalized_thresholds = (thresholds.unsqueeze(0) - class_locations) / class_scales
    P_k = 0.5 - (1/pi) * torch.atan(normalized_thresholds)
    P_k = torch.clamp(P_k, min=1e-7, max=1-1e-7)  # æ•°å€¼ç¨³å®šæ€§
    
    # OvRäºŒå…ƒäº¤å‰ç†µæŸå¤±
    y_binary = torch.zeros(batch_size, n_classes, device=device)
    y_binary.scatter_(1, y_true.unsqueeze(1), 1)
    
    bce_loss = -(y_binary * torch.log(P_k) + (1 - y_binary) * torch.log(1 - P_k))
    return torch.mean(bce_loss)
```

### 3.3 CAAC OvR (é«˜æ–¯åˆ†å¸ƒ) - CAACOvRGaussianModel

**æ”¯æŒå˜ä½“**: å›ºå®šé˜ˆå€¼ç‰ˆæœ¬ & å¯å­¦ä¹ é˜ˆå€¼ç‰ˆæœ¬  
**ç‰¹è‰²**: ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒCDFè®¡ç®—ç±»åˆ«æ¦‚ç‡

```python
class CAACOvRGaussianModel:
    def __init__(self, input_dim, n_classes, learnable_thresholds=False, ...):
        """
        Args:
            learnable_thresholds: æ˜¯å¦ä½¿ç”¨å¯å­¦ä¹ é˜ˆå€¼å‚æ•°
                - False: é˜ˆå€¼å›ºå®šä¸º0 (ä¼ ç»Ÿç‰ˆæœ¬)
                - True: é˜ˆå€¼ä¸ºå¯å­¦ä¹ å‚æ•° (æ–°å¢ç‰ˆæœ¬)
        """
        super().__init__()
        # ... ç½‘ç»œåˆå§‹åŒ– ...
        
        # é˜ˆå€¼å‚æ•°è®¾ç½®ï¼ˆä¸æŸ¯è¥¿ç‰ˆæœ¬å®Œå…¨ç›¸åŒï¼‰
        if learnable_thresholds:
            self.thresholds = nn.Parameter(torch.zeros(n_classes))
        else:
            self.register_buffer('thresholds', torch.zeros(n_classes))

def compute_loss(self, y_true, logits, location_param, scale_param):
    """
    CAACé«˜æ–¯æŸå¤±å‡½æ•°ï¼šä¸æŸ¯è¥¿ç‰ˆæœ¬ç›¸åŒçš„é€»è¾‘ï¼Œä½†ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒçš„çº¿æ€§ç»„åˆè§„åˆ™å’ŒCDF
    æ”¯æŒå›ºå®š/å¯å­¦ä¹ é˜ˆå€¼ä¸¤ç§æ¨¡å¼
    """
    # ... å‰ç½®è®¾ç½®ç›¸åŒ ...
    
    # è®¡ç®—ç±»åˆ«å¾—åˆ†çš„é«˜æ–¯åˆ†å¸ƒå‚æ•°
    class_locations, class_stds = self.model.action_net.compute_class_distribution_params(
        location_param, scale_param, distribution_type='gaussian'
    )
    
    # ä½¿ç”¨é˜ˆå€¼å‚æ•°ï¼ˆè‡ªåŠ¨æ”¯æŒå›ºå®š/å¯å­¦ä¹ ï¼‰
    thresholds = self.thresholds
    
    # é«˜æ–¯åˆ†å¸ƒCDFè®¡ç®—ç±»åˆ«æ¦‚ç‡
    normalized_thresholds = (thresholds.unsqueeze(0) - class_locations) / class_stds
    standard_normal = Normal(0, 1)
    P_k = 1 - standard_normal.cdf(normalized_thresholds)
    P_k = torch.clamp(P_k, min=1e-7, max=1-1e-7)
    
    # ... ç›¸åŒçš„BCEæŸå¤±è®¡ç®— ...
```

### 3.4 MLP (Softmax) - SoftmaxMLPModel

**ç‰¹è‰²**: æ ‡å‡†Softmax + äº¤å‰ç†µï¼Œå¿½ç•¥å°ºåº¦å‚æ•°

```python
def compute_loss(self, y_true, logits, location_param, scale_param):
    """
    æ ‡å‡†Softmaxäº¤å‰ç†µæŸå¤±å‡½æ•°
    ä»…ä½¿ç”¨logitsï¼Œå®Œå…¨å¿½ç•¥å°ºåº¦å‚æ•° scale_param
    """
    return F.cross_entropy(logits, y_true)
```

### 3.5 MLP (OvR Cross Entropy) - OvRCrossEntropyMLPModel  

**ç‰¹è‰²**: OvRç­–ç•¥ + äº¤å‰ç†µï¼Œå¿½ç•¥å°ºåº¦å‚æ•°

```python
def compute_loss(self, y_true, logits, location_param, scale_param):
    """
    OvRäº¤å‰ç†µæŸå¤±å‡½æ•°
    ä»…ä½¿ç”¨logitsï¼Œä¸ä½¿ç”¨å°ºåº¦å‚æ•°
    è¿™æ˜¯ä¸CAACæ–¹æ³•çš„æ ¸å¿ƒåŒºåˆ«ï¼šç›¸åŒOvRç­–ç•¥ä½†ä¸ä½¿ç”¨å°ºåº¦å‚æ•°
    """
    return F.cross_entropy(logits, y_true)
```

### 3.6 MLP (Crammer & Singer Hinge) - CrammerSingerMLPModel

**ç‰¹è‰²**: å¤šåˆ†ç±»é“°é“¾æŸå¤±ï¼ŒåŸºäºmarginæœ€å¤§åŒ–

```python
def compute_loss(self, y_true, logits, location_param, scale_param):
    """
    Crammer & Singer å¤šåˆ†ç±»é“°é“¾æŸå¤±å‡½æ•°
    ä»…ä½¿ç”¨logitsï¼Œå®Œå…¨å¿½ç•¥å°ºåº¦å‚æ•°
    """
    batch_size, n_classes = logits.shape
    
    # è·å–æ­£ç¡®ç±»åˆ«çš„åˆ†æ•°
    correct_scores = logits.gather(1, y_true.unsqueeze(1)).squeeze(1)
    
    # è®¡ç®—marginè¿å
    margins = logits - correct_scores.unsqueeze(1) + 1.0
    margins.scatter_(1, y_true.unsqueeze(1), 0)  # æ’é™¤æ­£ç¡®ç±»åˆ«
    
    # é“°é“¾æŸå¤±
    max_margins, _ = margins.max(dim=1)
    hinge_loss = F.relu(max_margins)
    
    return hinge_loss.mean()
```

---

## 4. æ¨¡å—æ¥å£è®¾è®¡

### 4.1 ç»Ÿä¸€é…ç½®æ¥å£

```python
# ç»Ÿä¸€å‚æ•°é…ç½®
common_params = {
    'representation_dim': 64,
    'latent_dim': None,  # é»˜è®¤ç­‰äºrepresentation_dimï¼Œä½“ç°æ¦‚å¿µå¯¹é½
    'feature_hidden_dims': [64],
    'abduction_hidden_dims': [128, 64],
    'lr': 0.001,
    'batch_size': 32,
    'learnable_thresholds': False,  # æ–°å¢ï¼šæ§åˆ¶é˜ˆå€¼å‚æ•°æ¨¡å¼
    'epochs': 100,
    'device': None,
    'early_stopping_patience': 10,
    'early_stopping_min_delta': 0.0001
}
```

### 4.2 è®­ç»ƒæ¥å£

```python
def fit(self, X_train, y_train, X_val=None, y_val=None, verbose=1):
    """
    ç»Ÿä¸€è®­ç»ƒæ¥å£
    
    Args:
        X_train: è®­ç»ƒç‰¹å¾ [N, D]
        y_train: è®­ç»ƒæ ‡ç­¾ [N]
        X_val: éªŒè¯ç‰¹å¾ [N_val, D] (å¯é€‰)
        y_val: éªŒè¯æ ‡ç­¾ [N_val] (å¯é€‰)
        verbose: è¾“å‡ºè¯¦ç»†ç¨‹åº¦ (0|1|2)
    
    Returns:
        self: æ”¯æŒé“¾å¼è°ƒç”¨
    """
    # ç»Ÿä¸€çš„è®­ç»ƒæµç¨‹å®ç°
```

### 4.3 é¢„æµ‹æ¥å£

```python
def predict_proba(self, X):
    """é¢„æµ‹ç±»åˆ«æ¦‚ç‡"""
    
def predict(self, X):
    """é¢„æµ‹ç±»åˆ«æ ‡ç­¾"""
    
def get_params(self, deep=True):
    """è·å–æ¨¡å‹å‚æ•°"""
    
def set_params(self, **params):
    """è®¾ç½®æ¨¡å‹å‚æ•°"""
```

---

## 5. æ‰©å±•æœºåˆ¶è®¾è®¡

### 5.1 æ–°æ–¹æ³•æ‰©å±•æ¥å£

```python
class NewMethodModel:
    """æ–°æ–¹æ³•æ‰©å±•æ¨¡æ¿"""
    
    def __init__(self, input_dim, representation_dim=64, latent_dim=None, ...):
        # éµå¾ªç»Ÿä¸€é…ç½®æ¨¡å¼
        self.latent_dim = latent_dim if latent_dim is not None else representation_dim
        # ... å…¶ä»–å‚æ•°è®¾ç½® ...
        self._setup_model_optimizer()
    
    def _setup_model_optimizer(self):
        """ä½¿ç”¨ç»Ÿä¸€ç½‘ç»œæ¶æ„"""
        self.model = UnifiedClassificationNetwork(
            self.input_dim, self.representation_dim, self.latent_dim, self.n_classes,
            self.feature_hidden_dims, self.abduction_hidden_dims
        ).to(self.device)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)
    
    def compute_loss(self, y_true, logits, location_param, scale_param):
        """å®ç°æ–°çš„æŸå¤±å‡½æ•°é€»è¾‘"""
        # æ–°æ–¹æ³•çš„åˆ›æ–°ç‚¹åœ¨è¿™é‡Œ
        pass
    
    # ç»§æ‰¿ç»Ÿä¸€çš„ fit, predict_proba, predict ç­‰æ–¹æ³•
```

### 5.2 æ¶æ„ç»„ä»¶æ‰©å±•

```python
# æ‰©å±•æ–°çš„ç½‘ç»œç»„ä»¶
class EnhancedFeatureNetwork(FeatureNetwork):
    """å¢å¼ºç‰¹å¾ç½‘ç»œ - é¢„ç•™æ‰©å±•ç‚¹"""
    
class AttentionAbductionNetwork(AbductionNetwork):
    """æ³¨æ„åŠ›æ¨ç†ç½‘ç»œ - é¢„ç•™æ‰©å±•ç‚¹"""
    
class DynamicActionNetwork(ActionNetwork):
    """åŠ¨æ€è¡ŒåŠ¨ç½‘ç»œ - é¢„ç•™æ‰©å±•ç‚¹"""
```

### 5.3 åˆ†å¸ƒç±»å‹æ‰©å±•

```python
def compute_class_distribution_params(self, location_param, scale_param, distribution_type='cauchy'):
    """æ”¯æŒæ›´å¤šåˆ†å¸ƒç±»å‹"""
    
    if distribution_type == 'cauchy':
        # ç°æœ‰æŸ¯è¥¿åˆ†å¸ƒé€»è¾‘
    elif distribution_type == 'gaussian':
        # ç°æœ‰é«˜æ–¯åˆ†å¸ƒé€»è¾‘
    elif distribution_type == 'student_t':
        # é¢„ç•™ï¼šStudent-tåˆ†å¸ƒæ‰©å±•
    elif distribution_type == 'laplace':
        # é¢„ç•™ï¼šæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒæ‰©å±•
    elif distribution_type == 'mixture':
        # é¢„ç•™ï¼šæ··åˆåˆ†å¸ƒæ‰©å±•
    else:
        raise ValueError(f"Unsupported distribution type: {distribution_type}")
```

---

## 6. å®ç°ç»†èŠ‚ä¸æœ€ä½³å®è·µ

### 6.1 æ•°å€¼ç¨³å®šæ€§

```python
# 1. å°ºåº¦å‚æ•°æ­£æ€§çº¦æŸ
scale_param = F.softplus(self.scale_head(shared_features))

# 2. æ¦‚ç‡å€¼æˆªæ–­
P_k = torch.clamp(P_k, min=1e-7, max=1-1e-7)

# 3. åˆ†æ¯ç¨³å®šæ€§
class_scales = torch.clamp(class_scales, min=1e-6)
```

### 6.2 å†…å­˜ä¼˜åŒ–

```python
# 1. åŠæ—¶é‡Šæ”¾ä¸­é—´å˜é‡
del intermediate_tensor

# 2. æ¢¯åº¦ç´¯ç§¯æ”¯æŒå¤§æ‰¹é‡
if (batch_idx + 1) % accumulation_steps == 0:
    optimizer.step()
    optimizer.zero_grad()

# 3. æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
with torch.cuda.amp.autocast():
    loss = compute_loss(...)
```

### 6.3 è®¾å¤‡å…¼å®¹æ€§

```python
# è‡ªåŠ¨è®¾å¤‡æ£€æµ‹
if device is None:
    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ç»Ÿä¸€è®¾å¤‡è¿ç§»
def to_device(self, device):
    self.model = self.model.to(device)
    self.device = device
    return self
```

---

## 7. ä½¿ç”¨ç¤ºä¾‹

### 7.1 åˆ›å»ºä¸ƒç§æ–¹æ³•å®ä¾‹

```python
# ç»Ÿä¸€å‚æ•°é…ç½®
common_params = {
    'input_dim': 20,
    'n_classes': 3,
    'representation_dim': 64,
    'latent_dim': None,  # é»˜è®¤ç­‰äºrepresentation_dim
    'feature_hidden_dims': [64],
    'abduction_hidden_dims': [128, 64],
    'lr': 0.001,
    'batch_size': 32,
    'epochs': 100
}

# åˆ›å»ºä¸ƒç§æ–¹æ³•çš„å®ä¾‹
methods = {
    # CAACç³»åˆ— - å›ºå®šé˜ˆå€¼ç‰ˆæœ¬
    'CAAC_Cauchy': CAACOvRModel(**{**common_params, 'learnable_thresholds': False}),
    'CAAC_Gaussian': CAACOvRGaussianModel(**{**common_params, 'learnable_thresholds': False}),
    
    # CAACç³»åˆ— - å¯å­¦ä¹ é˜ˆå€¼ç‰ˆæœ¬ï¼ˆæ–°å¢ï¼‰
    'CAAC_Cauchy_Learnable': CAACOvRModel(**{**common_params, 'learnable_thresholds': True}),
    'CAAC_Gaussian_Learnable': CAACOvRGaussianModel(**{**common_params, 'learnable_thresholds': True}),
    
    # MLPç³»åˆ—
    'MLP_Softmax': SoftmaxMLPModel(**common_params),
    'MLP_OvR_CE': OvRCrossEntropyMLPModel(**common_params),
    'MLP_Hinge': CrammerSingerMLPModel(**common_params)
}
```

### 7.2 è®­ç»ƒä¸è¯„ä¼°ç¤ºä¾‹

```python
# è®­ç»ƒæ‰€æœ‰æ–¹æ³•
for name, model in methods.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train, X_val, y_val, verbose=1)
    
    # è¯„ä¼°æ€§èƒ½
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {accuracy:.4f}")
```

### 7.3 å¯å­¦ä¹ é˜ˆå€¼åˆ†æ

```python
# åˆ†æå¯å­¦ä¹ é˜ˆå€¼çš„å­¦ä¹ ç»“æœ
learnable_models = ['CAAC_Cauchy_Learnable', 'CAAC_Gaussian_Learnable']

for name in learnable_models:
    model = methods[name]
    thresholds = model.thresholds.data.cpu().numpy()
    print(f"{name} learned thresholds: {thresholds}")
    
    # å¯è§†åŒ–é˜ˆå€¼åˆ†å¸ƒ
    plt.figure(figsize=(8, 4))
    plt.bar(range(len(thresholds)), thresholds)
    plt.title(f'{name} - Learned Thresholds by Class')
    plt.xlabel('Class Index')
    plt.ylabel('Threshold Value')
    plt.show()
```

---

## 8. æµ‹è¯•ä¸éªŒè¯

### 8.1 å•å…ƒæµ‹è¯•è®¾è®¡

```python
def test_feature_network():
    """æµ‹è¯•ç‰¹å¾ç½‘ç»œ"""
    
def test_abduction_network():
    """æµ‹è¯•æ¨ç†ç½‘ç»œ"""
    
def test_action_network():
    """æµ‹è¯•è¡ŒåŠ¨ç½‘ç»œ"""
    
def test_unified_network():
    """æµ‹è¯•ç»Ÿä¸€ç½‘ç»œ"""

def test_concept_alignment():
    """æµ‹è¯•æ¦‚å¿µå¯¹é½ï¼šd_latent = d_repr"""

def test_learnable_thresholds():
    """æµ‹è¯•å¯å­¦ä¹ é˜ˆå€¼åŠŸèƒ½"""
    # éªŒè¯å›ºå®šé˜ˆå€¼æ¨¡å¼
    model_fixed = CAACOvRModel(learnable_thresholds=False)
    assert not model_fixed.thresholds.requires_grad
    
    # éªŒè¯å¯å­¦ä¹ é˜ˆå€¼æ¨¡å¼
    model_learnable = CAACOvRModel(learnable_thresholds=True)
    assert model_learnable.thresholds.requires_grad
```

### 8.2 é›†æˆæµ‹è¯•

```python
def test_method_compatibility():
    """æµ‹è¯•ä¸ƒç§æ–¹æ³•çš„å…¼å®¹æ€§"""
    
def test_training_pipeline():
    """æµ‹è¯•è®­ç»ƒæµç¨‹"""
    
def test_prediction_consistency():
    """æµ‹è¯•é¢„æµ‹ä¸€è‡´æ€§"""
```

---

## 9. éƒ¨ç½²ä¸ä¼˜åŒ–

### 9.1 æ¨¡å‹ä¿å­˜ä¸åŠ è½½

```python
def save_model(self, filepath):
    """ä¿å­˜å®Œæ•´æ¨¡å‹çŠ¶æ€"""
    
def load_model(cls, filepath):
    """åŠ è½½æ¨¡å‹çŠ¶æ€"""
    
def export_onnx(self, filepath, input_shape):
    """å¯¼å‡ºONNXæ ¼å¼"""
```

### 9.2 æ¨ç†ä¼˜åŒ–

```python
def optimize_for_inference(self):
    """æ¨ç†ä¼˜åŒ–"""
    self.model.eval()
    # å¯é€‰ï¼šæ¨¡å‹é‡åŒ–ã€å‰ªæç­‰
    
def batch_predict(self, X, batch_size=1000):
    """æ‰¹é‡é¢„æµ‹ä¼˜åŒ–"""
```

---

## 10. æœªæ¥æ‰©å±•è§„åˆ’

### 10.1 æ¶æ„æ‰©å±•æ–¹å‘

- [ ] **å¤šæ¨¡æ€è¾“å…¥æ”¯æŒ**: æ‰©å±•FeatureNetworkæ”¯æŒå›¾åƒã€æ–‡æœ¬ç­‰
- [ ] **åŠ¨æ€æ¶æ„**: è‡ªé€‚åº”è°ƒæ•´ç½‘ç»œæ·±åº¦å’Œå®½åº¦
- [ ] **å…ƒå­¦ä¹ é›†æˆ**: æ”¯æŒå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
- [ ] **è”é‚¦å­¦ä¹ **: åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

### 10.2 ç®—æ³•åˆ›æ–°æ–¹å‘

- [ ] **è‡ªé€‚åº”åˆ†å¸ƒé€‰æ‹©**: æ ¹æ®æ•°æ®ç‰¹æ€§åŠ¨æ€é€‰æ‹©æœ€ä¼˜åˆ†å¸ƒ
- [ ] **è´å¶æ–¯é›†æˆ**: å¼•å…¥è´å¶æ–¯ç¥ç»ç½‘ç»œå¢å¼ºä¸ç¡®å®šæ€§
- [ ] **å¯¹æŠ—è®­ç»ƒ**: æå‡æ¨¡å‹é²æ£’æ€§
- [ ] **çŸ¥è¯†è’¸é¦**: æ¨¡å‹å‹ç¼©ä¸åŠ é€Ÿ

### 10.3 åº”ç”¨åœºæ™¯æ‰©å±•

- [ ] **å¤§è§„æ¨¡åˆ†ç±»**: æ”¯æŒç™¾ä¸‡çº§ç±»åˆ«åˆ†ç±»
- [ ] **å®æ—¶ç³»ç»Ÿ**: ä½å»¶è¿Ÿæ¨ç†ä¼˜åŒ–
- [ ] **è¾¹ç¼˜è®¡ç®—**: è½»é‡åŒ–éƒ¨ç½²æ–¹æ¡ˆ
- [ ] **å¯è§£é‡ŠAI**: å¢å¼ºå†³ç­–é€æ˜åº¦

---

**æ–‡æ¡£ç»“æŸ**

*æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†ç»Ÿä¸€æ¶æ„åˆ†ç±»æ–¹æ³•çš„ä»£ç è®¾è®¡ä¸å®ç°ã€‚æ–‡æ¡£å°†éšç€ä»£ç æ¼”è¿›æŒç»­æ›´æ–°ã€‚*

**ç›¸å…³æ–‡æ¡£**:
- ğŸ“Š **æ•°å­¦åŸç†**: `docs/unified_methods_mathematical_principles.md`
- ğŸ§  **ç ”ç©¶åŠ¨æœº**: `docs/motivation.md`
- ğŸ”¬ **å®éªŒå¯¹æ¯”**: `compare_methods.py`
- ğŸ—ï¸ **å‚è€ƒè®¾è®¡**: `architecture_design.md` 