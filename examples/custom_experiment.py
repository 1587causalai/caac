"""
ğŸ”¬ CAACé¡¹ç›®è‡ªå®šä¹‰å®éªŒç¤ºä¾‹

æœ¬æ–‡ä»¶å±•ç¤ºå¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰å®éªŒé…ç½®ï¼ŒåŒ…æ‹¬ï¼š
- è‡ªå®šä¹‰å®éªŒå‚æ•°
- äº¤äº’å¼å®éªŒè®¾è®¡
- é«˜çº§å®éªŒæ§åˆ¶

è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼špython examples/custom_experiment.py
"""

import sys
import os
import json
import numpy as np
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.experiments.experiment_manager import ExperimentManager
from src.models.caac_ovr_model import CAACOvRModel, CAACOvRGaussianModel
from sklearn.datasets import make_classification, load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score
import matplotlib.pyplot as plt


class CustomExperimentDesigner:
    """
    è‡ªå®šä¹‰å®éªŒè®¾è®¡å™¨
    """
    
    def __init__(self, results_base_dir="examples/custom_results"):
        self.manager = ExperimentManager(base_results_dir=results_base_dir)
        self.results_base_dir = results_base_dir
        
    def create_synthetic_dataset_experiment(self):
        """
        ç¤ºä¾‹1: åˆæˆæ•°æ®é›†å®éªŒ
        """
        print("=" * 60)
        print("ğŸ”¬ ç¤ºä¾‹1: åˆæˆæ•°æ®é›†å®éªŒ")
        print("=" * 60)
        
        # 1. åˆ›å»ºåˆæˆæ•°æ®é›†
        print("ğŸ“Š åˆ›å»ºåˆæˆæ•°æ®é›†...")
        datasets = {}
        
        # ç®€å•åˆ†ç¦»æ•°æ®
        X_easy, y_easy = make_classification(
            n_samples=1000, n_features=20, n_classes=3, n_informative=15,
            n_redundant=5, random_state=42, class_sep=2.0
        )
        datasets['easy'] = (X_easy, y_easy)
        
        # å›°éš¾åˆ†ç¦»æ•°æ®
        X_hard, y_hard = make_classification(
            n_samples=1000, n_features=20, n_classes=3, n_informative=10,
            n_redundant=5, random_state=42, class_sep=0.5
        )
        datasets['hard'] = (X_hard, y_hard)
        
        # é«˜ç»´æ•°æ®
        X_highdim, y_highdim = make_classification(
            n_samples=1000, n_features=100, n_classes=5, n_informative=50,
            n_redundant=25, random_state=42, class_sep=1.0
        )
        datasets['high_dimensional'] = (X_highdim, y_highdim)
        
        # 2. å¯¹æ¯ä¸ªæ•°æ®é›†è¿è¡Œå®éªŒ
        results = {}
        for name, (X, y) in datasets.items():
            print(f"\nğŸ”¥ æµ‹è¯•æ•°æ®é›†: {name}")
            
            # æ•°æ®é¢„å¤„ç†
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42, stratify=y
            )
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
            model = CAACOvRModel(
                input_dim=X.shape[1],
                n_classes=len(np.unique(y)),
                representation_dim=min(64, X.shape[1]),
                epochs=50,
                verbose=0
            )
            
            model.fit(X_train_scaled, y_train)
            predictions = model.predict(X_test_scaled)
            
            accuracy = accuracy_score(y_test, predictions)
            f1 = f1_score(y_test, predictions, average='macro')
            
            results[name] = {'accuracy': accuracy, 'f1': f1}
            print(f"  âœ… å‡†ç¡®ç‡: {accuracy:.4f}, F1: {f1:.4f}")
        
        # 3. ä¿å­˜ç»“æœ
        self._save_experiment_results("synthetic_dataset_experiment", results)
        return results
    
    def parameter_sensitivity_experiment(self):
        """
        ç¤ºä¾‹2: å‚æ•°æ•æ„Ÿæ€§å®éªŒ
        """
        print("\n" + "=" * 60)
        print("âš™ï¸ ç¤ºä¾‹2: å‚æ•°æ•æ„Ÿæ€§å®éªŒ")
        print("=" * 60)
        
        # 1. å‡†å¤‡æ•°æ®
        print("ğŸ“Š åŠ è½½Digitsæ•°æ®é›†...")
        data = load_digits()
        X, y = data.data, data.target
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # 2. å®šä¹‰å‚æ•°ç½‘æ ¼
        param_grids = {
            'representation_dim': [32, 64, 128],
            'learning_rate': [0.001, 0.01, 0.1],
            'uniqueness_constraint': [False, True],
            'learnable_thresholds': [False, True]
        }
        
        # 3. è¿è¡Œå‚æ•°æ•æ„Ÿæ€§åˆ†æ
        results = {}
        base_config = {
            'input_dim': X.shape[1],
            'n_classes': len(np.unique(y)),
            'epochs': 30,
            'verbose': 0
        }
        
        for param_name, param_values in param_grids.items():
            print(f"\nğŸ”§ æµ‹è¯•å‚æ•°: {param_name}")
            param_results = []
            
            for value in param_values:
                config = base_config.copy()
                if param_name == 'learning_rate':
                    config['lr'] = value
                else:
                    config[param_name] = value
                
                # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
                model = CAACOvRModel(**config)
                model.fit(X_train_scaled, y_train)
                predictions = model.predict(X_test_scaled)
                accuracy = accuracy_score(y_test, predictions)
                
                param_results.append(accuracy)
                print(f"  {param_name}={value}: {accuracy:.4f}")
            
            results[param_name] = dict(zip(param_values, param_results))
        
        # 4. å¯è§†åŒ–ç»“æœ
        self._plot_parameter_sensitivity(results)
        self._save_experiment_results("parameter_sensitivity_experiment", results)
        return results
    
    def distribution_comparison_experiment(self):
        """
        ç¤ºä¾‹3: åˆ†å¸ƒå¯¹æ¯”å®éªŒ (Cauchy vs Gaussian)
        """
        print("\n" + "=" * 60)
        print("ğŸ“Š ç¤ºä¾‹3: åˆ†å¸ƒå¯¹æ¯”å®éªŒ")
        print("=" * 60)
        
        # 1. å‡†å¤‡å¤šä¸ªæ•°æ®é›†
        datasets = {
            'digits': load_digits(),
        }
        
        # æ·»åŠ åˆæˆæ•°æ®é›†
        X_synthetic, y_synthetic = make_classification(
            n_samples=1000, n_features=20, n_classes=4,
            n_informative=15, random_state=42
        )
        datasets['synthetic'] = type('obj', (object,), {
            'data': X_synthetic, 'target': y_synthetic
        })()
        
        # 2. å¯¹æ¯”Cauchyå’ŒGaussianåˆ†å¸ƒ
        all_results = {}
        
        for dataset_name, data in datasets.items():
            print(f"\nğŸ“Š æµ‹è¯•æ•°æ®é›†: {dataset_name}")
            X, y = data.data, data.target
            
            # æ•°æ®é¢„å¤„ç†
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42, stratify=y
            )
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # æµ‹è¯•ä¸åŒåˆ†å¸ƒ
            models = {
                'CAAC_Cauchy': CAACOvRModel(
                    input_dim=X.shape[1],
                    n_classes=len(np.unique(y)),
                    representation_dim=64,
                    epochs=50,
                    verbose=0
                ),
                'CAAC_Gaussian': CAACOvRGaussianModel(
                    input_dim=X.shape[1],
                    n_classes=len(np.unique(y)),
                    representation_dim=64,
                    epochs=50,
                    verbose=0
                )
            }
            
            dataset_results = {}
            for model_name, model in models.items():
                print(f"  ğŸ”¥ è®­ç»ƒ {model_name}...")
                model.fit(X_train_scaled, y_train)
                predictions = model.predict(X_test_scaled)
                
                accuracy = accuracy_score(y_test, predictions)
                f1 = f1_score(y_test, predictions, average='macro')
                
                dataset_results[model_name] = {
                    'accuracy': accuracy,
                    'f1_score': f1
                }
                print(f"    âœ… å‡†ç¡®ç‡: {accuracy:.4f}, F1: {f1:.4f}")
            
            all_results[dataset_name] = dataset_results
        
        # 3. ä¿å­˜å’Œå¯è§†åŒ–ç»“æœ
        self._save_experiment_results("distribution_comparison_experiment", all_results)
        self._plot_distribution_comparison(all_results)
        return all_results
    
    def noise_robustness_custom_experiment(self):
        """
        ç¤ºä¾‹4: è‡ªå®šä¹‰å™ªå£°é²æ£’æ€§å®éªŒ
        """
        print("\n" + "=" * 60)
        print("ğŸ›¡ï¸ ç¤ºä¾‹4: è‡ªå®šä¹‰å™ªå£°é²æ£’æ€§å®éªŒ")
        print("=" * 60)
        
        # 1. å‡†å¤‡æ•°æ®
        X, y = make_classification(
            n_samples=2000, n_features=20, n_classes=3,
            n_informative=15, random_state=42, class_sep=1.5
        )
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # 2. è‡ªå®šä¹‰å™ªå£°æ°´å¹³å’Œç±»å‹
        noise_configs = [
            {'type': 'label_noise', 'levels': [0.0, 0.05, 0.1, 0.15, 0.2, 0.3]},
            {'type': 'feature_noise', 'levels': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]}
        ]
        
        all_results = {}
        
        for noise_config in noise_configs:
            noise_type = noise_config['type']
            noise_levels = noise_config['levels']
            
            print(f"\nğŸ”§ æµ‹è¯•å™ªå£°ç±»å‹: {noise_type}")
            
            type_results = {}
            for noise_level in noise_levels:
                print(f"  å™ªå£°æ°´å¹³: {noise_level}")
                
                # åº”ç”¨å™ªå£°
                if noise_type == 'label_noise':
                    X_train_noisy = X_train_scaled.copy()
                    y_train_noisy = self._add_label_noise(y_train, noise_level)
                else:  # feature_noise
                    X_train_noisy = self._add_feature_noise(X_train_scaled, noise_level)
                    y_train_noisy = y_train.copy()
                
                # è®­ç»ƒæ¨¡å‹
                model = CAACOvRModel(
                    input_dim=X.shape[1],
                    n_classes=len(np.unique(y)),
                    representation_dim=64,
                    epochs=50,
                    learnable_thresholds=True,
                    uniqueness_constraint=True,
                    verbose=0
                )
                
                model.fit(X_train_noisy, y_train_noisy)
                predictions = model.predict(X_test_scaled)
                accuracy = accuracy_score(y_test, predictions)
                
                type_results[noise_level] = accuracy
                print(f"    å‡†ç¡®ç‡: {accuracy:.4f}")
            
            all_results[noise_type] = type_results
        
        # 3. å¯è§†åŒ–å’Œä¿å­˜ç»“æœ
        self._plot_noise_robustness(all_results)
        self._save_experiment_results("noise_robustness_custom_experiment", all_results)
        return all_results
    
    def interactive_experiment_design(self):
        """
        ç¤ºä¾‹5: äº¤äº’å¼å®éªŒè®¾è®¡
        """
        print("\n" + "=" * 60)
        print("ğŸ® ç¤ºä¾‹5: äº¤äº’å¼å®éªŒè®¾è®¡ (è‡ªåŠ¨åŒ–ç‰ˆæœ¬)")
        print("=" * 60)
        
        # æ¨¡æ‹Ÿç”¨æˆ·é€‰æ‹©
        print("ğŸ“ è‡ªåŠ¨é…ç½®å®éªŒå‚æ•°...")
        
        experiment_config = {
            'dataset_type': 'synthetic',  # æ¨¡æ‹Ÿé€‰æ‹©
            'dataset_params': {
                'n_samples': 1500,
                'n_features': 30,
                'n_classes': 4,
                'class_sep': 1.0
            },
            'model_config': {
                'representation_dim': 64,
                'learnable_thresholds': True,
                'uniqueness_constraint': True,
                'epochs': 100
            },
            'experiment_params': {
                'test_size': 0.3,
                'cross_validation': True,
                'n_folds': 5
            }
        }
        
        print("âš™ï¸ é€‰æ‹©çš„é…ç½®:")
        print(json.dumps(experiment_config, indent=2))
        
        # æ‰§è¡Œè‡ªå®šä¹‰å®éªŒ
        result_dir = self.manager.run_custom_experiment(
            experiment_type='robustness',
            config={
                'datasets': ['synthetic_imbalanced'],
                'noise_levels': [0.0, 0.1, 0.2],
                'epochs': experiment_config['model_config']['epochs'],
                'representation_dim': experiment_config['model_config']['representation_dim']
            },
            save_name='interactive_designed_experiment'
        )
        
        print(f"âœ… äº¤äº’å¼å®éªŒå®Œæˆ! ç»“æœä¿å­˜åœ¨: {result_dir}")
        return result_dir
    
    def _add_label_noise(self, y, noise_level):
        """æ·»åŠ æ ‡ç­¾å™ªå£°"""
        if noise_level == 0:
            return y.copy()
        
        n_samples = len(y)
        n_noisy = int(n_samples * noise_level)
        noisy_indices = np.random.choice(n_samples, n_noisy, replace=False)
        
        y_noisy = y.copy()
        unique_labels = np.unique(y)
        
        for idx in noisy_indices:
            current_label = y_noisy[idx]
            possible_labels = unique_labels[unique_labels != current_label]
            y_noisy[idx] = np.random.choice(possible_labels)
        
        return y_noisy
    
    def _add_feature_noise(self, X, noise_level):
        """æ·»åŠ ç‰¹å¾å™ªå£°"""
        if noise_level == 0:
            return X.copy()
        
        noise = np.random.normal(0, noise_level, X.shape)
        return X + noise
    
    def _plot_parameter_sensitivity(self, results):
        """ç»˜åˆ¶å‚æ•°æ•æ„Ÿæ€§å›¾"""
        try:
            n_params = len(results)
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            axes = axes.flatten()
            
            for i, (param_name, param_results) in enumerate(results.items()):
                if i < len(axes):
                    ax = axes[i]
                    values = list(param_results.keys())
                    accuracies = list(param_results.values())
                    
                    ax.plot(values, accuracies, 'o-', linewidth=2, markersize=8)
                    ax.set_title(f'Sensitivity to {param_name}', fontsize=12)
                    ax.set_xlabel(param_name)
                    ax.set_ylabel('Accuracy')
                    ax.grid(True, alpha=0.3)
            
            # éšè—å¤šä½™çš„å­å›¾
            for i in range(len(results), len(axes)):
                axes[i].set_visible(False)
            
            plt.tight_layout()
            plt.savefig(f'{self.results_base_dir}/parameter_sensitivity.png', dpi=300, bbox_inches='tight')
            print("ğŸ“ˆ å‚æ•°æ•æ„Ÿæ€§å›¾å·²ä¿å­˜")
            plt.close()
        except Exception as e:
            print(f"âš ï¸ ç»˜å›¾å¤±è´¥: {e}")
    
    def _plot_distribution_comparison(self, results):
        """ç»˜åˆ¶åˆ†å¸ƒå¯¹æ¯”å›¾"""
        try:
            fig, axes = plt.subplots(1, 2, figsize=(12, 5))
            
            datasets = list(results.keys())
            models = list(results[datasets[0]].keys())
            
            metrics = ['accuracy', 'f1_score']
            metric_names = ['Accuracy', 'F1 Score']
            
            for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
                ax = axes[i]
                
                x = np.arange(len(datasets))
                width = 0.35
                
                for j, model in enumerate(models):
                    values = [results[dataset][model][metric] for dataset in datasets]
                    ax.bar(x + j*width, values, width, label=model)
                
                ax.set_xlabel('Dataset')
                ax.set_ylabel(metric_name)
                ax.set_title(f'{metric_name} Comparison')
                ax.set_xticks(x + width/2)
                ax.set_xticklabels(datasets)
                ax.legend()
                ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f'{self.results_base_dir}/distribution_comparison.png', dpi=300, bbox_inches='tight')
            print("ğŸ“Š åˆ†å¸ƒå¯¹æ¯”å›¾å·²ä¿å­˜")
            plt.close()
        except Exception as e:
            print(f"âš ï¸ ç»˜å›¾å¤±è´¥: {e}")
    
    def _plot_noise_robustness(self, results):
        """ç»˜åˆ¶å™ªå£°é²æ£’æ€§å›¾"""
        try:
            fig, axes = plt.subplots(1, 2, figsize=(12, 5))
            
            for i, (noise_type, type_results) in enumerate(results.items()):
                ax = axes[i]
                
                noise_levels = list(type_results.keys())
                accuracies = list(type_results.values())
                
                ax.plot(noise_levels, accuracies, 'o-', linewidth=2, markersize=8)
                ax.set_title(f'Robustness to {noise_type.replace("_", " ").title()}')
                ax.set_xlabel('Noise Level')
                ax.set_ylabel('Accuracy')
                ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f'{self.results_base_dir}/noise_robustness.png', dpi=300, bbox_inches='tight')
            print("ğŸ›¡ï¸ å™ªå£°é²æ£’æ€§å›¾å·²ä¿å­˜")
            plt.close()
        except Exception as e:
            print(f"âš ï¸ ç»˜å›¾å¤±è´¥: {e}")
    
    def _save_experiment_results(self, experiment_name, results):
        """ä¿å­˜å®éªŒç»“æœ"""
        try:
            os.makedirs(self.results_base_dir, exist_ok=True)
            
            result_file = f"{self.results_base_dir}/{experiment_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            # å¤„ç†numpyç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
            def convert_numpy(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, dict):
                    return {k: convert_numpy(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy(v) for v in obj]
                return obj
            
            results_serializable = convert_numpy(results)
            
            with open(result_file, 'w') as f:
                json.dump({
                    'experiment_name': experiment_name,
                    'timestamp': datetime.now().isoformat(),
                    'results': results_serializable
                }, f, indent=2)
            
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç»“æœå¤±è´¥: {e}")


def main():
    """
    è¿è¡Œæ‰€æœ‰è‡ªå®šä¹‰å®éªŒç¤ºä¾‹
    """
    print("ğŸ”¬ CAACé¡¹ç›®è‡ªå®šä¹‰å®éªŒç¤ºä¾‹")
    print("=" * 60)
    print("æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•è®¾è®¡å’Œè¿è¡Œè‡ªå®šä¹‰å®éªŒ")
    print()
    
    # åˆ›å»ºå®éªŒè®¾è®¡å™¨
    designer = CustomExperimentDesigner()
    
    try:
        # è¿è¡Œå„ç§è‡ªå®šä¹‰å®éªŒ
        print("ğŸš€ å¼€å§‹è¿è¡Œè‡ªå®šä¹‰å®éªŒ...")
        
        # 1. åˆæˆæ•°æ®é›†å®éªŒ
        synthetic_results = designer.create_synthetic_dataset_experiment()
        
        # 2. å‚æ•°æ•æ„Ÿæ€§å®éªŒ
        sensitivity_results = designer.parameter_sensitivity_experiment()
        
        # 3. åˆ†å¸ƒå¯¹æ¯”å®éªŒ
        distribution_results = designer.distribution_comparison_experiment()
        
        # 4. å™ªå£°é²æ£’æ€§å®éªŒ
        noise_results = designer.noise_robustness_custom_experiment()
        
        # 5. äº¤äº’å¼å®éªŒè®¾è®¡
        interactive_result = designer.interactive_experiment_design()
        
        # æ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸŠ æ‰€æœ‰è‡ªå®šä¹‰å®éªŒå®Œæˆ!")
        print("=" * 60)
        print("ğŸ“Š å®éªŒç»“æœæ€»ç»“:")
        print(f"  - åˆæˆæ•°æ®é›†å®éªŒ: {len(synthetic_results)} ä¸ªæ•°æ®é›†æµ‹è¯•")
        print(f"  - å‚æ•°æ•æ„Ÿæ€§å®éªŒ: {len(sensitivity_results)} ä¸ªå‚æ•°æµ‹è¯•")
        print(f"  - åˆ†å¸ƒå¯¹æ¯”å®éªŒ: {len(distribution_results)} ä¸ªæ•°æ®é›†å¯¹æ¯”")
        print(f"  - å™ªå£°é²æ£’æ€§å®éªŒ: {len(noise_results)} ç§å™ªå£°ç±»å‹æµ‹è¯•")
        print(f"  - äº¤äº’å¼å®éªŒ: ç»“æœä¿å­˜åœ¨ {interactive_result}")
        print()
        print("ğŸ’¡ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  - examples/custom_results/*.json - è¯¦ç»†ç»“æœæ•°æ®")
        print("  - examples/custom_results/*.png - å¯è§†åŒ–å›¾è¡¨")
        print()
        print("ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("  1. æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨åˆ†æç»“æœ")
        print("  2. åŸºäºå‚æ•°æ•æ„Ÿæ€§ç»“æœä¼˜åŒ–æ¨¡å‹é…ç½®")
        print("  3. å‚è€ƒè¿™äº›ç¤ºä¾‹è®¾è®¡ä½ è‡ªå·±çš„å®éªŒ")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œè‡ªå®šä¹‰å®éªŒæ—¶å‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥ç¯å¢ƒé…ç½®å’Œä¾èµ–å®‰è£…")


if __name__ == "__main__":
    main() 