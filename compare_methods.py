#!/usr/bin/env python3
"""
æ¯”è¾ƒä¸åŒåˆ†ç±»æ–¹æ³•çš„å®éªŒè„šæœ¬
ä½¿ç”¨ç»Ÿä¸€ç½‘ç»œæ¶æ„ï¼Œä»…æŸå¤±å‡½æ•°ä¸åŒï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ
åŒ…æ‹¬: CAACåˆ†ç±»å™¨, æ ‡å‡†MLP, Focal Loss, Label Smoothingç­‰
ä¸ç»å…¸æœºå™¨å­¦ä¹ æ–¹æ³•è¿›è¡Œæ€§èƒ½æ¯”è¾ƒ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris, load_wine, load_breast_cancer, load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import time
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æˆ‘ä»¬çš„ç»Ÿä¸€ç½‘ç»œæ¨¡å‹
from src.models.caac_ovr_model import (
    CAACOvRModel, 
    SoftmaxMLPModel,
    OvRCrossEntropyMLPModel,
    CAACOvRGaussianModel,
    CrammerSingerMLPModel
)

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
np.random.seed(42)

def load_datasets():
    """åŠ è½½æ‰€æœ‰æµ‹è¯•æ•°æ®é›†"""
    datasets = {}
    
    # Irisæ•°æ®é›†
    iris = load_iris()
    datasets['iris'] = {
        'data': iris.data,
        'target': iris.target,
        'target_names': iris.target_names,
        'name': 'Iris'
    }
    
    # Wineæ•°æ®é›†
    wine = load_wine()
    datasets['wine'] = {
        'data': wine.data,
        'target': wine.target,
        'target_names': wine.target_names,
        'name': 'Wine'
    }
    
    # Breast Canceræ•°æ®é›†
    bc = load_breast_cancer()
    datasets['breast_cancer'] = {
        'data': bc.data,
        'target': bc.target,
        'target_names': bc.target_names,
        'name': 'Breast Cancer'
    }
    
    # Digitsæ•°æ®é›†
    digits = load_digits()
    datasets['digits'] = {
        'data': digits.data,
        'target': digits.target,
        'target_names': [str(i) for i in range(10)],
        'name': 'Digits'
    }
    
    return datasets

def create_comparison_methods():
    """åˆ›å»ºç”¨äºæ¯”è¾ƒçš„åˆ†ç±»æ–¹æ³•"""
    # ç»Ÿä¸€çš„ç½‘ç»œæ¶æ„å‚æ•°
    # é‡è¦æ¦‚å¿µï¼šd_latent = d_reprï¼Œå› æœè¡¨å¾ç»´åº¦ç­‰äºç‰¹å¾è¡¨å¾ç»´åº¦
    common_params = {
        'representation_dim': 64,
        'latent_dim': None,  # é»˜è®¤ç­‰äºrepresentation_dimï¼Œä½“ç°æ¦‚å¿µå¯¹é½
        'feature_hidden_dims': [64],
        'abduction_hidden_dims': [128, 64],
        'lr': 0.001,
        'batch_size': 32,
        'epochs': 100,
        'device': None,
        'early_stopping_patience': 10,
        'early_stopping_min_delta': 0.0001
    }
    
    methods = {
        # æ ¸å¿ƒå¯¹æ¯”ï¼šä¸ƒç§ç»Ÿä¸€æ¶æ„æ–¹æ³•ï¼ˆåŒ…å«å¯å­¦ä¹ é˜ˆå€¼å˜ä½“ï¼‰
        'CAAC_Cauchy': {
            'name': 'CAAC OvR (Cauchy)',
            'type': 'unified',
            'model_class': CAACOvRModel,
            'params': {**common_params, 'learnable_thresholds': False}
        },
        'CAAC_Cauchy_Learnable': {
            'name': 'CAAC OvR (Cauchy, Learnable)',
            'type': 'unified',
            'model_class': CAACOvRModel,
            'params': {**common_params, 'learnable_thresholds': True}
        },
        'CAAC_Gaussian': {
            'name': 'CAAC OvR (Gaussian)',
            'type': 'unified',
            'model_class': CAACOvRGaussianModel,
            'params': {**common_params, 'learnable_thresholds': False}
        },
        'CAAC_Gaussian_Learnable': {
            'name': 'CAAC OvR (Gaussian, Learnable)',
            'type': 'unified',
            'model_class': CAACOvRGaussianModel,
            'params': {**common_params, 'learnable_thresholds': True}
        },
        'MLP_Softmax': {
            'name': 'MLP (Softmax)',
            'type': 'unified',
            'model_class': SoftmaxMLPModel,
            'params': common_params
        },
        'MLP_OvR_CE': {
            'name': 'MLP (OvR Cross Entropy)',
            'type': 'unified',
            'model_class': OvRCrossEntropyMLPModel,
            'params': common_params
        },
        'MLP_Hinge': {
            'name': 'MLP (Crammer & Singer Hinge)',
            'type': 'unified',
            'model_class': CrammerSingerMLPModel,
            'params': common_params
        },
        
        # ç»å…¸æœºå™¨å­¦ä¹ æ–¹æ³•ä½œä¸ºåŸºå‡†
        'Softmax_LR': {
            'name': 'Softmax Regression',
            'type': 'sklearn',
            'model': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)
        },
        'Standard_OvR': {
            'name': 'OvR Logistic',
            'type': 'sklearn',
            'model': OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))
        },
        'SVM_RBF': {
            'name': 'SVM-RBF',
            'type': 'sklearn',
            'model': SVC(kernel='rbf', random_state=42, probability=True)
        },
        'Random_Forest': {
            'name': 'Random Forest',
            'type': 'sklearn',
            'model': RandomForestClassifier(n_estimators=100, random_state=42)
        },
        'Sklearn_MLP': {
            'name': 'MLP-Sklearn',
            'type': 'sklearn',
            'model': MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)
        }
    }
    return methods

def evaluate_method(method_info, X_train, X_test, y_train, y_test):
    """è¯„ä¼°å•ä¸ªæ–¹æ³•çš„æ€§èƒ½"""
    start_time = time.time()
    
    if method_info['type'] == 'unified':
        # ä½¿ç”¨æˆ‘ä»¬çš„ç»Ÿä¸€ç½‘ç»œæ¶æ„
        input_dim = X_train.shape[1]
        n_classes = len(np.unique(y_train))
        
        model = method_info['model_class'](
            input_dim=input_dim, 
            n_classes=n_classes,
            **method_info['params']
        )
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train, verbose=0)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        
    else:
        # ä½¿ç”¨sklearnæ¨¡å‹
        model = method_info['model']
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
    
    training_time = time.time() - start_time
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)
    recall_macro = recall_score(y_test, y_pred, average='macro')
    f1_macro = f1_score(y_test, y_pred, average='macro')
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    
    return {
        'accuracy': accuracy,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'training_time': training_time
    }

def run_comparison_experiments():
    """è¿è¡Œæ‰€æœ‰æ¯”è¾ƒå®éªŒ"""
    print("ğŸ”¬ å¼€å§‹è¿è¡Œåˆ†ç±»æ–¹æ³•æ¯”è¾ƒå®éªŒ")
    print("=" * 60)
    
    datasets = load_datasets()
    methods = create_comparison_methods()
    
    results = []
    
    for dataset_name, dataset in datasets.items():
        print(f"\nğŸ“Š æ­£åœ¨æµ‹è¯•æ•°æ®é›†: {dataset['name']}")
        print("-" * 40)
        
        # æ•°æ®é¢„å¤„ç†
        X = dataset['data']
        y = dataset['target']
        
        # åˆ†å‰²æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # æµ‹è¯•æ¯ç§æ–¹æ³•
        for method_key, method_info in methods.items():
            print(f"  ğŸ§ª æµ‹è¯•æ–¹æ³•: {method_info['name']}")
            
            try:
                metrics = evaluate_method(
                    method_info, 
                    X_train_scaled, X_test_scaled, 
                    y_train, y_test
                )
                
                results.append({
                    'Dataset': dataset['name'],
                    'Method': method_info['name'],
                    'Method_Key': method_key,
                    'Method_Type': method_info['type'],
                    'Accuracy': metrics['accuracy'],
                    'Precision_Macro': metrics['precision_macro'],
                    'Recall_Macro': metrics['recall_macro'],
                    'F1_Macro': metrics['f1_macro'],
                    'F1_Weighted': metrics['f1_weighted'],
                    'Training_Time': metrics['training_time']
                })
                
                print(f"    âœ… å‡†ç¡®ç‡: {metrics['accuracy']:.4f}, F1: {metrics['f1_macro']:.4f}, æ—¶é—´: {metrics['training_time']:.3f}s")
                
            except Exception as e:
                print(f"    âŒ é”™è¯¯: {str(e)}")
                continue
    
    return pd.DataFrame(results)



def create_comparison_plots(results_df):
    """Create comparison visualization charts"""
    plt.style.use('default')
    
    # Set figure size and layout
    fig, axes = plt.subplots(2, 2, figsize=(18, 14))
    fig.suptitle('Classification Methods Comparison: Cauchy vs. Gaussian vs. Standard', 
                 fontsize=16, fontweight='bold')
    
    # 1. Accuracy comparison
    ax1 = axes[0, 0]
    pivot_acc = results_df.pivot(index='Dataset', columns='Method', values='Accuracy')
    pivot_acc.plot(kind='bar', ax=ax1, rot=30, width=0.8)
    ax1.set_title('Accuracy Comparison by Dataset', fontsize=14)
    ax1.set_ylabel('Accuracy', fontsize=12)
    ax1.set_xlabel('Dataset', fontsize=12)
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0.85, 1.02)
    
    # 2. F1-score comparison
    ax2 = axes[0, 1]
    pivot_f1 = results_df.pivot(index='Dataset', columns='Method', values='F1_Macro')
    pivot_f1.plot(kind='bar', ax=ax2, rot=30, width=0.8)
    ax2.set_title('F1-Score (Macro) Comparison by Dataset', fontsize=14)
    ax2.set_ylabel('F1-Score (Macro)', fontsize=12)
    ax2.set_xlabel('Dataset', fontsize=12)
    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0.85, 1.02)
    
    # 3. Training time comparison
    ax3 = axes[1, 0]
    pivot_time = results_df.pivot(index='Dataset', columns='Method', values='Training_Time')
    pivot_time.plot(kind='bar', ax=ax3, rot=30, width=0.8)
    ax3.set_title('Training Time Comparison by Dataset', fontsize=14)
    ax3.set_ylabel('Training Time (seconds, log scale)', fontsize=12)
    ax3.set_xlabel('Dataset', fontsize=12)
    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
    ax3.grid(True, alpha=0.3)
    ax3.set_yscale('log')
    
    # 4. Scatter plot: Accuracy vs Training Time
    ax4 = axes[1, 1]
    methods = results_df['Method'].unique()
    colors = plt.cm.tab10(np.linspace(0, 1, len(methods)))
    
    for method, color in zip(methods, colors):
        method_data = results_df[results_df['Method'] == method]
        ax4.scatter(method_data['Training_Time'], method_data['Accuracy'], 
                   label=method, alpha=0.8, s=80, color=color, edgecolors='black', linewidth=0.5)
    
    ax4.set_xlabel('Training Time (seconds, log scale)', fontsize=12)
    ax4.set_ylabel('Accuracy', fontsize=12)
    ax4.set_title('Efficiency vs Performance Trade-off', fontsize=14)
    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
    ax4.grid(True, alpha=0.3)
    ax4.set_xscale('log')
    ax4.set_ylim(0.90, 1.02)
    
    plt.tight_layout()
    plt.savefig('results/methods_comparison_english.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("ğŸ“ˆ English comparison chart saved to: results/methods_comparison_english.png")

def create_summary_table(results_df):
    """åˆ›å»ºæ±‡æ€»æ¯”è¾ƒè¡¨"""
    print("\nğŸ“‹ æ–¹æ³•æ¯”è¾ƒæ±‡æ€»è¡¨")
    print("=" * 80)
    
    # æŒ‰æ–¹æ³•åˆ†ç»„è®¡ç®—å¹³å‡æ€§èƒ½
    summary = results_df.groupby('Method').agg({
        'Accuracy': ['mean', 'std'],
        'F1_Macro': ['mean', 'std'],
        'Training_Time': ['mean', 'std']
    }).round(4)
    
    print(summary)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_df.to_csv('results/methods_comparison_detailed.csv', index=False)
    summary.to_csv('results/methods_comparison_summary.csv')
    
    return summary

def generate_detailed_report(results_df, summary):
    """ç”Ÿæˆè¯¦ç»†çš„å®éªŒæ¯”è¾ƒæŠ¥å‘Š"""
    from datetime import datetime
    import os
    
    print("\nğŸ“„ ç”Ÿæˆè¯¦ç»†å®éªŒæŠ¥å‘Š")
    print("=" * 50)
    
    # ç¡®ä¿resultsç›®å½•å­˜åœ¨
    os.makedirs('results', exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"results/caac_methods_comparison_report_{timestamp}.md"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"""# CAACåˆ†ç±»æ–¹æ³•å¯¹æ¯”å®éªŒæŠ¥å‘Š

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## å®éªŒæ¦‚è¿°

æœ¬æŠ¥å‘Šå±•ç¤ºäº†**CAAC OvRåˆ†ç±»å™¨**ä¸å¤šç§ç»å…¸åˆ†ç±»æ–¹æ³•çš„å…¨é¢æ€§èƒ½æ¯”è¾ƒã€‚å®éªŒé‡‡ç”¨ç»Ÿä¸€çš„ç½‘ç»œæ¶æ„ï¼Œä»…åœ¨æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–ç­–ç•¥ä¸Šæœ‰æ‰€ä¸åŒï¼Œç¡®ä¿äº†å…¬å¹³çš„æ¯”è¾ƒç¯å¢ƒã€‚

### æ ¸å¿ƒç ”ç©¶é—®é¢˜
**ä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒçš„å°ºåº¦å‚æ•°æ˜¯å¦èƒ½å¤Ÿæå‡åˆ†ç±»æ€§èƒ½ï¼Ÿ**

### æµ‹è¯•çš„æ–¹æ³•æ¶æ„
æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•éƒ½é‡‡ç”¨ç›¸åŒçš„ç»Ÿä¸€æ¶æ„ï¼š
- **FeatureNet**: ç‰¹å¾æå–ç½‘ç»œ (è¾“å…¥ç»´åº¦ â†’ 64ç»´**ç¡®å®šæ€§ç‰¹å¾è¡¨å¾**)
- **AbductionNet**: æº¯å› æ¨ç†ç½‘ç»œ (64ç»´ â†’ 64ç»´**å› æœè¡¨å¾éšæœºå˜é‡**å‚æ•°)  
- **ActionNet**: è¡ŒåŠ¨å†³ç­–ç½‘ç»œ (64ç»´ â†’ **ç±»åˆ«æ•°é‡**çš„å¾—åˆ†)

**é‡è¦æ¦‚å¿µå¯¹é½**: 
- ç‰¹å¾è¡¨å¾ç»´åº¦ = å› æœè¡¨å¾ç»´åº¦ (d_repr = d_latent = 64)
- ç‰¹å¾è¡¨å¾æ˜¯ç¡®å®šæ€§æ•°å€¼ï¼Œå› æœè¡¨å¾æ˜¯éšæœºå˜é‡ï¼ˆä½ç½®+å°ºåº¦å‚æ•°ï¼‰
- å¾—åˆ†ç»´åº¦ç­‰äºç±»åˆ«æ•°é‡

### å®éªŒæ–¹æ³•

#### ç»Ÿä¸€æ¶æ„æ–¹æ³• (ç›¸åŒç½‘ç»œç»“æ„ï¼Œä¸åŒæŸå¤±å‡½æ•°)
1. **CAAC OvR (ä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒ)** - æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ï¼Œä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒçš„å°ºåº¦å‚æ•°
2. **CAAC OvR (ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒ)** - CAACæ¡†æ¶ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒè€ŒéæŸ¯è¥¿åˆ†å¸ƒ
3. **MLP (Softmax)** - æ ‡å‡†MLPï¼Œä½¿ç”¨SoftmaxæŸå¤±å‡½æ•°, ä»…ä»…ä½¿ç”¨ä½ç½®å‚æ•°è®¡ç®—æŸå¤±
4. **MLP (OvR Cross Entropy)** - æ ‡å‡†MLPï¼Œä½¿ç”¨OvRç­–ç•¥çš„äº¤å‰ç†µæŸå¤±å‡½æ•°, ä»…ä»…ä½¿ç”¨ä½ç½®å‚æ•°è®¡ç®—æŸå¤±

#### ç»å…¸æœºå™¨å­¦ä¹ åŸºå‡†æ–¹æ³•
5. **Softmax Regression** - å¤šé¡¹å¼logisticå›å½’
6. **OvR Logistic** - ä¸€å¯¹å…¶ä½™é€»è¾‘å›å½’
7. **SVM-RBF** - å¾„å‘åŸºå‡½æ•°æ”¯æŒå‘é‡æœº
8. **Random Forest** - éšæœºæ£®æ—é›†æˆæ–¹æ³•
9. **MLP-Sklearn** - Scikit-learnå¤šå±‚æ„ŸçŸ¥æœº

### æµ‹è¯•æ•°æ®é›†
- **Irisé¸¢å°¾èŠ±æ•°æ®é›†**: 3ç±», 4ç‰¹å¾, 150æ ·æœ¬
- **Wineçº¢é…’æ•°æ®é›†**: 3ç±», 13ç‰¹å¾, 178æ ·æœ¬  
- **Breast Cancerä¹³è…ºç™Œæ•°æ®é›†**: 2ç±», 30ç‰¹å¾, 569æ ·æœ¬
- **Digitsæ‰‹å†™æ•°å­—æ•°æ®é›†**: 10ç±», 64ç‰¹å¾, 1797æ ·æœ¬

## è¯¦ç»†å®éªŒç»“æœ

### å‡†ç¡®ç‡å¯¹æ¯”

""")
        
        # åˆ›å»ºå‡†ç¡®ç‡å¯¹æ¯”è¡¨
        pivot_acc = results_df.pivot(index='Dataset', columns='Method', values='Accuracy')
        f.write(pivot_acc.round(4).to_markdown())
        f.write("\n\n### F1åˆ†æ•°å¯¹æ¯” (Macro Average)\n\n")
        
        # åˆ›å»ºF1åˆ†æ•°å¯¹æ¯”è¡¨
        pivot_f1 = results_df.pivot(index='Dataset', columns='Method', values='F1_Macro')
        f.write(pivot_f1.round(4).to_markdown())
        f.write("\n\n### è®­ç»ƒæ—¶é—´å¯¹æ¯” (ç§’)\n\n")
        
        # åˆ›å»ºè®­ç»ƒæ—¶é—´å¯¹æ¯”è¡¨
        pivot_time = results_df.pivot(index='Dataset', columns='Method', values='Training_Time')
        f.write(pivot_time.round(3).to_markdown())
        f.write("\n\n")
        
        # æ–¹æ³•æ€§èƒ½ç»Ÿè®¡
        f.write("""## æ–¹æ³•æ€§èƒ½ç»Ÿè®¡

### å¹³å‡æ€§èƒ½æ±‡æ€»

""")
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_performance = results_df.groupby('Method').agg({
            'Accuracy': ['mean', 'std'],
            'F1_Macro': ['mean', 'std'],
            'Training_Time': ['mean', 'std']
        }).round(4)
        
        f.write(avg_performance.to_markdown())
        f.write("\n\n### æ€§èƒ½æ’ååˆ†æ\n\n")
        
        # è®¡ç®—ç®€å•çš„å¹³å‡å€¼ç”¨äºæ’å
        simple_avg = results_df.groupby('Method').agg({
            'Accuracy': 'mean',
            'F1_Macro': 'mean',
            'Training_Time': 'mean'
        }).round(4)
        
        # æŒ‰å‡†ç¡®ç‡æ’åº
        acc_ranking = simple_avg.sort_values('Accuracy', ascending=False)
        f.write("#### å‡†ç¡®ç‡æ’å\n\n")
        for i, (method, row) in enumerate(acc_ranking.iterrows(), 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            f.write(f"{emoji} **{method}**: {row['Accuracy']:.2%}\n")
        f.write("\n")
        
        # æŒ‰F1åˆ†æ•°æ’åº
        f1_ranking = simple_avg.sort_values('F1_Macro', ascending=False)
        f.write("#### F1åˆ†æ•°æ’å\n\n")
        for i, (method, row) in enumerate(f1_ranking.iterrows(), 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            f.write(f"{emoji} **{method}**: {row['F1_Macro']:.2%}\n")
        f.write("\n")
        
        # æŒ‰è®­ç»ƒæ—¶é—´æ’åºï¼ˆè¶Šå¿«è¶Šå¥½ï¼‰
        time_ranking = simple_avg.sort_values('Training_Time', ascending=True)
        f.write("#### è®­ç»ƒæ•ˆç‡æ’å (è¶Šå¿«è¶Šå¥½)\n\n")
        for i, (method, row) in enumerate(time_ranking.iterrows(), 1):
            emoji = "ğŸš€" if i == 1 else "âš¡" if i == 2 else "ğŸ’¨" if i == 3 else f"{i}."
            f.write(f"{emoji} **{method}**: {row['Training_Time']:.3f}ç§’\n")
        f.write("\n")
        
        # ç»Ÿä¸€æ¶æ„æ–¹æ³•åˆ†æ
        unified_core_methods = [method for method in simple_avg.index if 'CAAC' in method or 'MLP (' in method]
        
        f.write(f"""## æ ¸å¿ƒå‘ç°ï¼šæŸ¯è¥¿åˆ†å¸ƒå°ºåº¦å‚æ•°çš„å½±å“

### ç»Ÿä¸€æ¶æ„æ–¹æ³•å¯¹æ¯”

æœ¬å®éªŒçš„æ ¸å¿ƒç›®æ ‡æ˜¯éªŒè¯**æŸ¯è¥¿åˆ†å¸ƒå°ºåº¦å‚æ•°**å¯¹åˆ†ç±»æ€§èƒ½çš„å½±å“ã€‚é€šè¿‡ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ç½‘ç»œæ¶æ„ï¼Œæˆ‘ä»¬å¯ä»¥å‡†ç¡®åˆ†æä¸åŒåˆ†å¸ƒé€‰æ‹©çš„æ•ˆæœã€‚

""")
        
        # åˆ†æç»Ÿä¸€æ¶æ„æ–¹æ³•
        unified_methods = results_df[results_df['Method_Type'] == 'unified']
        if not unified_methods.empty:
            unified_summary = unified_methods.groupby('Method').agg({
                'Accuracy': 'mean',
                'F1_Macro': 'mean',
                'Training_Time': 'mean'
            }).round(4)
            
            f.write("#### ç»Ÿä¸€æ¶æ„æ–¹æ³•æ€§èƒ½å¯¹æ¯”\n\n")
            f.write(unified_summary.to_markdown())
            f.write("\n\n")
            
            # æ‰¾å‡ºCAAC OvRçš„è¡¨ç°
            caac_performance = unified_summary.loc[unified_summary.index.str.contains('CAAC OvR', case=False)]
            if not caac_performance.empty:
                caac_acc = caac_performance.iloc[0]['Accuracy']
                caac_f1 = caac_performance.iloc[0]['F1_Macro']
                caac_time = caac_performance.iloc[0]['Training_Time']
                
                # ä¸å…¶ä»–ç»Ÿä¸€æ¶æ„æ–¹æ³•æ¯”è¾ƒ
                best_unified_acc = unified_summary['Accuracy'].max()
                best_unified_f1 = unified_summary['F1_Macro'].max()
                fastest_unified_time = unified_summary['Training_Time'].min()
                
                f.write(f"""#### æŸ¯è¥¿åˆ†å¸ƒå°ºåº¦å‚æ•°æ•ˆæœåˆ†æ

**CAAC OvR (æŸ¯è¥¿åˆ†å¸ƒ) è¡¨ç°:**
- å‡†ç¡®ç‡: {caac_acc:.2%}
- F1åˆ†æ•°: {caac_f1:.2%}  
- è®­ç»ƒæ—¶é—´: {caac_time:.3f}ç§’

**ç›¸å¯¹äºç»Ÿä¸€æ¶æ„ä¸­æœ€ä½³æ–¹æ³•:**
- å‡†ç¡®ç‡å·®è·: {(best_unified_acc - caac_acc):.2%}
- F1åˆ†æ•°å·®è·: {(best_unified_f1 - caac_f1):.2%}
- æ—¶é—´å·®è·: {caac_time - fastest_unified_time:.3f}ç§’

""")
        
        # ä¸ç»å…¸æ–¹æ³•æ¯”è¾ƒ
        sklearn_methods = results_df[results_df['Method_Type'] == 'sklearn']
        if not sklearn_methods.empty:
            sklearn_summary = sklearn_methods.groupby('Method').agg({
                'Accuracy': 'mean',
                'F1_Macro': 'mean',
                'Training_Time': 'mean'
            }).round(4)
            
            f.write("### ä¸ç»å…¸æœºå™¨å­¦ä¹ æ–¹æ³•å¯¹æ¯”\n\n")
            f.write(sklearn_summary.to_markdown())
            f.write("\n\n")
        
        # ç»“è®ºå’Œå»ºè®®
        f.write(f"""## å®éªŒç»“è®º

### ä¸»è¦å‘ç°

1. **æŸ¯è¥¿åˆ†å¸ƒå°ºåº¦å‚æ•°çš„ä½œç”¨**: 
   - åœ¨ç»Ÿä¸€æ¶æ„å®éªŒä¸­ï¼ŒæŸ¯è¥¿åˆ†å¸ƒå‚æ•°çš„æ•ˆæœéœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†ç‰¹æ€§è¿›ä¸€æ­¥åˆ†æ
   - ä¸åŒåˆ†å¸ƒé€‰æ‹©(æŸ¯è¥¿vsé«˜æ–¯)åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°æœ‰å·®å¼‚

2. **æ–¹æ³•é€‚ç”¨æ€§åˆ†æ**:
   - **é«˜å‡†ç¡®ç‡åœºæ™¯**: Random Forestå’ŒSVMè¡¨ç°çªå‡º
   - **è®­ç»ƒæ•ˆç‡åœºæ™¯**: ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•è®­ç»ƒæ›´å¿«
   - **ä¸ç¡®å®šæ€§é‡åŒ–åœºæ™¯**: CAACæ–¹æ³•æä¾›ç‹¬ç‰¹ä»·å€¼

3. **æ¶æ„è®¾è®¡éªŒè¯**:
   - ç»Ÿä¸€æ¶æ„è®¾è®¡ç¡®ä¿äº†å…¬å¹³æ¯”è¾ƒ
   - ç½‘ç»œæ·±åº¦å’Œå®½åº¦è®¾ç½®å¯¹å°æ•°æ®é›†é€‚å½“

### æ”¹è¿›å»ºè®®

**çŸ­æœŸæ”¹è¿›**:
1. è°ƒæ•´ç½‘ç»œæ¶æ„å‚æ•°ï¼Œé’ˆå¯¹ä¸åŒè§„æ¨¡æ•°æ®é›†ä¼˜åŒ–
2. å®æ–½æ›´ç²¾ç»†çš„è¶…å‚æ•°è°ƒä¼˜
3. å¢åŠ æ•°æ®å¢å¼ºæŠ€æœ¯

**é•¿æœŸå‘å±•**:
1. åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸ŠéªŒè¯æ–¹æ³•å¯æ‰©å±•æ€§  
2. æ¢ç´¢è‡ªé€‚åº”åˆ†å¸ƒé€‰æ‹©æœºåˆ¶
3. å¼€å‘å®æ—¶ä¸ç¡®å®šæ€§é‡åŒ–åº”ç”¨

### é€‚ç”¨åœºæ™¯æ¨è

**æ¨èä½¿ç”¨CAAC OvR**:
- éœ€è¦ä¸ç¡®å®šæ€§é‡åŒ–çš„å…³é”®å†³ç­–åœºæ™¯
- åŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ç­‰é«˜é£é™©åº”ç”¨
- ç ”ç©¶å’Œæ•™å­¦ä¸­çš„æ–¹æ³•è®ºéªŒè¯

**æ¨èä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•**:
- è¿½æ±‚æœ€é«˜å‡†ç¡®ç‡çš„ç«èµ›åœºæ™¯
- è®¡ç®—èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²
- å¿«é€ŸåŸå‹å¼€å‘å’Œbaselineå»ºç«‹

## å¯è§†åŒ–ç»“æœ

å®éªŒç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨åŒ…å«ï¼š
- å‡†ç¡®ç‡å¯¹æ¯”å›¾
- F1åˆ†æ•°å¯¹æ¯”å›¾  
- è®­ç»ƒæ—¶é—´å¯¹æ¯”å›¾
- æ•ˆç‡vsæ€§èƒ½æƒè¡¡æ•£ç‚¹å›¾

![æ–¹æ³•æ¯”è¾ƒå›¾](./methods_comparison_english.png)

## æ•°æ®æ–‡ä»¶

- **è¯¦ç»†ç»“æœ**: `results/methods_comparison_detailed.csv`
- **æ±‡æ€»ç»Ÿè®¡**: `results/methods_comparison_summary.csv`
- **å¯è§†åŒ–å›¾è¡¨**: `results/methods_comparison_english.png`

---

**å®éªŒé…ç½®ä¿¡æ¯**:
- Pythonç¯å¢ƒ: base condaç¯å¢ƒ
- éšæœºç§å­: 42 (ç¡®ä¿å¯é‡å¤æ€§)
- æ•°æ®åˆ†å‰²: 80%è®­ç»ƒ / 20%æµ‹è¯•
- ç‰¹å¾æ ‡å‡†åŒ–: StandardScaler
- æ—©åœç­–ç•¥: patience=10, min_delta=0.0001

*æŠ¥å‘Šç”±è‡ªåŠ¨åŒ–å®éªŒè„šæœ¬ç”Ÿæˆäº {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
""")
    
    print(f"âœ… è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    return report_file

def main():
    """Main function for CAAC OvR comparison experiment"""
    from datetime import datetime
    import os
    
    print("ğŸš€ CAAC OvR Cauchy Scale Parameter Analysis")
    print("Core Research Question: Does using Cauchy distribution scale parameters improve classification?")
    print("Unified Architecture: FeatureNet â†’ AbductionNet â†’ ActionNet")
    print("Datasets: Iris, Wine, Breast Cancer, Digits")
    print()
    
    start_time = datetime.now()
    
    # ç¡®ä¿resultsç›®å½•å­˜åœ¨
    os.makedirs('results', exist_ok=True)
    
    # Run comparison experiments
    print("ğŸ”¬ ç¬¬ä¸€æ­¥ï¼šè¿è¡Œæ‰€æœ‰æ–¹æ³•æ¯”è¾ƒå®éªŒ")
    results_df = run_comparison_experiments()
    
    # Create visualizations with English labels
    print("\nğŸ“Š ç¬¬äºŒæ­¥ï¼šç”Ÿæˆè‹±æ–‡å¯è§†åŒ–å›¾è¡¨")
    create_comparison_plots(results_df)
    
    # Create summary table
    print("\nğŸ“‹ ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºæ±‡æ€»ç»Ÿè®¡è¡¨")
    summary = create_summary_table(results_df)
    
    # Generate detailed report
    print("\nğŸ“„ ç¬¬å››æ­¥ï¼šç”Ÿæˆå®Œæ•´å®éªŒæŠ¥å‘Š")
    report_file = generate_detailed_report(results_df, summary)
    
    # è®¡ç®—æ€»è€—æ—¶
    total_time = datetime.now() - start_time
    
    print(f"\nğŸ‰ å®Œæ•´å®éªŒæµç¨‹æˆåŠŸå®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time.total_seconds():.1f}ç§’")
    print(f"ğŸ“Š è¯¦ç»†æ•°æ®: results/methods_comparison_detailed.csv")
    print(f"ğŸ“ˆ è‹±æ–‡å›¾è¡¨: results/methods_comparison_english.png")
    print(f"ğŸ“‹ æ±‡æ€»ç»Ÿè®¡: results/methods_comparison_summary.csv")
    print(f"ğŸ“„ å®Œæ•´æŠ¥å‘Š: {report_file}")
    print("\nğŸ¯ å®éªŒæŠ¥å‘ŠåŒ…å«:")
    print("   â€¢ è¯¦ç»†çš„æ–¹æ³•å¯¹æ¯”åˆ†æ")
    print("   â€¢ æŸ¯è¥¿åˆ†å¸ƒå°ºåº¦å‚æ•°æ•ˆæœéªŒè¯")
    print("   â€¢ é€‚ç”¨åœºæ™¯æ¨è")
    print("   â€¢ æ”¹è¿›å»ºè®®å’Œæœªæ¥æ–¹å‘")

if __name__ == "__main__":
    main() 