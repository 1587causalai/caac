# Fri May 30 2025 10:59:45


人事处


项目基金


简历


35岁特殊说明


芯梯科技


读书会报告


附件：



https://swarma.org/?p=29641 邓老师，这个项目资助可以写进来呀？这个资助是说明我在交叉学科方面的探索和一定影响力的证据



你好，我正在研究一个关于 $K > 2$ 类的多分类问题，并且我对如何从一些输入数值得到最终的分类结果 $Y$ 的概率分布有一些思考。

我有两种主要的方式来考虑这个问题：

1.  **第一种方式（K个输入）**：假设我有一组 $K$ 个实数值，比如 $z_1, z_2, \ldots, z_K$（这些可以看作是某个模型未经归一化的原始输出，即 logits）。我可以通过 softmax 函数 $p_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}$ 将它们转换为 $K$ 个概率 $p_1, \ldots, p_K$。然后，我的分类结果 $Y$ 就可以看作是一个服从参数为 $(p_1, \ldots, p_K)$ 的分类随机变量。

2.  **第二种方式（K-1个输入，类似ALR的思路）**：考虑到 softmax 的冗余性，我也想到，如果我只用 $K-1$ 个实数值，比如 $a_1, a_2, \ldots, a_{K-1}$。我可以将它们看作是前 $K-1$ 个类别的“相对” logits，并（隐式或显式地）设定第 $K$ 个类别的 logit 为 0（或者某个固定的参考值）。然后，我同样对这组 $(a_1, \ldots, a_{K-1}, 0)$ 应用 softmax 函数，得到 $K$ 个概率 $p'_1, \ldots, p'_K$，进而得到分类结果 $Y$。这种方法似乎与成分数据分析中的ALR（Additive Log-Ratio）变换的逆过程（或者说，ALR变换是从概率到这 $K-1$ 个值的过程）有关。

我的直觉是，第一种方式（使用 $K$ 个输入）由于 softmax 的平移不变性（所有 logits 加同一个常数，概率不变），导致从 logits 到概率的映射是多对一的，因此这个映射是**不可逆的**——也就是说，我们无法从概率唯一地反推出原始的 $K$ 个 logits。
而第二种方式（使用 $K-1$ 个输入并固定一个参考）似乎通过消除这种冗余，建立了一个从 $\mathbb{R}^{K-1}$ 到概率单纯形内部的**可逆映射**。你能否确认我的这个理解是否准确，并更清晰地阐述一下它们在这方面的差异？

**我的问题更进一步，涉及到当这些输入数值本身是随机变量时的情况。**
假设我采用第二种方法，即我输入的 $K-1$ 个数值是一个 $K-1$ 维的随机向量 $X = (X_1, \ldots, X_{K-1})$。例如，我们可以假设 $X$ 服从一个多元高斯分布。
在这种情况下：
1.  通过 softmax（以0为参考logit）作用于 $X$ 后得到的概率向量 $P(X) = (P_1(X), \ldots, P_K(X))$ 本身会服从什么分布？（我们讨论过，如果 $X$ 是高斯分布，那么 $P(X)$ 会是逻辑斯谛正态分布）。
2.  更重要的是，我最终关心的分类结果 $Y$（它在给定 $P(X)$ 的条件下服从分类分布）的**边际概率 $P(Y=i)$ 会是什么样的？**我们知道 $P(Y=i) = E_X[P_i(X)]$。

**我的核心追求，也是最让我困惑的一点是：我们能否找到一种输入随机向量 $X$ 的分布，某一种(ALR or softmax or others)方式计算概率 with its realization x， 使得最终的边际概率 $P(Y=i)$ 能够有一个简洁的、封闭的解析表达式？**

我想到一个方案何标准分布（如正态分布、指数分布等）并将其截断到 [0,1] 区间，然后使用 Stick-Breaking 构建概率

我希望你能围绕这些问题，特别是最后一个关于如何获得 $P(Y=i)$ 解析表达式的探索，给出你的见解。谢谢！