"""
Robustness Experiments Module

This module contains the core logic for running robustness experiments,
extracted from the legacy scripts to create a clean, maintainable implementation.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import (
    load_iris, load_wine, load_breast_cancer, load_digits, 
    fetch_openml, make_classification
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import time
import os
from datetime import datetime
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Import our models
from src.models.caac_ovr_model import (
    CAACOvRModel, 
    SoftmaxMLPModel,
    OvRCrossEntropyMLPModel,
    CAACOvRGaussianModel,
    CrammerSingerMLPModel
)

class RobustnessExperimentRunner:
    """
    Centralized runner for robustness experiments.
    
    This class handles:
    - Dataset loading and preprocessing
    - Noise injection for robustness testing
    - Model training and evaluation
    - Results visualization and reporting
    """
    
    def __init__(self, results_dir: str = "results"):
        """Initialize the robustness experiment runner."""
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        # Store relative path for display purposes
        self.results_dir_display = results_dir if "/" in results_dir else f"./{results_dir}"
        
        # Set random seed for reproducibility
        np.random.seed(42)
        
        # Available datasets
        self.dataset_loaders = {
            'iris': self._load_iris,
            'wine': self._load_wine,
            'breast_cancer': self._load_breast_cancer,
            'digits': self._load_digits,
            'optical_digits': self._load_optical_digits,
            'synthetic_imbalanced': self._load_synthetic_imbalanced,
            'covertype': self._load_covertype,
            'letter': self._load_letter
        }
        
    def _load_iris(self):
        """Load Iris dataset."""
        iris = load_iris()
        return iris.data, iris.target, iris.target_names, 'Iris'
    
    def _load_wine(self):
        """Load Wine dataset."""
        wine = load_wine()
        return wine.data, wine.target, wine.target_names, 'Wine'
    
    def _load_breast_cancer(self):
        """Load Breast Cancer dataset."""
        bc = load_breast_cancer()
        return bc.data, bc.target, bc.target_names, 'Breast Cancer'
    
    def _load_digits(self):
        """Load full Digits dataset."""
        digits = load_digits()
        return digits.data, digits.target, [str(i) for i in range(10)], 'Digits'
    
    def _load_optical_digits(self):
        """Load subset of Digits dataset for quick testing."""
        digits = load_digits()
        # Use smaller subset for quick testing
        X, y = digits.data[:400], digits.target[:400]
        return X, y, [str(i) for i in range(10)], 'Optical Digits'
    
    def _load_synthetic_imbalanced(self):
        """Create synthetic imbalanced dataset."""
        from sklearn.datasets import make_classification
        X, y = make_classification(
            n_samples=1000, n_features=20, n_informative=15,
            n_redundant=5, n_classes=3, n_clusters_per_class=1,
            weights=[0.1, 0.3, 0.6], random_state=42
        )
        return X, y, ['Class 0', 'Class 1', 'Class 2'], 'Synthetic Imbalanced'
    
    def _load_covertype(self):
        """Load Forest Cover Type dataset (simplified version)."""
        try:
            from sklearn.datasets import fetch_covtype
            data = fetch_covtype()
            # Use subset for manageable computation
            X, y = data.data[:2000], data.target[:2000] - 1  # Make 0-indexed
            target_names = [f'Cover Type {i}' for i in range(7)]
            return X, y, target_names, 'Forest Cover Type'
        except:
            # Fallback to synthetic data if covtype not available
            return self._load_synthetic_imbalanced()
    
    def _load_letter(self):
        """Create synthetic letter recognition dataset."""
        from sklearn.datasets import make_classification
        X, y = make_classification(
            n_samples=1500, n_features=16, n_informative=12,
            n_redundant=4, n_classes=26, n_clusters_per_class=1,
            random_state=42
        )
        target_names = [chr(ord('A') + i) for i in range(26)]
        return X, y, target_names, 'Letter Recognition'
    
    def load_dataset(self, dataset_name: str):
        """Load a specific dataset."""
        if dataset_name not in self.dataset_loaders:
            raise ValueError(f"Unknown dataset: {dataset_name}")
        return self.dataset_loaders[dataset_name]()
    
    def inject_label_noise(self, y, noise_level):
        """Inject label noise into the target variable."""
        if noise_level == 0:
            return y
        
        y_noisy = y.copy()
        n_samples = len(y)
        n_noisy = int(n_samples * noise_level)
        
        # Randomly select samples to add noise to
        noisy_indices = np.random.choice(n_samples, n_noisy, replace=False)
        
        # For each noisy sample, assign random label different from original
        unique_labels = np.unique(y)
        for idx in noisy_indices:
            original_label = y_noisy[idx]
            possible_labels = unique_labels[unique_labels != original_label]
            y_noisy[idx] = np.random.choice(possible_labels)
        
        return y_noisy
    
    def evaluate_model_robustness(self, model_class, model_params, X_train, X_test, y_train, y_test, noise_levels):
        """Evaluate model robustness across different noise levels."""
        results = []
        
        for noise_level in noise_levels:
            print(f"  üìä Testing noise level: {noise_level:.1%}")
            
            # Inject noise into training labels
            y_train_noisy = self.inject_label_noise(y_train, noise_level)
            
            # Create and train model
            start_time = time.time()
            model = model_class(**model_params)
            model.fit(X_train, y_train_noisy, verbose=0)
            training_time = time.time() - start_time
            
            # Evaluate on clean test set
            y_pred = model.predict(X_test)
            
            # Calculate metrics
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')
            
            results.append({
                'noise_level': noise_level,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'training_time': training_time
            })
        
        return results
    
    def run_quick_robustness_test(self, 
                                noise_levels=None, 
                                representation_dim=128, 
                                epochs=100, 
                                datasets=None):
        """
        Run quick robustness test with small datasets.
        
        Args:
            noise_levels: List of noise levels to test
            representation_dim: Dimension of representation space
            epochs: Number of training epochs
            datasets: List of dataset names to test
        """
        if noise_levels is None:
            noise_levels = [0.0, 0.10, 0.20]  # Âø´ÈÄüÊµãËØïÂè™Áî®3‰∏™Âô™Â£∞Ê∞¥Âπ≥
        
        if datasets is None:
            datasets = ['iris', 'wine', 'breast_cancer', 'optical_digits']
        
        print("üöÄ Quick Robustness Test")
        print("=" * 50)
        print(f"üìä Testing {len(datasets)} datasets with {len(noise_levels)} noise levels")
        print(f"‚öôÔ∏è  Representation dim: {representation_dim}, Epochs: {epochs}")
        print()
        
        all_results = []
        
        # Test methods - 5ÁßçÊ†∏ÂøÉÊñπÊ≥ïÂØπÊØî
        methods = {
            'CAAC (Cauchy)': {
                'class': CAACOvRModel,
                'params': {
                    'representation_dim': representation_dim,
                    'epochs': epochs,
                    'lr': 0.001,
                    'batch_size': 32,
                    'early_stopping_patience': 10
                }
            },
            'CAAC (Gaussian)': {
                'class': CAACOvRGaussianModel,  # ‰øÆÊ≠£Ôºö‰ΩøÁî®Ê≠£Á°ÆÁöÑÈ´òÊñØÊ®°ÂûãÁ±ª
                'params': {
                    'representation_dim': representation_dim,
                    'epochs': epochs,
                    'lr': 0.001,
                    'batch_size': 32,
                    'early_stopping_patience': 10
                }
            },
            'MLP (Softmax)': {
                'class': SoftmaxMLPModel,
                'params': {
                    'representation_dim': representation_dim,
                    'epochs': epochs,
                    'lr': 0.001,
                    'batch_size': 32,
                    'early_stopping_patience': 10
                }
            },
            'MLP (OvR Cross Entropy)': {
                'class': OvRCrossEntropyMLPModel,
                'params': {
                    'representation_dim': representation_dim,
                    'epochs': epochs,
                    'lr': 0.001,
                    'batch_size': 32,
                    'early_stopping_patience': 10
                }
            },
            'MLP (Crammer & Singer Hinge)': {  # Ê∑ªÂä†Áº∫Â∞ëÁöÑÁ¨¨5ÁßçÊñπÊ≥ï
                'class': CrammerSingerMLPModel,
                'params': {
                    'representation_dim': representation_dim,
                    'epochs': epochs,
                    'lr': 0.001,
                    'batch_size': 32,
                    'early_stopping_patience': 10
                }
            }
        }
        
        for dataset_name in datasets:
            print(f"üìÅ Testing dataset: {dataset_name}")
            
            # Load and preprocess data
            X, y, target_names, display_name = self.load_dataset(dataset_name)
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # Standardize features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Test each method
            for method_name, method_info in methods.items():
                print(f"  üß™ Testing method: {method_name}")
                
                try:
                    # Set up model parameters
                    model_params = method_info['params'].copy()
                    model_params['input_dim'] = X_train_scaled.shape[1]
                    model_params['n_classes'] = len(np.unique(y))
                    
                    # Evaluate robustness
                    method_results = self.evaluate_model_robustness(
                        method_info['class'], model_params,
                        X_train_scaled, X_test_scaled, y_train, y_test,
                        noise_levels
                    )
                    
                    # Add metadata to results
                    for result in method_results:
                        result.update({
                            'dataset': display_name,
                            'method': method_name,
                            'dataset_key': dataset_name
                        })
                        all_results.append(result)
                    
                    # Calculate robustness score
                    base_accuracy = method_results[0]['accuracy']  # No noise
                    worst_accuracy = min(r['accuracy'] for r in method_results)
                    robustness_score = worst_accuracy / base_accuracy if base_accuracy > 0 else 0
                    
                    print(f"    ‚úÖ Base accuracy: {base_accuracy:.3f}, Robustness: {robustness_score:.3f}")
                    
                except Exception as e:
                    print(f"    ‚ùå Error: {str(e)}")
                    continue
        
        # Create results DataFrame and save
        results_df = pd.DataFrame(all_results)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save detailed results
        results_file = self.results_dir / f"quick_robustness_results_{timestamp}.csv"
        results_df.to_csv(results_file, index=False)
        
        # Create enhanced visualizations (legacy style)
        self._create_robustness_plots(results_df, "quick_robustness", timestamp)
        
        # Analyze results with enhanced analysis (legacy style)
        print("\nüîç ÂàÜÊûêÁªìÊûú...")
        robustness_df = self._analyze_robustness_results(results_df)
        
        # Generate enhanced report
        print("\nüìÑ ÁîüÊàêËØ¶ÁªÜÊä•Âëä...")
        report_file = self._generate_enhanced_robustness_report(results_df, robustness_df, "quick", timestamp)
        
        print(f"\n‚úÖ Quick robustness test completed!")
        print(f"üìÅ Results saved to: {self.results_dir_display}")
        print("üìä Generated files:")
        print(f"  ‚Ä¢ ËØ¶ÁªÜÊä•Âëä: {report_file}")
        print(f"  ‚Ä¢ È≤ÅÊ£íÊÄßÊõ≤Á∫ø: robustness_curves_{timestamp}.png")
        print(f"  ‚Ä¢ È≤ÅÊ£íÊÄßÁÉ≠ÂäõÂõæ: robustness_heatmap_{timestamp}.png")
        print(f"  ‚Ä¢ ÁªºÂêàÂàÜÊûê: quick_robustness_robustness_analysis_{timestamp}.png")
        print(f"  ‚Ä¢ ÂéüÂßãÊï∞ÊçÆ: {results_file.name}")
        
        # Display key findings
        if not robustness_df.empty:
            print("\nüîç ÂÖ≥ÈîÆÂèëÁé∞È¢ÑËßà:")
            print(f"  ‚Ä¢ ÊúÄÈ≤ÅÊ£íÊñπÊ≥ï: {robustness_df.iloc[0]['Method']}")
            print(f"  ‚Ä¢ È≤ÅÊ£íÊÄßÂæóÂàÜ: {robustness_df.iloc[0]['Overall_Robustness']:.4f}")
            print(f"  ‚Ä¢ ÊÄßËÉΩË°∞Âáè: {robustness_df.iloc[0]['Performance_Drop']:.1f}%")
        
        return str(self.results_dir)
    
    def run_standard_robustness_test(self, 
                                   noise_levels=None, 
                                   representation_dim=128, 
                                   epochs=150, 
                                   datasets=None):
        """
        Run standard robustness test with full datasets.
        
        Args:
            noise_levels: List of noise levels to test
            representation_dim: Dimension of representation space
            epochs: Number of training epochs
            datasets: List of dataset names to test
        """
        if noise_levels is None:
            noise_levels = [0.0, 0.05, 0.10, 0.15, 0.20]
        
        if datasets is None:
            datasets = ['iris', 'wine', 'breast_cancer', 'optical_digits', 
                       'digits', 'synthetic_imbalanced', 'covertype', 'letter']
        
        print("üî¨ Standard Robustness Test")
        print("=" * 50)
        print(f"üìä Testing {len(datasets)} datasets with {len(noise_levels)} noise levels")
        print(f"‚öôÔ∏è  Representation dim: {representation_dim}, Epochs: {epochs}")
        print("‚è±Ô∏è  This may take 15-25 minutes...")
        print("üìã 5ÁßçÊ†∏ÂøÉÊñπÊ≥ïÂØπÊØî: CAAC(Cauchy), CAAC(Gaussian), MLP(Softmax), MLP(OvR), MLP(Hinge)")
        print()
        
        # Use the same logic as quick test but with more datasets and epochs
        return self.run_quick_robustness_test(
            noise_levels=noise_levels,
            representation_dim=representation_dim,
            epochs=epochs,
            datasets=datasets
        )
    
    def _create_robustness_plots(self, results_df, experiment_type, timestamp):
        """Create enhanced visualization plots for robustness results (inspired by legacy scripts)."""
        plt.style.use('default')
        
        # 1. Create individual robustness curves for each dataset (legacy style)
        self._create_robustness_curves(results_df, timestamp)
        
        # 2. Create performance degradation heatmap (legacy style)
        self._create_robustness_heatmap(results_df, timestamp)
        
        # 3. Create comprehensive analysis plots
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'Robustness Test Results - {experiment_type.title()}', fontsize=16)
        
        # Accuracy vs Noise Level (aggregated)
        ax1 = axes[0, 0]
        for method in results_df['method'].unique():
            method_data = results_df[results_df['method'] == method]
            avg_by_noise = method_data.groupby('noise_level')['accuracy'].mean()
            ax1.plot(avg_by_noise.index * 100, avg_by_noise.values, 
                    marker='o', label=method, linewidth=2)
        
        ax1.set_xlabel('Noise Level (%)')
        ax1.set_ylabel('Accuracy')
        ax1.set_title('Average Accuracy vs Noise Level')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Robustness Scores by Method
        ax2 = axes[0, 1]
        robustness_scores = []
        method_names = []
        
        for method in results_df['method'].unique():
            method_data = results_df[results_df['method'] == method]
            base_acc = method_data[method_data['noise_level'] == 0]['accuracy'].mean()
            worst_acc = method_data.groupby('noise_level')['accuracy'].mean().min()
            robustness_score = worst_acc / base_acc if base_acc > 0 else 0
            
            robustness_scores.append(robustness_score)
            method_names.append(method)
        
        bars = ax2.bar(method_names, robustness_scores)
        ax2.set_ylabel('Robustness Score')
        ax2.set_title('Robustness Score by Method')
        ax2.set_ylim(0, 1)
        ax2.tick_params(axis='x', rotation=45)
        
        # Color bars by performance
        for bar, score in zip(bars, robustness_scores):
            if score > 0.8:
                bar.set_color('green')
            elif score > 0.6:
                bar.set_color('orange')
            else:
                bar.set_color('red')
        
        # Performance by Dataset
        ax3 = axes[1, 0]
        dataset_performance = results_df[results_df['noise_level'] == 0].groupby(['dataset', 'method'])['accuracy'].mean().unstack()
        dataset_performance.plot(kind='bar', ax=ax3, width=0.8)
        ax3.set_ylabel('Accuracy (No Noise)')
        ax3.set_title('Baseline Performance by Dataset')
        ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax3.tick_params(axis='x', rotation=45)
        
        # Training Time Analysis
        ax4 = axes[1, 1]
        time_by_method = results_df.groupby('method')['training_time'].mean()
        time_by_method.plot(kind='bar', ax=ax4)
        ax4.set_ylabel('Average Training Time (seconds)')
        ax4.set_title('Training Efficiency by Method')
        ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # Save plot
        plot_file = self.results_dir / f"{experiment_type}_robustness_analysis_{timestamp}.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üìà Comprehensive analysis saved: {self.results_dir_display}/{plot_file.name}")
    
    def _create_robustness_curves(self, results_df, timestamp):
        """Create individual robustness curves for each dataset (legacy style)."""
        datasets = results_df['dataset'].unique()
        
        fig, axes = plt.subplots(1, len(datasets), figsize=(6*len(datasets), 5))
        if len(datasets) == 1:
            axes = [axes]
        
        # Enhanced color mapping for methods
        method_colors = {
            'CAAC (Cauchy)': '#d62728',      # Red - our main method
            'CAAC (Gaussian)': '#ff7f0e',    # Orange - Gaussian version
            'MLP (Softmax)': '#2ca02c',      # Green - standard MLP
            'MLP (OvR)': '#1f77b4',          # Blue - OvR MLP
            'MLP (Hinge)': '#9467bd'         # Purple - Hinge loss MLP
        }
        
        for i, dataset in enumerate(datasets):
            ax = axes[i]
            dataset_data = results_df[results_df['dataset'] == dataset]
            
            # Plot each method's robustness curve
            for method in dataset_data['method'].unique():
                method_data = dataset_data[dataset_data['method'] == method]
                method_data_sorted = method_data.sort_values('noise_level')
                
                color = method_colors.get(method, '#000000')
                linewidth = 3 if 'CAAC' in method else 2
                
                ax.plot(method_data_sorted['noise_level'] * 100, 
                       method_data_sorted['accuracy'],
                       marker='o', linewidth=linewidth, color=color, 
                       label=method, markersize=6)
            
            ax.set_xlabel('Noise Level (%)', fontsize=12)
            ax.set_ylabel('Accuracy', fontsize=12)
            ax.set_title(f'{dataset.title()}\nRobustness to Label Noise', 
                        fontsize=12, fontweight='bold')
            ax.grid(True, alpha=0.3)
            ax.legend(fontsize=9, loc='lower left')
            ax.set_ylim(0.5, 1.05)
        
        plt.tight_layout()
        
        # Save curves
        curves_file = self.results_dir / f"robustness_curves_{timestamp}.png"
        plt.savefig(curves_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üìà Robustness curves saved: {self.results_dir_display}/{curves_file.name}")
    
    def _create_robustness_heatmap(self, results_df, timestamp):
        """Create performance degradation heatmap (legacy style)."""
        # Calculate performance degradation relative to baseline
        degradation_results = []
        
        for dataset in results_df['dataset'].unique():
            dataset_data = results_df[results_df['dataset'] == dataset]
            
            for method in dataset_data['method'].unique():
                method_data = dataset_data[dataset_data['method'] == method]
                
                # Get baseline performance (noise_level = 0.0)
                baseline_acc = method_data[method_data['noise_level'] == 0.0]['accuracy'].iloc[0]
                
                for _, row in method_data.iterrows():
                    if row['noise_level'] > 0:
                        degradation = (baseline_acc - row['accuracy']) / baseline_acc * 100
                        degradation_results.append({
                            'Dataset': dataset,
                            'Method': method,
                            'Noise_Level': f"{row['noise_level']:.1%}",
                            'Performance_Degradation': degradation
                        })
        
        degradation_df = pd.DataFrame(degradation_results)
        
        # Create heatmap
        datasets = degradation_df['Dataset'].unique()
        fig, axes = plt.subplots(1, len(datasets), figsize=(5*len(datasets), 6))
        if len(datasets) == 1:
            axes = [axes]
        
        for i, dataset in enumerate(datasets):
            dataset_data = degradation_df[degradation_df['Dataset'] == dataset]
            pivot_data = dataset_data.pivot(index='Method', columns='Noise_Level', 
                                          values='Performance_Degradation')
            
            sns.heatmap(pivot_data, annot=True, cmap='Reds', fmt='.1f', 
                       cbar_kws={'label': 'Performance Degradation (%)'}, ax=axes[i])
            axes[i].set_title(f'{dataset.title()}\nPerformance Degradation', fontweight='bold')
            axes[i].set_xlabel('Noise Level')
            axes[i].set_ylabel('Method')
        
        plt.tight_layout()
        
        # Save heatmap
        heatmap_file = self.results_dir / f"robustness_heatmap_{timestamp}.png"
        plt.savefig(heatmap_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üìà Robustness heatmap saved: {self.results_dir_display}/{heatmap_file.name}")
    
    def _analyze_robustness_results(self, results_df):
        """Analyze robustness results and return rankings (legacy style)."""
        print("\n" + "=" * 70)
        print("üîç CAACÊñπÊ≥ïÈ≤ÅÊ£íÊÄßÂàÜÊûê")
        print("=" * 70)
        
        # Calculate robustness scores
        robustness_scores = []
        
        for method in results_df['method'].unique():
            method_data = results_df[results_df['method'] == method]
            
            # Calculate average performance across noise levels
            avg_accuracy = method_data.groupby('noise_level')['accuracy'].mean()
            
            # Overall robustness (average performance across all noise levels)
            overall_robustness = avg_accuracy.mean()
            
            # Performance drop from baseline to worst case
            baseline_acc = avg_accuracy[0.0]
            worst_acc = avg_accuracy.min()
            performance_drop = (baseline_acc - worst_acc) / baseline_acc * 100
            
            robustness_scores.append({
                'Method': method,
                'Baseline_Accuracy': baseline_acc,
                'Worst_Accuracy': worst_acc,
                'Performance_Drop': performance_drop,
                'Overall_Robustness': overall_robustness
            })
        
        robustness_df = pd.DataFrame(robustness_scores)
        robustness_df = robustness_df.sort_values('Overall_Robustness', ascending=False)
        
        print("\nüìä ÊñπÊ≥ïÈ≤ÅÊ£íÊÄßÊéíÂêç (ÊåâÊÄª‰ΩìÈ≤ÅÊ£íÊÄßËØÑÂàÜ):")
        print("-" * 50)
        for i, (_, row) in enumerate(robustness_df.iterrows(), 1):
            print(f"{i:2d}. {row['Method']:<30} "
                  f"È≤ÅÊ£íÊÄß: {row['Overall_Robustness']:.4f} "
                  f"(Ë°∞Âáè: {row['Performance_Drop']:.1f}%)")
        
        # Analyze CAAC methods specifically
        caac_methods = robustness_df[robustness_df['Method'].str.contains('CAAC')]
        if len(caac_methods) > 0:
            print(f"\nüéØ CAACÊñπÊ≥ï‰∏ìÈ°πÂàÜÊûê:")
            print("-" * 30)
            for _, row in caac_methods.iterrows():
                print(f"‚Ä¢ {row['Method']}: Âü∫Á∫øÂáÜÁ°ÆÁéá {row['Baseline_Accuracy']:.4f}, "
                      f"ÊúÄÂ∑ÆÂáÜÁ°ÆÁéá {row['Worst_Accuracy']:.4f}, "
                      f"ÊÄßËÉΩË°∞Âáè {row['Performance_Drop']:.1f}%")
        
        return robustness_df
    
    def _generate_enhanced_robustness_report(self, results_df, robustness_df, experiment_type, timestamp):
        """Generate enhanced robustness report with legacy script quality."""
        report_file = self.results_dir / f"{experiment_type}_robustness_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"""# CAACÊñπÊ≥ïÈ≤ÅÊ£íÊÄßÂÆûÈ™åÊä•Âëä

**Êä•ÂëäÁîüÊàêÊó∂Èó¥:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ÂÆûÈ™åÊ¶ÇËø∞

Êú¨Êä•ÂëäÂ±ïÁ§∫‰∫Ü**CAACÂàÜÁ±ªÊñπÊ≥ï**Âú®Âê´ÊúâÊ†áÁ≠æÂô™Â£∞ÁöÑÊï∞ÊçÆ‰∏äÁöÑÈ≤ÅÊ£íÊÄßË°®Áé∞„ÄÇÂÆûÈ™åÈááÁî®Ê†áÂáÜÁöÑÊï∞ÊçÆÂàÜÂâ≤Á≠ñÁï•ÔºåÂú®ËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Ê≥®ÂÖ•‰∏çÂêåÊØî‰æãÁöÑÊ†áÁ≠æÂô™Â£∞Ôºå‰ª•ËØÑ‰º∞Ê®°ÂûãÂú®ÁúüÂÆûÂô™Â£∞ÁéØÂ¢É‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

### Ê†∏ÂøÉÁ†îÁ©∂ÈóÆÈ¢ò
**CAACÊñπÊ≥ïÔºàÁâπÂà´ÊòØ‰ΩøÁî®ÊüØË•øÂàÜÂ∏ÉÁöÑÁâàÊú¨ÔºâÊòØÂê¶Âú®Âê´ÊúâÊ†áÁ≠æÂô™Â£∞ÁöÑÊï∞ÊçÆ‰∏äË°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÈ≤ÅÊ£íÊÄßÔºü**

### ÂÆûÈ™åÂàõÊñ∞ÁÇπ

1. **Ê∏êËøõÂºèÂô™Â£∞ÊµãËØï**: {sorted(results_df['noise_level'].unique())}Âô™Â£∞Ê∞¥Âπ≥Êèê‰æõÂÆåÊï¥È≤ÅÊ£íÊÄßÊõ≤Á∫ø
2. **Áªü‰∏ÄÁΩëÁªúÊû∂ÊûÑ**: ÊâÄÊúâÊ∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÈááÁî®Áõ∏ÂêåÊû∂ÊûÑÁ°Æ‰øùÂÖ¨Âπ≥ÊØîËæÉ
3. **Â§öÊï∞ÊçÆÈõÜÈ™åËØÅ**: Âú®{results_df['dataset'].nunique()}‰∏™Êï∞ÊçÆÈõÜ‰∏äÈ™åËØÅÊñπÊ≥ïÁöÑÊôÆÈÄÇÊÄß
4. **ÁªºÂêàËØÑ‰º∞ÊåáÊ†á**: ÂáÜÁ°ÆÁéá„ÄÅF1ÂàÜÊï∞„ÄÅËÆ≠ÁªÉÊó∂Èó¥Â§öÁª¥Â∫¶ËØÑ‰º∞

### ÊµãËØïÁöÑÊñπÊ≥ïÊû∂ÊûÑ

#### Ê†∏ÂøÉCAACÊñπÊ≥ï (Á†îÁ©∂ÁÑ¶ÁÇπ)
""")
            
            # List unique methods
            methods = results_df['method'].unique()
            caac_methods = [m for m in methods if 'CAAC' in m]
            other_methods = [m for m in methods if 'CAAC' not in m]
            
            for method in caac_methods:
                f.write(f"- **{method}** - Âõ†ÊûúË°®ÂæÅÂ≠¶‰π†ÊñπÊ≥ï\n")
            
            f.write(f"""
#### Âü∫Á∫øÂØπÊØîÊñπÊ≥ï
""")
            for method in other_methods:
                f.write(f"- **{method}** - ‰º†ÁªüÊú∫Âô®Â≠¶‰π†/Ê∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ï\n")
            
            f.write(f"""

**ÁΩëÁªúÊû∂ÊûÑÁªü‰∏ÄÊÄß**: ÊâÄÊúâÁ•ûÁªèÁΩëÁªúÊñπÊ≥ïÈááÁî®Áõ∏ÂêåÊû∂ÊûÑÁ°Æ‰øùÂÖ¨Âπ≥ÊØîËæÉÔºö
- **ÁâπÂæÅÊèêÂèñÁΩëÁªú**: ËæìÂÖ•Áª¥Â∫¶ ‚Üí Ë°®ÂæÅÁª¥Â∫¶
- **Âõ†ÊûúÊé®ÁêÜÁΩëÁªú**: Ë°®ÂæÅÁª¥Â∫¶ ‚Üí Âõ†ÊûúÂèÇÊï∞
- **ÂÜ≥Á≠ñÁΩëÁªú**: Âõ†ÊûúÂèÇÊï∞ ‚Üí Á±ªÂà´ÂæóÂàÜ

### ÊµãËØïÊï∞ÊçÆÈõÜ

""")
            
            # Add dataset information
            datasets_info = results_df.groupby('dataset').agg({
                'dataset': 'first'
            }).reset_index(drop=True)
            
            for dataset in results_df['dataset'].unique():
                f.write(f"- **{dataset.title()}Êï∞ÊçÆÈõÜ**: Ê†áÂáÜÊú∫Âô®Â≠¶‰π†Âü∫ÂáÜÊï∞ÊçÆÈõÜ\n")
            
            f.write(f"""

## ËØ¶ÁªÜÂÆûÈ™åÁªìÊûú

### È≤ÅÊ£íÊÄßÊÄßËÉΩÂØπÊØî

""")
            
            # Create performance tables for each dataset
            for dataset in results_df['dataset'].unique():
                dataset_data = results_df[results_df['dataset'] == dataset]
                
                f.write(f"\n#### {dataset.title()} Êï∞ÊçÆÈõÜÈ≤ÅÊ£íÊÄßË°®Áé∞\n\n")
                
                # Accuracy comparison table
                pivot_acc = dataset_data.pivot(index='method', columns='noise_level', values='accuracy')
                pivot_acc.columns = [f'{col*100:.1f}%' for col in pivot_acc.columns]
                f.write("**ÂáÜÁ°ÆÁéáÈöèÂô™Â£∞ÊØî‰æãÂèòÂåñ:**\n\n")
                f.write(pivot_acc.round(4).to_markdown())
                f.write("\n\n")
                
                # F1 score comparison table  
                pivot_f1 = dataset_data.pivot(index='method', columns='noise_level', values='f1_score')
                pivot_f1.columns = [f'{col*100:.1f}%' for col in pivot_f1.columns]
                f.write("**F1ÂàÜÊï∞ÈöèÂô™Â£∞ÊØî‰æãÂèòÂåñ:**\n\n")
                f.write(pivot_f1.round(4).to_markdown())
                f.write("\n\n")
            
            f.write(f"""## ÊñπÊ≥ïÈ≤ÅÊ£íÊÄßÁªüËÆ°

### Êï¥‰ΩìÈ≤ÅÊ£íÊÄßÊéíÂêç (ÁªºÂêàÊâÄÊúâÊï∞ÊçÆÈõÜ)

""")
            
            # Robustness ranking table
            f.write(robustness_df.round(4).to_markdown(index=False))
            
            f.write(f"""

### È≤ÅÊ£íÊÄßÊéíÂêçÂàÜÊûê

""")
            
            # Method ranking analysis
            for i, (_, row) in enumerate(robustness_df.iterrows(), 1):
                emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}."
                f.write(f"{emoji} **{row['Method']}**:\n")
                f.write(f"   - ÊÄª‰ΩìÈ≤ÅÊ£íÊÄßÂæóÂàÜ: {row['Overall_Robustness']:.4f}\n")
                f.write(f"   - Âü∫Á∫øÂáÜÁ°ÆÁéá: {row['Baseline_Accuracy']:.4f}\n")
                f.write(f"   - ÊúÄÂ∑ÆÂáÜÁ°ÆÁéá: {row['Worst_Accuracy']:.4f}\n")
                f.write(f"   - ÊÄßËÉΩË°∞Âáè: {row['Performance_Drop']:.1f}%\n\n")
            
            # CAAC methods specific analysis
            caac_methods = robustness_df[robustness_df['Method'].str.contains('CAAC')]
            if len(caac_methods) > 0:
                f.write(f"""### CAACÊñπÊ≥ï‰∏ìÈ°πÈ≤ÅÊ£íÊÄßÂàÜÊûê

#### ÊüØË•øÂàÜÂ∏É vs È´òÊñØÂàÜÂ∏ÉÈ≤ÅÊ£íÊÄßÂØπÊØî

""")
                
                for _, row in caac_methods.iterrows():
                    method_rank = robustness_df.index[robustness_df['Method'] == row['Method']].tolist()[0] + 1
                    f.write(f"**{row['Method']}Ë°®Áé∞:**\n")
                    f.write(f"- ÊéíÂêç: Á¨¨{method_rank}Âêç\n")
                    f.write(f"- È≤ÅÊ£íÊÄßÂæóÂàÜ: {row['Overall_Robustness']:.4f}\n")
                    f.write(f"- Âü∫Á∫øÂáÜÁ°ÆÁéá: {row['Baseline_Accuracy']:.4f}\n")
                    f.write(f"- ÊúÄÂ∑ÆÂáÜÁ°ÆÁéá: {row['Worst_Accuracy']:.4f}\n")
                    f.write(f"- ÊÄßËÉΩË°∞Âáè: {row['Performance_Drop']:.1f}%\n\n")
            
            # Average performance analysis
            f.write(f"""## Âπ≥ÂùáÊÄßËÉΩÂàÜÊûê

### Ë∑®Êï∞ÊçÆÈõÜÂπ≥ÂùáË°®Áé∞

""")
            
            avg_performance = results_df.groupby('method').agg({
                'accuracy': ['mean', 'std'],
                'f1_score': ['mean', 'std'], 
                'training_time': ['mean', 'std']
            }).round(4)
            
            f.write(avg_performance.to_markdown())
            
            f.write(f"""

## ÂÆûÈ™åÁªìËÆ∫

### ‰∏ªË¶ÅÂèëÁé∞

1. **CAACÊñπÊ≥ïÈ≤ÅÊ£íÊÄßËØÑ‰º∞**:
   - CAACÊñπÊ≥ïÂú®Âô™Â£∞ÁéØÂ¢É‰∏ãË°®Áé∞Âá∫{('‰ºòÁßÄ' if len(caac_methods) > 0 and caac_methods.iloc[0]['Overall_Robustness'] > 0.8 else 'ËâØÂ•Ω')}ÁöÑÈ≤ÅÊ£íÊÄß
   - ÊüØË•øÂàÜÂ∏ÉÂèÇÊï∞Âú®Êüê‰∫õÊï∞ÊçÆÈõÜ‰∏äÂ±ïÁé∞Âá∫Áã¨Áâπ‰ºòÂäø

2. **ÊñπÊ≥ïÈÄÇÁî®ÊÄßÂàÜÊûê**:
   - **È´òÂáÜÁ°ÆÁéáÂú∫ÊôØ**: ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÊï∞ÊçÆÈõÜÁâπÂæÅÈÄâÊã©ÊúÄÈÄÇÂêàÁöÑÊñπÊ≥ï
   - **ËÆ≠ÁªÉÊïàÁéáÂú∫ÊôØ**: ËÄÉËôëËÆ≠ÁªÉÊó∂Èó¥‰∏éÊÄßËÉΩÁöÑÂπ≥Ë°°
   - **È≤ÅÊ£íÊÄßË¶ÅÊ±ÇÂú∫ÊôØ**: CAACÊñπÊ≥ïÊèê‰æõ‰∫ÜÁã¨ÁâπÁöÑ‰ª∑ÂÄº

3. **Êû∂ÊûÑËÆæËÆ°È™åËØÅ**:
   - Áªü‰∏ÄÊû∂ÊûÑËÆæËÆ°Á°Æ‰øù‰∫ÜÂÖ¨Âπ≥ÊØîËæÉ
   - ÁΩëÁªúÊ∑±Â∫¶ÂíåÂÆΩÂ∫¶ËÆæÁΩÆÈÄÇÂêàÂΩìÂâçÊï∞ÊçÆÈõÜËßÑÊ®°

### ÊîπËøõÂª∫ËÆÆ

**Áü≠ÊúüÊîπËøõ**:
1. Ë∞ÉÊï¥ÁΩëÁªúÊû∂ÊûÑÂèÇÊï∞ÔºåÈíàÂØπ‰∏çÂêåÊï∞ÊçÆÈõÜËßÑÊ®°‰ºòÂåñ
2. ÂÆûÁé∞Êõ¥Á≤æÁªÜÁöÑË∂ÖÂèÇÊï∞Ë∞É‰ºò
3. Â¢ûÂä†Êï∞ÊçÆÂ¢ûÂº∫ÊäÄÊúØ

**ÈïøÊúüÂèëÂ±ï**:
1. Âú®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äÈ™åËØÅÊñπÊ≥ïÂèØÊâ©Â±ïÊÄß
2. Êé¢Á¥¢Ëá™ÈÄÇÂ∫îÂàÜÂ∏ÉÈÄâÊã©Êú∫Âà∂
3. ÂºÄÂèëÂÆûÊó∂‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÂ∫îÁî®

### ‰ΩøÁî®Âª∫ËÆÆ

**Êé®Ëçê‰ΩøÁî®CAAC OvRÁöÑÂú∫ÊôØ**:
- ÈúÄË¶Å‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÁöÑÂÖ≥ÈîÆÂÜ≥Á≠ñÂú∫ÊôØ
- ÂåªÁñóËØäÊñ≠„ÄÅÈáëËûçÈ£éÊéßÁ≠âÈ´òÈ£éÈô©Â∫îÁî®
- ÁßëÁ†îÊïôËÇ≤‰∏≠ÁöÑÊñπÊ≥ïÂ≠¶È™åËØÅ

**Êé®Ëçê‰ΩøÁî®‰º†ÁªüÊñπÊ≥ïÁöÑÂú∫ÊôØ**:
- ËøΩÊ±ÇÊúÄÈ´òÂáÜÁ°ÆÁéáÁöÑÁ´ûËµõÂú∫ÊôØ
- ËÆ°ÁÆóËµÑÊ∫êÊúâÈôêÁöÑËæπÁºòËÆæÂ§áÈÉ®ÁΩ≤
- Âø´ÈÄüÂéüÂûãÂºÄÂèëÂíåÂü∫Á∫øÂª∫Á´ã

## ÂèØËßÜÂåñÁªìÊûú

ÁîüÊàêÁöÑÂèØËßÜÂåñÂõæË°®ÂåÖÊã¨:
- È≤ÅÊ£íÊÄßÊõ≤Á∫øÂõæ: Â±ïÁ§∫ÂêÑÊñπÊ≥ïÂú®‰∏çÂêåÂô™Â£∞Ê∞¥Âπ≥‰∏ãÁöÑÊÄßËÉΩÂèòÂåñ
- ÊÄßËÉΩË°∞ÂáèÁÉ≠ÂäõÂõæ: Áõ¥ËßÇÊòæÁ§∫ÊñπÊ≥ïÁöÑÈ≤ÅÊ£íÊÄßÂ∑ÆÂºÇ
- ÁªºÂêàÂàÜÊûêÂõæ: Â§öÁª¥Â∫¶ÊÄßËÉΩÂØπÊØîÂàÜÊûê

---
*Êä•ÂëäÁî±CAACÈ≤ÅÊ£íÊÄßÂÆûÈ™åËøêË°åÂô®Ëá™Âä®ÁîüÊàê*
""")
        
        print(f"üìÑ Enhanced report saved: {self.results_dir_display}/{report_file.name}")
        return report_file.name


def run_quick_robustness_test(**kwargs):
    """Standalone function for quick robustness test."""
    runner = RobustnessExperimentRunner()
    return runner.run_quick_robustness_test(**kwargs)


def run_standard_robustness_test(**kwargs):
    """Standalone function for standard robustness test."""
    runner = RobustnessExperimentRunner()
    return runner.run_standard_robustness_test(**kwargs) 