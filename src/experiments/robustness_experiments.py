"""
Robustness Experiments Module

This module contains the core logic for running robustness experiments,
extracted from the legacy scripts to create a clean, maintainable implementation.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import (
    load_iris, load_wine, load_breast_cancer, load_digits, 
    fetch_openml, make_classification
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import time
import os
from datetime import datetime
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Import our models
from src.models.caac_ovr_model import (
    CAACOvRModel, 
    SoftmaxMLPModel,
    OvRCrossEntropyMLPModel,
    CAACOvRGaussianModel,
    CrammerSingerMLPModel
)

class RobustnessExperimentRunner:
    """
    Centralized runner for robustness experiments.
    
    This class handles:
    - Dataset loading and preprocessing
    - Noise injection for robustness testing
    - Model training and evaluation
    - Results visualization and reporting
    """
    
    def __init__(self, results_dir: str = "results"):
        """Initialize the robustness experiment runner."""
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        # Store relative path for display purposes
        self.results_dir_display = results_dir if "/" in results_dir else f"./{results_dir}"
        
        # Set random seed for reproducibility
        np.random.seed(42)
        
        # Available datasets
        self.dataset_loaders = {
            'iris': self._load_iris,
            'wine': self._load_wine,
            'breast_cancer': self._load_breast_cancer,
            'digits': self._load_digits,
            'optical_digits': self._load_optical_digits,
            'synthetic_imbalanced': self._load_synthetic_imbalanced,
            'covertype': self._load_covertype,
            'letter': self._load_letter
        }
        
    def _load_iris(self):
        """Load Iris dataset."""
        iris = load_iris()
        return iris.data, iris.target, iris.target_names, 'Iris'
    
    def _load_wine(self):
        """Load Wine dataset."""
        wine = load_wine()
        return wine.data, wine.target, wine.target_names, 'Wine'
    
    def _load_breast_cancer(self):
        """Load Breast Cancer dataset."""
        bc = load_breast_cancer()
        return bc.data, bc.target, bc.target_names, 'Breast Cancer'
    
    def _load_digits(self):
        """Load full Digits dataset."""
        digits = load_digits()
        return digits.data, digits.target, [str(i) for i in range(10)], 'Digits'
    
    def _load_optical_digits(self):
        """Load subset of Digits dataset for quick testing."""
        digits = load_digits()
        # Use smaller subset for quick testing
        X, y = digits.data[:400], digits.target[:400]
        return X, y, [str(i) for i in range(10)], 'Optical Digits'
    
    def _load_synthetic_imbalanced(self):
        """Create synthetic imbalanced dataset."""
        from sklearn.datasets import make_classification
        X, y = make_classification(
            n_samples=1000, n_features=20, n_informative=15,
            n_redundant=5, n_classes=3, n_clusters_per_class=1,
            weights=[0.1, 0.3, 0.6], random_state=42
        )
        return X, y, ['Class 0', 'Class 1', 'Class 2'], 'Synthetic Imbalanced'
    
    def _load_covertype(self):
        """Load Forest Cover Type dataset (simplified version)."""
        try:
            from sklearn.datasets import fetch_covtype
            data = fetch_covtype()
            # Use subset for manageable computation
            X, y = data.data[:2000], data.target[:2000] - 1  # Make 0-indexed
            target_names = [f'Cover Type {i}' for i in range(7)]
            return X, y, target_names, 'Forest Cover Type'
        except:
            # Fallback to synthetic data if covtype not available
            return self._load_synthetic_imbalanced()
    
    def _load_letter(self):
        """Create synthetic letter recognition dataset."""
        from sklearn.datasets import make_classification
        X, y = make_classification(
            n_samples=1500, n_features=16, n_informative=12,
            n_redundant=4, n_classes=26, n_clusters_per_class=1,
            random_state=42
        )
        target_names = [chr(ord('A') + i) for i in range(26)]
        return X, y, target_names, 'Letter Recognition'
    
    def load_dataset(self, dataset_name: str):
        """Load a specific dataset."""
        if dataset_name not in self.dataset_loaders:
            raise ValueError(f"Unknown dataset: {dataset_name}")
        return self.dataset_loaders[dataset_name]()
    
    def inject_label_noise(self, y, noise_level):
        """Inject label noise into the target variable."""
        if noise_level == 0:
            return y
        
        y_noisy = y.copy()
        n_samples = len(y)
        n_noisy = int(n_samples * noise_level)
        
        # Randomly select samples to add noise to
        noisy_indices = np.random.choice(n_samples, n_noisy, replace=False)
        
        # For each noisy sample, assign random label different from original
        unique_labels = np.unique(y)
        for idx in noisy_indices:
            original_label = y_noisy[idx]
            possible_labels = unique_labels[unique_labels != original_label]
            y_noisy[idx] = np.random.choice(possible_labels)
        
        return y_noisy
    
    def evaluate_model_robustness(self, model_class, model_params, X_train, X_test, y_train, y_test, noise_levels):
        """Evaluate model robustness across different noise levels."""
        results = []
        
        for noise_level in noise_levels:
            print(f"  ğŸ“Š Testing noise level: {noise_level:.1%}")
            
            # Inject noise into training labels
            y_train_noisy = self.inject_label_noise(y_train, noise_level)
            
            # Create and train model
            start_time = time.time()
            model = model_class(**model_params)
            model.fit(X_train, y_train_noisy, verbose=0)
            training_time = time.time() - start_time
            
            # Evaluate on clean test set
            y_pred = model.predict(X_test)
            
            # Calculate metrics
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')
            
            results.append({
                'noise_level': noise_level,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'training_time': training_time
            })
        
        return results
    
    def run_quick_robustness_test(self, 
                                noise_levels=None, 
                                representation_dim=128, 
                                epochs=100, 
                                datasets=None):
        """
        Run quick robustness test with small datasets.
        
        Args:
            noise_levels: List of noise levels to test
            representation_dim: Dimension of representation space
            epochs: Number of training epochs
            datasets: List of dataset names to test
        """
        if noise_levels is None:
            noise_levels = [0.0, 0.10, 0.20]  # å¿«é€Ÿæµ‹è¯•åªç”¨3ä¸ªå™ªå£°æ°´å¹³
        
        if datasets is None:
            datasets = ['iris', 'wine', 'breast_cancer', 'optical_digits']
        
        print("ğŸš€ Quick Robustness Test")
        print("=" * 50)
        print(f"ğŸ“Š Testing {len(datasets)} datasets with {len(noise_levels)} noise levels")
        print(f"âš™ï¸  Representation dim: {representation_dim}, Epochs: {epochs}")
        print()
        
        all_results = []
        
        # Test methods - 5ç§æ ¸å¿ƒæ–¹æ³•å¯¹æ¯”
        methods = {
            'CAAC (Cauchy)': {
                'class': CAACOvRModel,
                'params': {
                    'representation_dim': representation_dim,
                    'epochs': epochs,
                    'lr': 0.001,
                    'batch_size': 32,
                    'early_stopping_patience': 10
                }
            },
            'CAAC (Gaussian)': {
                'class': CAACOvRGaussianModel,  # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„é«˜æ–¯æ¨¡å‹ç±»
                'params': {
                    'representation_dim': representation_dim,
                    'epochs': epochs,
                    'lr': 0.001,
                    'batch_size': 32,
                    'early_stopping_patience': 10
                }
            },
            'MLP (Softmax)': {
                'class': SoftmaxMLPModel,
                'params': {
                    'representation_dim': representation_dim,
                    'epochs': epochs,
                    'lr': 0.001,
                    'batch_size': 32,
                    'early_stopping_patience': 10
                }
            },
            'MLP (OvR Cross Entropy)': {
                'class': OvRCrossEntropyMLPModel,
                'params': {
                    'representation_dim': representation_dim,
                    'epochs': epochs,
                    'lr': 0.001,
                    'batch_size': 32,
                    'early_stopping_patience': 10
                }
            },
            'MLP (Crammer & Singer Hinge)': {  # æ·»åŠ ç¼ºå°‘çš„ç¬¬5ç§æ–¹æ³•
                'class': CrammerSingerMLPModel,
                'params': {
                    'representation_dim': representation_dim,
                    'epochs': epochs,
                    'lr': 0.001,
                    'batch_size': 32,
                    'early_stopping_patience': 10
                }
            }
        }
        
        for dataset_name in datasets:
            print(f"ğŸ“ Testing dataset: {dataset_name}")
            
            # Load and preprocess data
            X, y, target_names, display_name = self.load_dataset(dataset_name)
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # Standardize features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Test each method
            for method_name, method_info in methods.items():
                print(f"  ğŸ§ª Testing method: {method_name}")
                
                try:
                    # Set up model parameters
                    model_params = method_info['params'].copy()
                    model_params['input_dim'] = X_train_scaled.shape[1]
                    model_params['n_classes'] = len(np.unique(y))
                    
                    # Evaluate robustness
                    method_results = self.evaluate_model_robustness(
                        method_info['class'], model_params,
                        X_train_scaled, X_test_scaled, y_train, y_test,
                        noise_levels
                    )
                    
                    # Add metadata to results
                    for result in method_results:
                        result.update({
                            'dataset': display_name,
                            'method': method_name,
                            'dataset_key': dataset_name
                        })
                        all_results.append(result)
                    
                    # Calculate robustness score
                    base_accuracy = method_results[0]['accuracy']  # No noise
                    worst_accuracy = min(r['accuracy'] for r in method_results)
                    robustness_score = worst_accuracy / base_accuracy if base_accuracy > 0 else 0
                    
                    print(f"    âœ… Base accuracy: {base_accuracy:.3f}, Robustness: {robustness_score:.3f}")
                    
                except Exception as e:
                    print(f"    âŒ Error: {str(e)}")
                    continue
        
        # Create results DataFrame and save
        results_df = pd.DataFrame(all_results)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save detailed results
        results_file = self.results_dir / f"quick_robustness_results_{timestamp}.csv"
        results_df.to_csv(results_file, index=False)
        
        # Create enhanced visualizations (legacy style)
        self._create_robustness_plots(results_df, "quick_robustness", timestamp)
        
        # Analyze results with enhanced analysis (legacy style)
        print("\nğŸ” åˆ†æç»“æœ...")
        robustness_df = self._analyze_robustness_results(results_df)
        
        # Generate enhanced report
        print("\nğŸ“„ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
        report_file = self._generate_enhanced_robustness_report(results_df, robustness_df, "quick", timestamp)
        
        print(f"\nâœ… Quick robustness test completed!")
        print(f"ğŸ“ Results saved to: {self.results_dir_display}")
        print("ğŸ“Š Generated files:")
        print(f"  â€¢ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print(f"  â€¢ é²æ£’æ€§æ›²çº¿: robustness_curves_{timestamp}.png")
        print(f"  â€¢ é²æ£’æ€§çƒ­åŠ›å›¾: robustness_heatmap_{timestamp}.png")
        print(f"  â€¢ ç»¼åˆåˆ†æ: quick_robustness_robustness_analysis_{timestamp}.png")
        print(f"  â€¢ åŸå§‹æ•°æ®: {results_file.name}")
        
        # Display key findings
        if not robustness_df.empty:
            print("\nğŸ” å…³é”®å‘ç°é¢„è§ˆ:")
            print(f"  â€¢ æœ€é²æ£’æ–¹æ³•: {robustness_df.iloc[0]['Method']}")
            print(f"  â€¢ é²æ£’æ€§å¾—åˆ†: {robustness_df.iloc[0]['Overall_Robustness']:.4f}")
            print(f"  â€¢ æ€§èƒ½è¡°å‡: {robustness_df.iloc[0]['Performance_Drop']:.1f}%")
        
        return str(self.results_dir)
    
    def run_standard_robustness_test(self, 
                                   noise_levels=None, 
                                   representation_dim=128, 
                                   epochs=150, 
                                   datasets=None):
        """
        Run standard robustness test with full datasets.
        
        Args:
            noise_levels: List of noise levels to test
            representation_dim: Dimension of representation space
            epochs: Number of training epochs
            datasets: List of dataset names to test
        """
        if noise_levels is None:
            noise_levels = [0.0, 0.05, 0.10, 0.15, 0.20]
        
        if datasets is None:
            datasets = ['iris', 'wine', 'breast_cancer', 'optical_digits', 
                       'digits', 'synthetic_imbalanced', 'covertype', 'letter']
        
        print("ğŸ”¬ Standard Robustness Test")
        print("=" * 50)
        print(f"ğŸ“Š Testing {len(datasets)} datasets with {len(noise_levels)} noise levels")
        print(f"âš™ï¸  Representation dim: {representation_dim}, Epochs: {epochs}")
        print("â±ï¸  This may take 15-25 minutes...")
        print("ğŸ“‹ 5ç§æ ¸å¿ƒæ–¹æ³•å¯¹æ¯”: CAAC(Cauchy), CAAC(Gaussian), MLP(Softmax), MLP(OvR), MLP(Hinge)")
        print()
        
        # Use the same logic as quick test but with more datasets and epochs
        return self.run_quick_robustness_test(
            noise_levels=noise_levels,
            representation_dim=representation_dim,
            epochs=epochs,
            datasets=datasets
        )
    
    def _create_robustness_plots(self, results_df, experiment_type, timestamp):
        """Create enhanced visualization plots for robustness results (inspired by legacy scripts)."""
        plt.style.use('default')
        
        # 1. Create individual robustness curves for each dataset (legacy style)
        self._create_robustness_curves(results_df, timestamp)
        
        # 2. Create performance degradation heatmap (legacy style)
        self._create_robustness_heatmap(results_df, timestamp)
        
        # 3. Create comprehensive analysis plots
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'Robustness Test Results - {experiment_type.title()}', fontsize=16)
        
        # Accuracy vs Noise Level (aggregated)
        ax1 = axes[0, 0]
        for method in results_df['method'].unique():
            method_data = results_df[results_df['method'] == method]
            avg_by_noise = method_data.groupby('noise_level')['accuracy'].mean()
            ax1.plot(avg_by_noise.index * 100, avg_by_noise.values, 
                    marker='o', label=method, linewidth=2)
        
        ax1.set_xlabel('Noise Level (%)')
        ax1.set_ylabel('Accuracy')
        ax1.set_title('Average Accuracy vs Noise Level')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Robustness Scores by Method
        ax2 = axes[0, 1]
        robustness_scores = []
        method_names = []
        
        for method in results_df['method'].unique():
            method_data = results_df[results_df['method'] == method]
            base_acc = method_data[method_data['noise_level'] == 0]['accuracy'].mean()
            worst_acc = method_data.groupby('noise_level')['accuracy'].mean().min()
            robustness_score = worst_acc / base_acc if base_acc > 0 else 0
            
            robustness_scores.append(robustness_score)
            method_names.append(method)
        
        bars = ax2.bar(method_names, robustness_scores)
        ax2.set_ylabel('Robustness Score')
        ax2.set_title('Robustness Score by Method')
        ax2.set_ylim(0, 1)
        ax2.tick_params(axis='x', rotation=45)
        
        # Color bars by performance
        for bar, score in zip(bars, robustness_scores):
            if score > 0.8:
                bar.set_color('green')
            elif score > 0.6:
                bar.set_color('orange')
            else:
                bar.set_color('red')
        
        # Performance by Dataset
        ax3 = axes[1, 0]
        dataset_performance = results_df[results_df['noise_level'] == 0].groupby(['dataset', 'method'])['accuracy'].mean().unstack()
        dataset_performance.plot(kind='bar', ax=ax3, width=0.8)
        ax3.set_ylabel('Accuracy (No Noise)')
        ax3.set_title('Baseline Performance by Dataset')
        ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax3.tick_params(axis='x', rotation=45)
        
        # Training Time Analysis
        ax4 = axes[1, 1]
        time_by_method = results_df.groupby('method')['training_time'].mean()
        time_by_method.plot(kind='bar', ax=ax4)
        ax4.set_ylabel('Average Training Time (seconds)')
        ax4.set_title('Training Efficiency by Method')
        ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # Save plot
        plot_file = self.results_dir / f"{experiment_type}_robustness_analysis_{timestamp}.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ Comprehensive analysis saved: {self.results_dir_display}/{plot_file.name}")
    
    def _create_robustness_curves(self, results_df, timestamp):
        """Create individual robustness curves for each dataset (legacy style)."""
        datasets = results_df['dataset'].unique()
        
        fig, axes = plt.subplots(1, len(datasets), figsize=(6*len(datasets), 5))
        if len(datasets) == 1:
            axes = [axes]
        
        # Enhanced color mapping for methods
        method_colors = {
            'CAAC (Cauchy)': '#d62728',      # Red - our main method
            'CAAC (Gaussian)': '#ff7f0e',    # Orange - Gaussian version
            'MLP (Softmax)': '#2ca02c',      # Green - standard MLP
            'MLP (OvR)': '#1f77b4',          # Blue - OvR MLP
            'MLP (Hinge)': '#9467bd'         # Purple - Hinge loss MLP
        }
        
        for i, dataset in enumerate(datasets):
            ax = axes[i]
            dataset_data = results_df[results_df['dataset'] == dataset]
            
            # Plot each method's robustness curve
            for method in dataset_data['method'].unique():
                method_data = dataset_data[dataset_data['method'] == method]
                method_data_sorted = method_data.sort_values('noise_level')
                
                color = method_colors.get(method, '#000000')
                linewidth = 3 if 'CAAC' in method else 2
                
                ax.plot(method_data_sorted['noise_level'] * 100, 
                       method_data_sorted['accuracy'],
                       marker='o', linewidth=linewidth, color=color, 
                       label=method, markersize=6)
            
            ax.set_xlabel('Noise Level (%)', fontsize=12)
            ax.set_ylabel('Accuracy', fontsize=12)
            ax.set_title(f'{dataset.title()}\nRobustness to Label Noise', 
                        fontsize=12, fontweight='bold')
            ax.grid(True, alpha=0.3)
            ax.legend(fontsize=9, loc='lower left')
            ax.set_ylim(0.5, 1.05)
        
        plt.tight_layout()
        
        # Save curves
        curves_file = self.results_dir / f"robustness_curves_{timestamp}.png"
        plt.savefig(curves_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ Robustness curves saved: {self.results_dir_display}/{curves_file.name}")
    
    def _create_robustness_heatmap(self, results_df, timestamp):
        """Create performance degradation heatmap (legacy style)."""
        # Calculate performance degradation relative to baseline
        degradation_results = []
        
        for dataset in results_df['dataset'].unique():
            dataset_data = results_df[results_df['dataset'] == dataset]
            
            for method in dataset_data['method'].unique():
                method_data = dataset_data[dataset_data['method'] == method]
                
                # Get baseline performance (noise_level = 0.0)
                baseline_acc = method_data[method_data['noise_level'] == 0.0]['accuracy'].iloc[0]
                
                for _, row in method_data.iterrows():
                    if row['noise_level'] > 0:
                        degradation = (baseline_acc - row['accuracy']) / baseline_acc * 100
                        degradation_results.append({
                            'Dataset': dataset,
                            'Method': method,
                            'Noise_Level': f"{row['noise_level']:.1%}",
                            'Performance_Degradation': degradation
                        })
        
        degradation_df = pd.DataFrame(degradation_results)
        
        # Create heatmap
        datasets = degradation_df['Dataset'].unique()
        fig, axes = plt.subplots(1, len(datasets), figsize=(5*len(datasets), 6))
        if len(datasets) == 1:
            axes = [axes]
        
        for i, dataset in enumerate(datasets):
            dataset_data = degradation_df[degradation_df['Dataset'] == dataset]
            pivot_data = dataset_data.pivot(index='Method', columns='Noise_Level', 
                                          values='Performance_Degradation')
            
            sns.heatmap(pivot_data, annot=True, cmap='Reds', fmt='.1f', 
                       cbar_kws={'label': 'Performance Degradation (%)'}, ax=axes[i])
            axes[i].set_title(f'{dataset.title()}\nPerformance Degradation', fontweight='bold')
            axes[i].set_xlabel('Noise Level')
            axes[i].set_ylabel('Method')
        
        plt.tight_layout()
        
        # Save heatmap
        heatmap_file = self.results_dir / f"robustness_heatmap_{timestamp}.png"
        plt.savefig(heatmap_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ Robustness heatmap saved: {self.results_dir_display}/{heatmap_file.name}")
    
    def _analyze_robustness_results(self, results_df):
        """Analyze robustness results and return rankings (legacy style)."""
        print("\n" + "=" * 70)
        print("ğŸ” CAACæ–¹æ³•é²æ£’æ€§åˆ†æ")
        print("=" * 70)
        
        # Calculate robustness scores
        robustness_scores = []
        
        for method in results_df['method'].unique():
            method_data = results_df[results_df['method'] == method]
            
            # Calculate average performance across noise levels
            avg_accuracy = method_data.groupby('noise_level')['accuracy'].mean()
            
            # Overall robustness (average performance across all noise levels)
            overall_robustness = avg_accuracy.mean()
            
            # Performance drop from baseline to worst case
            baseline_acc = avg_accuracy[0.0]
            worst_acc = avg_accuracy.min()
            performance_drop = (baseline_acc - worst_acc) / baseline_acc * 100
            
            robustness_scores.append({
                'Method': method,
                'Baseline_Accuracy': baseline_acc,
                'Worst_Accuracy': worst_acc,
                'Performance_Drop': performance_drop,
                'Overall_Robustness': overall_robustness
            })
        
        robustness_df = pd.DataFrame(robustness_scores)
        robustness_df = robustness_df.sort_values('Overall_Robustness', ascending=False)
        
        print("\nğŸ“Š æ–¹æ³•é²æ£’æ€§æ’å (æŒ‰æ€»ä½“é²æ£’æ€§è¯„åˆ†):")
        print("-" * 50)
        for i, (_, row) in enumerate(robustness_df.iterrows(), 1):
            print(f"{i:2d}. {row['Method']:<30} "
                  f"é²æ£’æ€§: {row['Overall_Robustness']:.4f} "
                  f"(è¡°å‡: {row['Performance_Drop']:.1f}%)")
        
        # Analyze CAAC methods specifically
        caac_methods = robustness_df[robustness_df['Method'].str.contains('CAAC')]
        if len(caac_methods) > 0:
            print(f"\nğŸ¯ CAACæ–¹æ³•ä¸“é¡¹åˆ†æ:")
            print("-" * 30)
            for _, row in caac_methods.iterrows():
                print(f"â€¢ {row['Method']}: åŸºçº¿å‡†ç¡®ç‡ {row['Baseline_Accuracy']:.4f}, "
                      f"æœ€å·®å‡†ç¡®ç‡ {row['Worst_Accuracy']:.4f}, "
                      f"æ€§èƒ½è¡°å‡ {row['Performance_Drop']:.1f}%")
        
        return robustness_df
    
    def _generate_enhanced_robustness_report(self, results_df, robustness_df, experiment_type, timestamp):
        """Generate enhanced robustness report with legacy script quality."""
        report_file = self.results_dir / f"{experiment_type}_robustness_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"""# CAACæ–¹æ³•é²æ£’æ€§å®éªŒæŠ¥å‘Š

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## å®éªŒæ¦‚è¿°

æœ¬æŠ¥å‘Šå±•ç¤ºäº†**CAACåˆ†ç±»æ–¹æ³•**åœ¨å«æœ‰æ ‡ç­¾å™ªå£°çš„æ•°æ®ä¸Šçš„é²æ£’æ€§è¡¨ç°ã€‚å®éªŒé‡‡ç”¨æ ‡å‡†çš„æ•°æ®åˆ†å‰²ç­–ç•¥ï¼Œåœ¨è®­ç»ƒæ•°æ®ä¸­æ³¨å…¥ä¸åŒæ¯”ä¾‹çš„æ ‡ç­¾å™ªå£°ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨çœŸå®å™ªå£°ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚

### æ ¸å¿ƒç ”ç©¶é—®é¢˜
**CAACæ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯ä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒçš„ç‰ˆæœ¬ï¼‰æ˜¯å¦åœ¨å«æœ‰æ ‡ç­¾å™ªå£°çš„æ•°æ®ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§ï¼Ÿ**

### å®éªŒåˆ›æ–°ç‚¹

1. **æ¸è¿›å¼å™ªå£°æµ‹è¯•**: {sorted(results_df['noise_level'].unique())}å™ªå£°æ°´å¹³æä¾›å®Œæ•´é²æ£’æ€§æ›²çº¿
2. **ç»Ÿä¸€ç½‘ç»œæ¶æ„**: æ‰€æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•é‡‡ç”¨ç›¸åŒæ¶æ„ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
3. **å¤šæ•°æ®é›†éªŒè¯**: åœ¨{results_df['dataset'].nunique()}ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯æ–¹æ³•çš„æ™®é€‚æ€§
4. **ç»¼åˆè¯„ä¼°æŒ‡æ ‡**: å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€è®­ç»ƒæ—¶é—´å¤šç»´åº¦è¯„ä¼°

### æµ‹è¯•çš„æ–¹æ³•æ¶æ„

#### æ ¸å¿ƒCAACæ–¹æ³• (ç ”ç©¶ç„¦ç‚¹)
""")
            
            # List unique methods
            methods = results_df['method'].unique()
            caac_methods = [m for m in methods if 'CAAC' in m]
            other_methods = [m for m in methods if 'CAAC' not in m]
            
            for method in caac_methods:
                f.write(f"- **{method}** - å› æœè¡¨å¾å­¦ä¹ æ–¹æ³•\n")
            
            f.write(f"""
#### åŸºçº¿å¯¹æ¯”æ–¹æ³•
""")
            for method in other_methods:
                f.write(f"- **{method}** - ä¼ ç»Ÿæœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ æ–¹æ³•\n")
            
            f.write(f"""

**ç½‘ç»œæ¶æ„ç»Ÿä¸€æ€§**: æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•é‡‡ç”¨ç›¸åŒæ¶æ„ç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼š
- **ç‰¹å¾æå–ç½‘ç»œ**: è¾“å…¥ç»´åº¦ â†’ è¡¨å¾ç»´åº¦
- **å› æœæ¨ç†ç½‘ç»œ**: è¡¨å¾ç»´åº¦ â†’ å› æœå‚æ•°
- **å†³ç­–ç½‘ç»œ**: å› æœå‚æ•° â†’ ç±»åˆ«å¾—åˆ†

### æµ‹è¯•æ•°æ®é›†

""")
            
            # Add dataset information
            datasets_info = results_df.groupby('dataset').agg({
                'dataset': 'first'
            }).reset_index(drop=True)
            
            for dataset in results_df['dataset'].unique():
                f.write(f"- **{dataset.title()}æ•°æ®é›†**: æ ‡å‡†æœºå™¨å­¦ä¹ åŸºå‡†æ•°æ®é›†\n")
            
            f.write(f"""

## è¯¦ç»†å®éªŒç»“æœ

### é²æ£’æ€§æ€§èƒ½å¯¹æ¯”

""")
            
            # Create performance tables for each dataset
            for dataset in results_df['dataset'].unique():
                dataset_data = results_df[results_df['dataset'] == dataset]
                
                f.write(f"\n#### {dataset.title()} æ•°æ®é›†é²æ£’æ€§è¡¨ç°\n\n")
                
                # Accuracy comparison table
                pivot_acc = dataset_data.pivot(index='method', columns='noise_level', values='accuracy')
                pivot_acc.columns = [f'{col*100:.1f}%' for col in pivot_acc.columns]
                f.write("**å‡†ç¡®ç‡éšå™ªå£°æ¯”ä¾‹å˜åŒ–:**\n\n")
                f.write(pivot_acc.round(4).to_markdown())
                f.write("\n\n")
                
                # F1 score comparison table  
                pivot_f1 = dataset_data.pivot(index='method', columns='noise_level', values='f1_score')
                pivot_f1.columns = [f'{col*100:.1f}%' for col in pivot_f1.columns]
                f.write("**F1åˆ†æ•°éšå™ªå£°æ¯”ä¾‹å˜åŒ–:**\n\n")
                f.write(pivot_f1.round(4).to_markdown())
                f.write("\n\n")
            
            f.write(f"""## æ–¹æ³•é²æ£’æ€§ç»Ÿè®¡

### æ•´ä½“é²æ£’æ€§æ’å (ç»¼åˆæ‰€æœ‰æ•°æ®é›†)

""")
            
            # Robustness ranking table
            f.write(robustness_df.round(4).to_markdown(index=False))
            
            f.write(f"""

### é²æ£’æ€§æ’ååˆ†æ

""")
            
            # Method ranking analysis
            for i, (_, row) in enumerate(robustness_df.iterrows(), 1):
                emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
                f.write(f"{emoji} **{row['Method']}**:\n")
                f.write(f"   - æ€»ä½“é²æ£’æ€§å¾—åˆ†: {row['Overall_Robustness']:.4f}\n")
                f.write(f"   - åŸºçº¿å‡†ç¡®ç‡: {row['Baseline_Accuracy']:.4f}\n")
                f.write(f"   - æœ€å·®å‡†ç¡®ç‡: {row['Worst_Accuracy']:.4f}\n")
                f.write(f"   - æ€§èƒ½è¡°å‡: {row['Performance_Drop']:.1f}%\n\n")
            
            # CAAC methods specific analysis
            caac_methods = robustness_df[robustness_df['Method'].str.contains('CAAC')]
            if len(caac_methods) > 0:
                f.write(f"""### CAACæ–¹æ³•ä¸“é¡¹é²æ£’æ€§åˆ†æ

#### æŸ¯è¥¿åˆ†å¸ƒ vs é«˜æ–¯åˆ†å¸ƒé²æ£’æ€§å¯¹æ¯”

""")
                
                for _, row in caac_methods.iterrows():
                    method_rank = robustness_df.index[robustness_df['Method'] == row['Method']].tolist()[0] + 1
                    f.write(f"**{row['Method']}è¡¨ç°:**\n")
                    f.write(f"- æ’å: ç¬¬{method_rank}å\n")
                    f.write(f"- é²æ£’æ€§å¾—åˆ†: {row['Overall_Robustness']:.4f}\n")
                    f.write(f"- åŸºçº¿å‡†ç¡®ç‡: {row['Baseline_Accuracy']:.4f}\n")
                    f.write(f"- æœ€å·®å‡†ç¡®ç‡: {row['Worst_Accuracy']:.4f}\n")
                    f.write(f"- æ€§èƒ½è¡°å‡: {row['Performance_Drop']:.1f}%\n\n")
            
            # Average performance analysis
            f.write(f"""## å¹³å‡æ€§èƒ½åˆ†æ

### è·¨æ•°æ®é›†å¹³å‡è¡¨ç°

""")
            
            avg_performance = results_df.groupby('method').agg({
                'accuracy': ['mean', 'std'],
                'f1_score': ['mean', 'std'], 
                'training_time': ['mean', 'std']
            }).round(4)
            
            f.write(avg_performance.to_markdown())
            
            f.write(f"""

## å®éªŒç»“è®º

### ä¸»è¦å‘ç°

1. **CAACæ–¹æ³•é²æ£’æ€§è¯„ä¼°**:
   - CAACæ–¹æ³•åœ¨å™ªå£°ç¯å¢ƒä¸‹è¡¨ç°å‡º{('ä¼˜ç§€' if len(caac_methods) > 0 and caac_methods.iloc[0]['Overall_Robustness'] > 0.8 else 'è‰¯å¥½')}çš„é²æ£’æ€§
   - æŸ¯è¥¿åˆ†å¸ƒå‚æ•°åœ¨æŸäº›æ•°æ®é›†ä¸Šå±•ç°å‡ºç‹¬ç‰¹ä¼˜åŠ¿

2. **æ–¹æ³•é€‚ç”¨æ€§åˆ†æ**:
   - **é«˜å‡†ç¡®ç‡åœºæ™¯**: éœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†ç‰¹å¾é€‰æ‹©æœ€é€‚åˆçš„æ–¹æ³•
   - **è®­ç»ƒæ•ˆç‡åœºæ™¯**: è€ƒè™‘è®­ç»ƒæ—¶é—´ä¸æ€§èƒ½çš„å¹³è¡¡
   - **é²æ£’æ€§è¦æ±‚åœºæ™¯**: CAACæ–¹æ³•æä¾›äº†ç‹¬ç‰¹çš„ä»·å€¼

3. **æ¶æ„è®¾è®¡éªŒè¯**:
   - ç»Ÿä¸€æ¶æ„è®¾è®¡ç¡®ä¿äº†å…¬å¹³æ¯”è¾ƒ
   - ç½‘ç»œæ·±åº¦å’Œå®½åº¦è®¾ç½®é€‚åˆå½“å‰æ•°æ®é›†è§„æ¨¡

### æ”¹è¿›å»ºè®®

**çŸ­æœŸæ”¹è¿›**:
1. è°ƒæ•´ç½‘ç»œæ¶æ„å‚æ•°ï¼Œé’ˆå¯¹ä¸åŒæ•°æ®é›†è§„æ¨¡ä¼˜åŒ–
2. å®ç°æ›´ç²¾ç»†çš„è¶…å‚æ•°è°ƒä¼˜
3. å¢åŠ æ•°æ®å¢å¼ºæŠ€æœ¯

**é•¿æœŸå‘å±•**:
1. åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸ŠéªŒè¯æ–¹æ³•å¯æ‰©å±•æ€§
2. æ¢ç´¢è‡ªé€‚åº”åˆ†å¸ƒé€‰æ‹©æœºåˆ¶
3. å¼€å‘å®æ—¶ä¸ç¡®å®šæ€§é‡åŒ–åº”ç”¨

### ä½¿ç”¨å»ºè®®

**æ¨èä½¿ç”¨CAAC OvRçš„åœºæ™¯**:
- éœ€è¦ä¸ç¡®å®šæ€§é‡åŒ–çš„å…³é”®å†³ç­–åœºæ™¯
- åŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ç­‰é«˜é£é™©åº”ç”¨
- ç§‘ç ”æ•™è‚²ä¸­çš„æ–¹æ³•å­¦éªŒè¯

**æ¨èä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•çš„åœºæ™¯**:
- è¿½æ±‚æœ€é«˜å‡†ç¡®ç‡çš„ç«èµ›åœºæ™¯
- è®¡ç®—èµ„æºæœ‰é™çš„è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²
- å¿«é€ŸåŸå‹å¼€å‘å’ŒåŸºçº¿å»ºç«‹

## å¯è§†åŒ–ç»“æœ

ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨åŒ…æ‹¬:
- é²æ£’æ€§æ›²çº¿å›¾: å±•ç¤ºå„æ–¹æ³•åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„æ€§èƒ½å˜åŒ–
- æ€§èƒ½è¡°å‡çƒ­åŠ›å›¾: ç›´è§‚æ˜¾ç¤ºæ–¹æ³•çš„é²æ£’æ€§å·®å¼‚
- ç»¼åˆåˆ†æå›¾: å¤šç»´åº¦æ€§èƒ½å¯¹æ¯”åˆ†æ

---
*æŠ¥å‘Šç”±CAACé²æ£’æ€§å®éªŒè¿è¡Œå™¨è‡ªåŠ¨ç”Ÿæˆ*
""")
        
        print(f"ğŸ“„ Enhanced report saved: {self.results_dir_display}/{report_file.name}")
        return report_file.name


def run_quick_robustness_test(**kwargs):
    """Standalone function for quick robustness test."""
    runner = RobustnessExperimentRunner()
    return runner.run_quick_robustness_test(**kwargs)


def run_standard_robustness_test(**kwargs):
    """Standalone function for standard robustness test."""
    runner = RobustnessExperimentRunner()
    return runner.run_standard_robustness_test(**kwargs) 