"""
Method Comparison Experiments Module

This module contains the core logic for running method comparison experiments,
extracted from the legacy scripts to create a clean, maintainable implementation.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris, load_wine, load_breast_cancer, load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import time
import os
from datetime import datetime
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Import our models
from src.models.caac_ovr_model import (
    CAACOvRModel, 
    SoftmaxMLPModel,
    OvRCrossEntropyMLPModel,
    CAACOvRGaussianModel,
    CrammerSingerMLPModel
)

class MethodComparisonRunner:
    """
    Centralized runner for method comparison experiments.
    
    This class handles:
    - Dataset loading and preprocessing
    - Model configuration and training
    - Performance evaluation and comparison
    - Results visualization and reporting
    """
    
    def __init__(self, results_dir: str = "results"):
        """Initialize the method comparison runner."""
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        # Store relative path for display purposes
        self.results_dir_display = results_dir if "/" in results_dir else f"./{results_dir}"
        
        # Set random seed for reproducibility
        np.random.seed(42)
    
    def load_datasets(self):
        """Load all test datasets."""
        datasets = {}
        
        # Iris dataset
        iris = load_iris()
        datasets['iris'] = {
            'data': iris.data,
            'target': iris.target,
            'target_names': iris.target_names,
            'name': 'Iris'
        }
        
        # Wine dataset
        wine = load_wine()
        datasets['wine'] = {
            'data': wine.data,
            'target': wine.target,
            'target_names': wine.target_names,
            'name': 'Wine'
        }
        
        # Breast Cancer dataset
        bc = load_breast_cancer()
        datasets['breast_cancer'] = {
            'data': bc.data,
            'target': bc.target,
            'target_names': bc.target_names,
            'name': 'Breast Cancer'
        }
        
        # Digits dataset
        digits = load_digits()
        datasets['digits'] = {
            'data': digits.data,
            'target': digits.target,
            'target_names': [str(i) for i in range(10)],
            'name': 'Digits'
        }
        
        return datasets
    
    def create_comparison_methods(self, representation_dim=64, epochs=100):
        """Create all methods for comparison - ä¸æœ€æ–°é²æ£’æ€§å®éªŒä¿æŒä¸€è‡´çš„5ç§æ ¸å¿ƒç¥ç»ç½‘ç»œæ–¹æ³•."""
        # ç»Ÿä¸€ç½‘ç»œæ¶æ„å‚æ•° - ä¸é²æ£’æ€§å®éªŒä¿æŒä¸€è‡´
        common_params = {
            'representation_dim': representation_dim,
            'lr': 0.001,
            'batch_size': 32,
            'epochs': epochs,
            'early_stopping_patience': 10
        }
        
        methods = {
            # æ ¸å¿ƒæ¨æ–­è¡ŒåŠ¨æ¡†æ¶æ–¹æ³• - åŸºç¡€ç‰ˆæœ¬
            'CAAC_Cauchy': {
                'name': 'CAAC (Cauchy)',
                'type': 'unified',
                'model_class': CAACOvRModel,
                'params': {**common_params, 'learnable_thresholds': False}
            },
            'CAAC_Gaussian': {
                'name': 'CAAC (Gaussian)', 
                'type': 'unified',
                'model_class': CAACOvRGaussianModel,
                'params': {**common_params, 'learnable_thresholds': False}
            },
            
            # æ¨æ–­è¡ŒåŠ¨æ¡†æ¶ - å¯å­¦ä¹ é˜ˆå€¼å˜ä½“
            'CAAC_Cauchy_Learnable': {
                'name': 'CAAC (Cauchy, Learnable)',
                'type': 'unified',
                'model_class': CAACOvRModel,
                'params': {**common_params, 'learnable_thresholds': True}
            },
            'CAAC_Gaussian_Learnable': {
                'name': 'CAAC (Gaussian, Learnable)',
                'type': 'unified',
                'model_class': CAACOvRGaussianModel,
                'params': {**common_params, 'learnable_thresholds': True}
            },
            
            # æ¨æ–­è¡ŒåŠ¨æ¡†æ¶ - å”¯ä¸€æ€§çº¦æŸå˜ä½“
            'CAAC_Cauchy_Unique': {
                'name': 'CAAC Cauchy (Uniqueness)',
                'type': 'unified',
                'model_class': CAACOvRModel,
                'params': {**common_params, 'learnable_thresholds': False, 'uniqueness_constraint': True, 'uniqueness_samples': 3, 'uniqueness_weight': 0.05}
            },
            'CAAC_Gaussian_Unique': {
                'name': 'CAAC Gaussian (Uniqueness)',
                'type': 'unified',
                'model_class': CAACOvRGaussianModel,
                'params': {**common_params, 'learnable_thresholds': False, 'uniqueness_constraint': True, 'uniqueness_samples': 3, 'uniqueness_weight': 0.05}
            },
            
            # æ¨æ–­è¡ŒåŠ¨æ¡†æ¶ - å¯å­¦ä¹ é˜ˆå€¼+å”¯ä¸€æ€§çº¦æŸç»„åˆ
            'CAAC_Cauchy_Learnable_Unique': {
                'name': 'CAAC Cauchy (Learnable+Uniqueness)',
                'type': 'unified',
                'model_class': CAACOvRModel,
                'params': {**common_params, 'learnable_thresholds': True, 'uniqueness_constraint': True, 'uniqueness_samples': 3, 'uniqueness_weight': 0.05}
            },
            'CAAC_Gaussian_Learnable_Unique': {
                'name': 'CAAC Gaussian (Learnable+Uniqueness)',
                'type': 'unified',
                'model_class': CAACOvRGaussianModel,
                'params': {**common_params, 'learnable_thresholds': True, 'uniqueness_constraint': True, 'uniqueness_samples': 3, 'uniqueness_weight': 0.05}
            },
            
            # æ ‡å‡†æ·±åº¦å­¦ä¹ æ–¹æ³•
            'MLP_Softmax': {
                'name': 'MLP (Softmax)',
                'type': 'unified',
                'model_class': SoftmaxMLPModel,
                'params': common_params
            },
            'MLP_OvR_CE': {
                'name': 'MLP (OvR Cross Entropy)',
                'type': 'unified',
                'model_class': OvRCrossEntropyMLPModel,
                'params': common_params
            },
            'MLP_Hinge': {
                'name': 'MLP (Crammer & Singer Hinge)',
                'type': 'unified',
                'model_class': CrammerSingerMLPModel,
                'params': common_params
            },
            
            # Classical machine learning methods as baselines
            'Softmax_LR': {
                'name': 'Softmax Regression',
                'type': 'sklearn',
                'model': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)
            },
            'Standard_OvR': {
                'name': 'OvR Logistic',
                'type': 'sklearn',
                'model': OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))
            },
            'SVM_RBF': {
                'name': 'SVM-RBF',
                'type': 'sklearn',
                'model': SVC(kernel='rbf', random_state=42, probability=True)
            },
            'Random_Forest': {
                'name': 'Random Forest',
                'type': 'sklearn',
                'model': RandomForestClassifier(n_estimators=100, random_state=42)
            },
            'Sklearn_MLP': {
                'name': 'MLP-Sklearn',
                'type': 'sklearn',
                'model': MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)
            }
        }
        return methods
    
    def evaluate_method(self, method_info, X_train, X_test, y_train, y_test):
        """Evaluate a single method's performance."""
        start_time = time.time()
        
        if method_info['type'] == 'unified':
            # Use our unified network architecture
            input_dim = X_train.shape[1]
            n_classes = len(np.unique(y_train))
            
            model = method_info['model_class'](
                input_dim=input_dim, 
                n_classes=n_classes,
                **method_info['params']
            )
            
            # Train model
            model.fit(X_train, y_train, verbose=0)
            
            # Predict
            y_pred = model.predict(X_test)
            
        else:
            # Use sklearn model
            model = method_info['model']
            # Train model
            model.fit(X_train, y_train)
            
            # Predict
            y_pred = model.predict(X_test)
        
        training_time = time.time() - start_time
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)
        recall_macro = recall_score(y_test, y_pred, average='macro')
        f1_macro = f1_score(y_test, y_pred, average='macro')
        f1_weighted = f1_score(y_test, y_pred, average='weighted')
        
        return {
            'accuracy': accuracy,
            'precision_macro': precision_macro,
            'recall_macro': recall_macro,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'training_time': training_time
        }
    
    def run_comparison_experiments(self, representation_dim=64, epochs=100, datasets=None):
        """Run comprehensive method comparison experiments."""
        print("ğŸ”¬ åˆ†ç±»æ–¹æ³•å¯¹æ¯”å®éªŒ - å®Œæ•´æ¨æ–­è¡ŒåŠ¨æ¡†æ¶è¯„ä¼°")
        print("=" * 60)
        print("ğŸ“‹ æµ‹è¯•æ–¹æ³•ï¼ˆ11ç§ç»Ÿä¸€æ¶æ„æ–¹æ³• + 5ç§ç»å…¸æ–¹æ³•ï¼‰ï¼š")
        print("   ğŸ§  æ¨æ–­è¡ŒåŠ¨æ¡†æ¶åŸºç¡€ç‰ˆæœ¬ï¼š")
        print("      â€¢ CAAC (Cauchy) - æŸ¯è¥¿åˆ†å¸ƒ + å›ºå®šé˜ˆå€¼")
        print("      â€¢ CAAC (Gaussian) - é«˜æ–¯åˆ†å¸ƒ + å›ºå®šé˜ˆå€¼")
        print("   âš™ï¸  æ¨æ–­è¡ŒåŠ¨æ¡†æ¶å¯å­¦ä¹ é˜ˆå€¼ç‰ˆæœ¬ï¼š")
        print("      â€¢ CAAC (Cauchy, Learnable) - æŸ¯è¥¿åˆ†å¸ƒ + å¯å­¦ä¹ é˜ˆå€¼")
        print("      â€¢ CAAC (Gaussian, Learnable) - é«˜æ–¯åˆ†å¸ƒ + å¯å­¦ä¹ é˜ˆå€¼")
        print("   ğŸ”’ æ¨æ–­è¡ŒåŠ¨æ¡†æ¶å”¯ä¸€æ€§çº¦æŸç‰ˆæœ¬ï¼š")
        print("      â€¢ CAAC Cauchy (Uniqueness) - æŸ¯è¥¿åˆ†å¸ƒ + å”¯ä¸€æ€§çº¦æŸ")
        print("      â€¢ CAAC Gaussian (Uniqueness) - é«˜æ–¯åˆ†å¸ƒ + å”¯ä¸€æ€§çº¦æŸ")
        print("   ğŸ”§ æ¨æ–­è¡ŒåŠ¨æ¡†æ¶ç»„åˆç‰ˆæœ¬ï¼š")
        print("      â€¢ CAAC Cauchy (Learnable+Uniqueness) - æŸ¯è¥¿+å¯å­¦ä¹ é˜ˆå€¼+å”¯ä¸€æ€§")
        print("      â€¢ CAAC Gaussian (Learnable+Uniqueness) - é«˜æ–¯+å¯å­¦ä¹ é˜ˆå€¼+å”¯ä¸€æ€§")
        print("   ğŸ“Š æ ‡å‡†æ·±åº¦å­¦ä¹ å¯¹ç…§ï¼š")
        print("      â€¢ MLP (Softmax) - æ ‡å‡†Softmaxåˆ†ç±»å™¨")
        print("      â€¢ MLP (OvR Cross Entropy) - ä¸€å¯¹å¤šäº¤å‰ç†µ")
        print("      â€¢ MLP (Crammer & Singer Hinge) - é“°é“¾æŸå¤±")
        print()
        
        # Load datasets
        all_datasets = self.load_datasets()
        if datasets:
            # Filter to specified datasets
            all_datasets = {k: v for k, v in all_datasets.items() if k in datasets}
        
        # Create methods for comparison
        methods = self.create_comparison_methods(representation_dim, epochs)
        
        results = []
        
        for dataset_name, dataset in all_datasets.items():
            print(f"\nğŸ“Š Testing dataset: {dataset['name']}")
            print("-" * 40)
            
            # Data preprocessing
            X = dataset['data']
            y = dataset['target']
            
            # Split dataset
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # Standardize features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Test each method
            for method_key, method_info in methods.items():
                print(f"  ğŸ§ª Testing method: {method_info['name']}")
                
                try:
                    metrics = self.evaluate_method(
                        method_info, 
                        X_train_scaled, X_test_scaled, 
                        y_train, y_test
                    )
                    
                    results.append({
                        'Dataset': dataset['name'],
                        'Method': method_info['name'],
                        'Method_Key': method_key,
                        'Method_Type': method_info['type'],
                        'Accuracy': metrics['accuracy'],
                        'Precision_Macro': metrics['precision_macro'],
                        'Recall_Macro': metrics['recall_macro'],
                        'F1_Macro': metrics['f1_macro'],
                        'F1_Weighted': metrics['f1_weighted'],
                        'Training_Time': metrics['training_time']
                    })
                    
                    print(f"    âœ… Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_macro']:.4f}, Time: {metrics['training_time']:.3f}s")
                    
                except Exception as e:
                    print(f"    âŒ Error: {str(e)}")
                    continue
        
        # Create results DataFrame
        results_df = pd.DataFrame(results)
        
        # Generate visualizations and reports
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self._create_comparison_plots(results_df, timestamp)
        summary = self._create_summary_table(results_df, timestamp)
        self._generate_detailed_report(results_df, summary, timestamp)
        
        print("\nâœ… Method comparison experiments completed!")
        print(f"ğŸ“ Results saved to: {self.results_dir_display}")
        print("ğŸ“Š Generated files:")
        print(f"   - {self.results_dir_display}/methods_comparison_english_{timestamp}.png")
        print(f"   - {self.results_dir_display}/methods_comparison_detailed_{timestamp}.csv") 
        print(f"   - {self.results_dir_display}/methods_comparison_summary_{timestamp}.csv")
        print(f"   - {self.results_dir_display}/caac_methods_comparison_report_{timestamp}.md")
        
        return str(self.results_dir)
    
    def _create_comparison_plots(self, results_df, timestamp):
        """Create comparison visualization charts with English labels."""
        plt.style.use('default')
        
        # Set figure size and layout
        fig, axes = plt.subplots(2, 2, figsize=(18, 14))
        fig.suptitle('Classification Methods Comparison: Cauchy vs. Gaussian vs. Standard', 
                     fontsize=16, fontweight='bold')
        
        # 1. Accuracy comparison
        ax1 = axes[0, 0]
        pivot_acc = results_df.pivot(index='Dataset', columns='Method', values='Accuracy')
        pivot_acc.plot(kind='bar', ax=ax1, rot=30, width=0.8)
        ax1.set_title('Accuracy Comparison by Dataset', fontsize=14)
        ax1.set_ylabel('Accuracy', fontsize=12)
        ax1.set_xlabel('Dataset', fontsize=12)
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0.85, 1.02)
        
        # 2. F1-score comparison
        ax2 = axes[0, 1]
        pivot_f1 = results_df.pivot(index='Dataset', columns='Method', values='F1_Macro')
        pivot_f1.plot(kind='bar', ax=ax2, rot=30, width=0.8)
        ax2.set_title('F1-Score (Macro) Comparison by Dataset', fontsize=14)
        ax2.set_ylabel('F1-Score (Macro)', fontsize=12)
        ax2.set_xlabel('Dataset', fontsize=12)
        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0.85, 1.02)
        
        # 3. Training time comparison
        ax3 = axes[1, 0]
        pivot_time = results_df.pivot(index='Dataset', columns='Method', values='Training_Time')
        pivot_time.plot(kind='bar', ax=ax3, rot=30, width=0.8)
        ax3.set_title('Training Time Comparison by Dataset', fontsize=14)
        ax3.set_ylabel('Training Time (seconds, log scale)', fontsize=12)
        ax3.set_xlabel('Dataset', fontsize=12)
        ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        ax3.grid(True, alpha=0.3)
        ax3.set_yscale('log')
        
        # 4. Scatter plot: Accuracy vs Training Time
        ax4 = axes[1, 1]
        methods = results_df['Method'].unique()
        colors = plt.cm.tab10(np.linspace(0, 1, len(methods)))
        
        for method, color in zip(methods, colors):
            method_data = results_df[results_df['Method'] == method]
            ax4.scatter(method_data['Training_Time'], method_data['Accuracy'], 
                       label=method, alpha=0.8, s=80, color=color, edgecolors='black', linewidth=0.5)
        
        ax4.set_xlabel('Training Time (seconds, log scale)', fontsize=12)
        ax4.set_ylabel('Accuracy', fontsize=12)
        ax4.set_title('Efficiency vs Performance Trade-off', fontsize=14)
        ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        ax4.grid(True, alpha=0.3)
        ax4.set_xscale('log')
        ax4.set_ylim(0.90, 1.02)
        
        plt.tight_layout()
        plot_file = self.results_dir / f'methods_comparison_english_{timestamp}.png'
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“ˆ English comparison chart saved: {self.results_dir_display}/{plot_file.name}")
    
    def _create_summary_table(self, results_df, timestamp):
        """Create summary comparison table."""
        print("\nğŸ“‹ Method Comparison Summary Table")
        print("=" * 80)
        
        # Calculate average performance by method
        summary = results_df.groupby('Method').agg({
            'Accuracy': ['mean', 'std'],
            'F1_Macro': ['mean', 'std'],
            'Training_Time': ['mean', 'std']
        }).round(4)
        
        print(summary)
        
        # Save detailed results
        results_df.to_csv(self.results_dir / f'methods_comparison_detailed_{timestamp}.csv', index=False)
        summary.to_csv(self.results_dir / f'methods_comparison_summary_{timestamp}.csv')
        
        return summary
    
    def _generate_detailed_report(self, results_df, summary, timestamp):
        """ç”Ÿæˆè¯¦ç»†çš„å®éªŒæ¯”è¾ƒæŠ¥å‘Šï¼ˆä¸­æ–‡ç‰ˆï¼‰ã€‚"""
        print("\nğŸ“„ ç”Ÿæˆè¯¦ç»†å®éªŒæŠ¥å‘Š")
        print("=" * 50)
        
        report_file = self.results_dir / f"caac_methods_comparison_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"""# CAACåˆ†ç±»æ–¹æ³•å¯¹æ¯”å®éªŒæŠ¥å‘Š

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## å®éªŒæ¦‚è¿°

æœ¬æŠ¥å‘Šå±•ç¤ºäº†**CAAC OvRåˆ†ç±»å™¨**ä¸å¤šç§ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•çš„å…¨é¢æ€§èƒ½æ¯”è¾ƒã€‚å®éªŒé‡‡ç”¨ç»Ÿä¸€çš„ç½‘ç»œæ¶æ„ï¼Œä»…åœ¨æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–ç­–ç•¥ä¸Šæœ‰æ‰€ä¸åŒï¼Œç¡®ä¿äº†å…¬å¹³çš„æ¯”è¾ƒç¯å¢ƒã€‚

### æ ¸å¿ƒç ”ç©¶é—®é¢˜
**æ¨æ–­è¡ŒåŠ¨æ¡†æ¶ä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒçš„å°ºåº¦å‚æ•°æ˜¯å¦ä¼˜äºé«˜æ–¯åˆ†å¸ƒå’Œæ ‡å‡†æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Ÿ**

### æ¨æ–­è¡ŒåŠ¨æ¡†æ¶æ¶æ„
æˆ‘ä»¬çš„æ¨æ–­è¡ŒåŠ¨æ¡†æ¶ï¼ˆCAACæ–¹æ³•ï¼‰é‡‡ç”¨ç»Ÿä¸€çš„ä¸‰é˜¶æ®µæ¶æ„ï¼š
- **FeatureNet**: ç‰¹å¾æå–ç½‘ç»œ (è¾“å…¥ç»´åº¦ â†’ 64ç»´**ç¡®å®šæ€§ç‰¹å¾è¡¨å¾**)
- **AbductionNet**: æº¯å› æ¨ç†ç½‘ç»œ (64ç»´ â†’ 64ç»´**å› æœè¡¨å¾éšæœºå˜é‡**å‚æ•°)  
- **ActionNet**: è¡ŒåŠ¨å†³ç­–ç½‘ç»œ (64ç»´ â†’ **ç±»åˆ«æ•°é‡**çš„å¾—åˆ†)

**æ¨æ–­è¡ŒåŠ¨æ¡†æ¶æ ¸å¿ƒæ€æƒ³**: 
- ç‰¹å¾è¡¨å¾æ˜¯ç¡®å®šæ€§æ•°å€¼ï¼Œå› æœè¡¨å¾æ˜¯éšæœºå˜é‡ï¼ˆä½ç½®+å°ºåº¦å‚æ•°ï¼‰
- æŸ¯è¥¿åˆ†å¸ƒvsé«˜æ–¯åˆ†å¸ƒï¼šä¸åŒçš„å°ºåº¦å‚æ•°å»ºæ¨¡ç­–ç•¥
- é€šè¿‡æ¦‚ç‡æ¨ç†å®ç°æ›´é²æ£’çš„åˆ†ç±»å†³ç­–
- æ ‡å‡†æ–¹æ³•ä»…ä½¿ç”¨ä½ç½®å‚æ•°ï¼Œå¿½ç•¥äº†ä¸ç¡®å®šæ€§å»ºæ¨¡

### å®éªŒæ–¹æ³•

#### ç»Ÿä¸€æ¶æ„æ–¹æ³• (11ç§ç¥ç»ç½‘ç»œæ–¹æ³•)

**æ¨æ–­è¡ŒåŠ¨æ¡†æ¶åŸºç¡€ç‰ˆæœ¬ (2ç§æ–¹æ³•):**
1. **CAAC (Cauchy)** - æ¨æ–­è¡ŒåŠ¨æ¡†æ¶ï¼ŒæŸ¯è¥¿åˆ†å¸ƒå»ºæ¨¡ + å›ºå®šé˜ˆå€¼
2. **CAAC (Gaussian)** - æ¨æ–­è¡ŒåŠ¨æ¡†æ¶ï¼Œé«˜æ–¯åˆ†å¸ƒå»ºæ¨¡ + å›ºå®šé˜ˆå€¼

**æ¨æ–­è¡ŒåŠ¨æ¡†æ¶å¯å­¦ä¹ é˜ˆå€¼ç‰ˆæœ¬ (2ç§æ–¹æ³•):**
3. **CAAC (Cauchy, Learnable)** - æŸ¯è¥¿åˆ†å¸ƒ + å¯å­¦ä¹ é˜ˆå€¼å‚æ•°
4. **CAAC (Gaussian, Learnable)** - é«˜æ–¯åˆ†å¸ƒ + å¯å­¦ä¹ é˜ˆå€¼å‚æ•°

**æ¨æ–­è¡ŒåŠ¨æ¡†æ¶å”¯ä¸€æ€§çº¦æŸç‰ˆæœ¬ (2ç§æ–¹æ³•):**
5. **CAAC Cauchy (Uniqueness)** - æŸ¯è¥¿åˆ†å¸ƒ + æ½œåœ¨å‘é‡é‡‡æ ·å”¯ä¸€æ€§çº¦æŸ
6. **CAAC Gaussian (Uniqueness)** - é«˜æ–¯åˆ†å¸ƒ + æ½œåœ¨å‘é‡é‡‡æ ·å”¯ä¸€æ€§çº¦æŸ

**æ¨æ–­è¡ŒåŠ¨æ¡†æ¶ç»„åˆç‰ˆæœ¬ (2ç§æ–¹æ³•):**
7. **CAAC Cauchy (Learnable+Uniqueness)** - æŸ¯è¥¿åˆ†å¸ƒ + å¯å­¦ä¹ é˜ˆå€¼ + å”¯ä¸€æ€§çº¦æŸ
8. **CAAC Gaussian (Learnable+Uniqueness)** - é«˜æ–¯åˆ†å¸ƒ + å¯å­¦ä¹ é˜ˆå€¼ + å”¯ä¸€æ€§çº¦æŸ

**æ ‡å‡†æ·±åº¦å­¦ä¹ å¯¹ç…§æ–¹æ³• (3ç§æ–¹æ³•):**
9. **MLP (Softmax)** - æ ‡å‡†å¤šå±‚æ„ŸçŸ¥æœºï¼Œä½¿ç”¨SoftmaxæŸå¤±å‡½æ•°
10. **MLP (OvR Cross Entropy)** - ä¸€å¯¹å¤šç­–ç•¥çš„äº¤å‰ç†µæŸå¤±å‡½æ•°
11. **MLP (Crammer & Singer Hinge)** - å¤šç±»é“°é“¾æŸå¤±å‡½æ•°

**å”¯ä¸€æ€§çº¦æŸè¯´æ˜:**
- é€šè¿‡é‡‡æ ·å¤šä¸ªæ½œåœ¨å‘é‡å®ä¾‹åŒ–ï¼Œåº”ç”¨æœ€å¤§-æ¬¡å¤§é—´éš”çº¦æŸå¢å¼ºå†³ç­–ç¡®å®šæ€§
- é‡‡æ ·æ¬¡æ•°ï¼š3æ¬¡ï¼Œçº¦æŸæƒé‡ï¼š0.05ï¼ˆå®éªŒå‘ç°å€¾å‘äºé™ä½å‡†ç¡®ç‡ï¼Œä¸»è¦ç”¨ä½œç†è®ºå¯¹ç…§ç ”ç©¶ï¼‰

#### ç»å…¸æœºå™¨å­¦ä¹ åŸºå‡†æ–¹æ³• (å¯¹ç…§ç»„)
6. **Softmax Regression** - å¤šé¡¹å¼logisticå›å½’
7. **OvR Logistic** - ä¸€å¯¹å…¶ä½™é€»è¾‘å›å½’
8. **SVM-RBF** - å¾„å‘åŸºå‡½æ•°æ”¯æŒå‘é‡æœº
9. **Random Forest** - éšæœºæ£®æ—é›†æˆæ–¹æ³•
10. **MLP-Sklearn** - Scikit-learnå¤šå±‚æ„ŸçŸ¥æœº

### æµ‹è¯•æ•°æ®é›†
- **Irisé¸¢å°¾èŠ±æ•°æ®é›†**: 3ç±», 4ç‰¹å¾, 150æ ·æœ¬
- **Wineçº¢é…’æ•°æ®é›†**: 3ç±», 13ç‰¹å¾, 178æ ·æœ¬  
- **Breast Cancerä¹³è…ºç™Œæ•°æ®é›†**: 2ç±», 30ç‰¹å¾, 569æ ·æœ¬
- **Digitsæ‰‹å†™æ•°å­—æ•°æ®é›†**: 10ç±», 64ç‰¹å¾, 1797æ ·æœ¬

## è¯¦ç»†å®éªŒç»“æœ

### å‡†ç¡®ç‡å¯¹æ¯”

""")
            
            # åˆ›å»ºå‡†ç¡®ç‡å¯¹æ¯”è¡¨
            pivot_acc = results_df.pivot(index='Dataset', columns='Method', values='Accuracy')
            f.write(pivot_acc.round(4).to_markdown())
            f.write("\n\n### F1åˆ†æ•°å¯¹æ¯” (Macro Average)\n\n")
            
            # åˆ›å»ºF1åˆ†æ•°å¯¹æ¯”è¡¨
            pivot_f1 = results_df.pivot(index='Dataset', columns='Method', values='F1_Macro')
            f.write(pivot_f1.round(4).to_markdown())
            f.write("\n\n### è®­ç»ƒæ—¶é—´å¯¹æ¯” (ç§’)\n\n")
            
            # åˆ›å»ºè®­ç»ƒæ—¶é—´å¯¹æ¯”è¡¨
            pivot_time = results_df.pivot(index='Dataset', columns='Method', values='Training_Time')
            f.write(pivot_time.round(3).to_markdown())
            f.write("\n\n")
            
            # æ–¹æ³•æ€§èƒ½ç»Ÿè®¡
            f.write("""## æ–¹æ³•æ€§èƒ½ç»Ÿè®¡

### å¹³å‡æ€§èƒ½æ±‡æ€»

""")
            
            # è®¡ç®—å¹³å‡æ€§èƒ½
            avg_performance = results_df.groupby('Method').agg({
                'Accuracy': ['mean', 'std'],
                'F1_Macro': ['mean', 'std'],
                'Training_Time': ['mean', 'std']
            }).round(4)
            
            f.write(avg_performance.to_markdown())
            f.write("\n\n### æ€§èƒ½æ’ååˆ†æ\n\n")
            
            # è®¡ç®—ç®€å•çš„å¹³å‡å€¼ç”¨äºæ’å
            simple_avg = results_df.groupby('Method').agg({
                'Accuracy': 'mean',
                'F1_Macro': 'mean',
                'Training_Time': 'mean'
            }).round(4)
            
            # æŒ‰å‡†ç¡®ç‡æ’åº
            acc_ranking = simple_avg.sort_values('Accuracy', ascending=False)
            f.write("#### å‡†ç¡®ç‡æ’å\n\n")
            for i, (method, row) in enumerate(acc_ranking.iterrows(), 1):
                emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
                f.write(f"{emoji} **{method}**: {row['Accuracy']:.2%}\n")
            f.write("\n")
            
            # æŒ‰F1åˆ†æ•°æ’åº
            f1_ranking = simple_avg.sort_values('F1_Macro', ascending=False)
            f.write("#### F1åˆ†æ•°æ’å\n\n")
            for i, (method, row) in enumerate(f1_ranking.iterrows(), 1):
                emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
                f.write(f"{emoji} **{method}**: {row['F1_Macro']:.2%}\n")
            f.write("\n")
            
            # æŒ‰è®­ç»ƒæ—¶é—´æ’åºï¼ˆè¶Šå¿«è¶Šå¥½ï¼‰
            time_ranking = simple_avg.sort_values('Training_Time', ascending=True)
            f.write("#### è®­ç»ƒæ•ˆç‡æ’å (è¶Šå¿«è¶Šå¥½)\n\n")
            for i, (method, row) in enumerate(time_ranking.iterrows(), 1):
                emoji = "ğŸš€" if i == 1 else "âš¡" if i == 2 else "ğŸ’¨" if i == 3 else f"{i}."
                f.write(f"{emoji} **{method}**: {row['Training_Time']:.3f}ç§’\n")
            f.write("\n")
            
            f.write(f"""## æ ¸å¿ƒå‘ç°ï¼šæŸ¯è¥¿åˆ†å¸ƒå°ºåº¦å‚æ•°çš„å½±å“

### ç»Ÿä¸€æ¶æ„æ–¹æ³•å¯¹æ¯”

æœ¬å®éªŒçš„æ ¸å¿ƒç›®æ ‡æ˜¯éªŒè¯**æŸ¯è¥¿åˆ†å¸ƒå°ºåº¦å‚æ•°**å¯¹åˆ†ç±»æ€§èƒ½çš„å½±å“ã€‚é€šè¿‡ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ç½‘ç»œæ¶æ„ï¼Œæˆ‘ä»¬å¯ä»¥å‡†ç¡®åˆ†æä¸åŒåˆ†å¸ƒé€‰æ‹©çš„æ•ˆæœã€‚

""")
            
            # åˆ†æç»Ÿä¸€æ¶æ„æ–¹æ³•
            unified_methods = results_df[results_df['Method_Type'] == 'unified']
            if not unified_methods.empty:
                unified_summary = unified_methods.groupby('Method').agg({
                    'Accuracy': 'mean',
                    'F1_Macro': 'mean',
                    'Training_Time': 'mean'
                }).round(4)
                
                f.write("#### ç»Ÿä¸€æ¶æ„æ–¹æ³•æ€§èƒ½å¯¹æ¯”\n\n")
                f.write(unified_summary.to_markdown())
                f.write("\n\n")
            
            # ä¸ç»å…¸æ–¹æ³•æ¯”è¾ƒ
            sklearn_methods = results_df[results_df['Method_Type'] == 'sklearn']
            if not sklearn_methods.empty:
                sklearn_summary = sklearn_methods.groupby('Method').agg({
                    'Accuracy': 'mean',
                    'F1_Macro': 'mean',
                    'Training_Time': 'mean'
                }).round(4)
                
                f.write("### ä¸ç»å…¸æœºå™¨å­¦ä¹ æ–¹æ³•å¯¹æ¯”\n\n")
                f.write(sklearn_summary.to_markdown())
                f.write("\n\n")
            
            # ç»“è®ºå’Œå»ºè®®
            f.write(f"""## å®éªŒç»“è®º

### ä¸»è¦å‘ç°

1. **æŸ¯è¥¿åˆ†å¸ƒå°ºåº¦å‚æ•°çš„ä½œç”¨**: 
   - åœ¨ç»Ÿä¸€æ¶æ„å®éªŒä¸­ï¼ŒæŸ¯è¥¿åˆ†å¸ƒå‚æ•°çš„æ•ˆæœéœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†ç‰¹æ€§è¿›ä¸€æ­¥åˆ†æ
   - ä¸åŒåˆ†å¸ƒé€‰æ‹©(æŸ¯è¥¿vsé«˜æ–¯)åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°æœ‰å·®å¼‚

2. **æ–¹æ³•é€‚ç”¨æ€§åˆ†æ**:
   - **é«˜å‡†ç¡®ç‡åœºæ™¯**: Random Forestå’ŒSVMè¡¨ç°çªå‡º
   - **è®­ç»ƒæ•ˆç‡åœºæ™¯**: ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•è®­ç»ƒæ›´å¿«
   - **ä¸ç¡®å®šæ€§é‡åŒ–åœºæ™¯**: CAACæ–¹æ³•æä¾›ç‹¬ç‰¹ä»·å€¼

3. **æ¶æ„è®¾è®¡éªŒè¯**:
   - ç»Ÿä¸€æ¶æ„è®¾è®¡ç¡®ä¿äº†å…¬å¹³æ¯”è¾ƒ
   - ç½‘ç»œæ·±åº¦å’Œå®½åº¦è®¾ç½®å¯¹å°æ•°æ®é›†é€‚å½“

### æ”¹è¿›å»ºè®®

**çŸ­æœŸæ”¹è¿›**:
1. è°ƒæ•´ç½‘ç»œæ¶æ„å‚æ•°ï¼Œé’ˆå¯¹ä¸åŒè§„æ¨¡æ•°æ®é›†ä¼˜åŒ–
2. å®æ–½æ›´ç²¾ç»†çš„è¶…å‚æ•°è°ƒä¼˜
3. å¢åŠ æ•°æ®å¢å¼ºæŠ€æœ¯

**é•¿æœŸå‘å±•**:
1. åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸ŠéªŒè¯æ–¹æ³•å¯æ‰©å±•æ€§  
2. æ¢ç´¢è‡ªé€‚åº”åˆ†å¸ƒé€‰æ‹©æœºåˆ¶
3. å¼€å‘å®æ—¶ä¸ç¡®å®šæ€§é‡åŒ–åº”ç”¨

### é€‚ç”¨åœºæ™¯æ¨è

**æ¨èä½¿ç”¨CAAC OvR**:
- éœ€è¦ä¸ç¡®å®šæ€§é‡åŒ–çš„å…³é”®å†³ç­–åœºæ™¯
- åŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ç­‰é«˜é£é™©åº”ç”¨
- ç ”ç©¶å’Œæ•™å­¦ä¸­çš„æ–¹æ³•è®ºéªŒè¯

**æ¨èä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•**:
- è¿½æ±‚æœ€é«˜å‡†ç¡®ç‡çš„ç«èµ›åœºæ™¯
- è®¡ç®—èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²
- å¿«é€ŸåŸå‹å¼€å‘å’Œbaselineå»ºç«‹

## å¯è§†åŒ–ç»“æœ

å®éªŒç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨åŒ…å«ï¼š
- å‡†ç¡®ç‡å¯¹æ¯”å›¾
- F1åˆ†æ•°å¯¹æ¯”å›¾  
- è®­ç»ƒæ—¶é—´å¯¹æ¯”å›¾
- æ•ˆç‡vsæ€§èƒ½æƒè¡¡æ•£ç‚¹å›¾

![æ–¹æ³•æ¯”è¾ƒå›¾](./methods_comparison_english_{timestamp}.png)

## æ•°æ®æ–‡ä»¶

- **è¯¦ç»†ç»“æœ**: `methods_comparison_detailed_{timestamp}.csv`
- **æ±‡æ€»ç»Ÿè®¡**: `methods_comparison_summary_{timestamp}.csv`
- **å¯è§†åŒ–å›¾è¡¨**: `methods_comparison_english_{timestamp}.png`

---

**å®éªŒé…ç½®ä¿¡æ¯**:
- Pythonç¯å¢ƒ: base condaç¯å¢ƒ
- éšæœºç§å­: 42 (ç¡®ä¿å¯é‡å¤æ€§)
- æ•°æ®åˆ†å‰²: 80%è®­ç»ƒ / 20%æµ‹è¯•
- ç‰¹å¾æ ‡å‡†åŒ–: StandardScaler
- æ—©åœç­–ç•¥: patience=10, min_delta=0.0001

*æŠ¥å‘Šç”±è‡ªåŠ¨åŒ–å®éªŒè„šæœ¬ç”Ÿæˆäº {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
""")
        
        print(f"âœ… Detailed report generated: {report_file.name}")
        return report_file


def run_comparison_experiments(**kwargs):
    """Standalone function for method comparison experiments."""
    runner = MethodComparisonRunner()
    return runner.run_comparison_experiments(**kwargs) 