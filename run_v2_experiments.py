"""
V2 Shared Cauchy OvR Classifier å®éªŒè„šæœ¬

æµ‹è¯•åŸºäºå…±äº«æ½œåœ¨æŸ¯è¥¿å‘é‡çš„One-vs-Reståˆ†ç±»å™¨åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½ï¼Œ
åŒ…æ‹¬å¤šåˆ†ç±»ä»»åŠ¡ã€ä¸ç¡®å®šæ€§é‡åŒ–ã€å¤§è§„æ¨¡ç±»åˆ«å¤„ç†ã€å™ªå£°é²æ£’æ€§ç­‰ã€‚
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import json
from datetime import datetime
from sklearn.datasets import make_classification, make_blobs
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.multiclass import OneVsRestClassifier
import time
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥V2æ¨¡å‹
from src.models import (
    SharedCauchyOvRClassifier,
    create_loss_function,
    SharedCauchyOvRTrainer
)


def generate_multiclass_dataset(
    n_samples: int = 2000,
    n_features: int = 20,
    n_classes: int = 10,
    n_informative: int = 15,
    noise_level: float = 0.1,
    outlier_ratio: float = 0.0,
    cluster_std: float = 1.0,
    random_state: int = 42
):
    """ç”Ÿæˆå¤šåˆ†ç±»æ•°æ®é›†"""
    print(f"ç”Ÿæˆæ•°æ®é›†: {n_samples}æ ·æœ¬, {n_features}ç‰¹å¾, {n_classes}ç±»åˆ«")
    
    # ç”ŸæˆåŸºç¡€æ•°æ®
    X, y = make_classification(
        n_samples=n_samples,
        n_features=n_features,
        n_classes=n_classes,
        n_informative=n_informative,
        n_redundant=n_features - n_informative,
        n_clusters_per_class=1,
        class_sep=1.0,
        random_state=random_state
    )
    
    # æ·»åŠ å™ªå£°
    if noise_level > 0:
        noise = np.random.normal(0, noise_level, X.shape)
        X = X + noise
    
    # æ·»åŠ å¼‚å¸¸å€¼
    if outlier_ratio > 0:
        n_outliers = int(len(X) * outlier_ratio)
        outlier_indices = np.random.choice(len(X), n_outliers, replace=False)
        # ç”Ÿæˆè¿œç¦»æ­£å¸¸æ•°æ®çš„å¼‚å¸¸å€¼
        outlier_scale = 3.0
        X[outlier_indices] = np.random.normal(
            X.mean(axis=0), 
            X.std(axis=0) * outlier_scale, 
            (n_outliers, n_features)
        )
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    
    return X, y, scaler


def create_data_loaders(X, y, test_size=0.2, val_size=0.1, batch_size=64):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    # åˆ†å‰²æ•°æ®
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=y
    )
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size/(1-test_size), 
        random_state=42, stratify=y_temp
    )
    
    # è½¬æ¢ä¸ºå¼ é‡
    X_train = torch.FloatTensor(X_train)
    X_val = torch.FloatTensor(X_val)
    X_test = torch.FloatTensor(X_test)
    y_train = torch.LongTensor(y_train)
    y_val = torch.LongTensor(y_val)
    y_test = torch.LongTensor(y_test)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    test_dataset = TensorDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    return {
        'train_loader': train_loader,
        'val_loader': val_loader,
        'test_loader': test_loader,
        'data': {
            'X_train': X_train.numpy(),
            'X_val': X_val.numpy(),
            'X_test': X_test.numpy(),
            'y_train': y_train.numpy(),
            'y_val': y_val.numpy(),
            'y_test': y_test.numpy()
        }
    }


def run_shared_cauchy_experiment(
    data_loaders: dict,
    n_features: int,
    n_classes: int,
    latent_dim: int = None,
    loss_type: str = 'ovr_bce',
    epochs: int = 50,
    learning_rate: float = 1e-3,
    hidden_dims: list = None,
    device: str = 'auto'
):
    """è¿è¡ŒShared Cauchy OvRå®éªŒ"""
    
    if latent_dim is None:
        latent_dim = min(n_classes // 2, 20)  # é»˜è®¤æ½œåœ¨ç»´åº¦
    
    if hidden_dims is None:
        hidden_dims = [128, 64]
    
    print(f"è¿è¡ŒShared Cauchy OvRå®éªŒ:")
    print(f"  - æ½œåœ¨ç»´åº¦: {latent_dim}")
    print(f"  - æŸå¤±å‡½æ•°: {loss_type}")
    print(f"  - éšè—å±‚: {hidden_dims}")
    
    # åˆ›å»ºæ¨¡å‹
    model = SharedCauchyOvRClassifier(
        input_dim=n_features,
        num_classes=n_classes,
        latent_dim=latent_dim,
        hidden_dims=hidden_dims
    )
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    loss_function = create_loss_function(loss_type)
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=5, factor=0.5
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SharedCauchyOvRTrainer(
        model=model,
        loss_function=loss_function,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device
    )
    
    # è®­ç»ƒ
    start_time = time.time()
    history = trainer.train(
        train_loader=data_loaders['train_loader'],
        val_loader=data_loaders['val_loader'],
        num_epochs=epochs,
        early_stopping_patience=10
    )
    train_time = time.time() - start_time
    
    # è¯„ä¼°
    results = trainer.evaluate(
        data_loaders['test_loader'],
        class_names=[f'Class_{i}' for i in range(n_classes)]
    )
    
    # ä¸ç¡®å®šæ€§åˆ†æ
    uncertainty_analysis = trainer.analyze_uncertainty(data_loaders['test_loader'])
    
    return {
        'model': model,
        'trainer': trainer,
        'history': history,
        'results': results,
        'uncertainty_analysis': uncertainty_analysis,
        'train_time': train_time,
        'config': {
            'latent_dim': latent_dim,
            'loss_type': loss_type,
            'epochs': epochs,
            'learning_rate': learning_rate,
            'hidden_dims': hidden_dims
        }
    }


def run_baseline_experiments(data_loaders: dict, n_classes: int):
    """è¿è¡ŒåŸºçº¿æ–¹æ³•å®éªŒ"""
    print("è¿è¡ŒåŸºçº¿æ–¹æ³•å®éªŒ...")
    
    # æå–æ•°æ®
    X_train = data_loaders['data']['X_train']
    X_test = data_loaders['data']['X_test']
    y_train = data_loaders['data']['y_train']
    y_test = data_loaders['data']['y_test']
    
    baselines = {
        'Softmax_LR': LogisticRegression(
            multi_class='multinomial', 
            solver='lbfgs', 
            max_iter=1000,
            random_state=42
        ),
        'OvR_LR': OneVsRestClassifier(
            LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)
        ),
        'RandomForest': RandomForestClassifier(
            n_estimators=100, 
            random_state=42
        ),
        'SVM_OvR': OneVsRestClassifier(
            SVC(kernel='rbf', probability=True, random_state=42)
        ),
        'MLP': MLPClassifier(
            hidden_layer_sizes=(128, 64),
            max_iter=500,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1
        )
    }
    
    baseline_results = {}
    
    for name, model in baselines.items():
        print(f"  è®­ç»ƒ {name}...")
        start_time = time.time()
        
        try:
            model.fit(X_train, y_train)
            train_time = time.time() - start_time
            
            # é¢„æµ‹
            y_pred = model.predict(X_test)
            y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred, average='weighted')
            
            baseline_results[name] = {
                'model': model,
                'accuracy': accuracy,
                'f1': f1,
                'train_time': train_time,
                'predictions': y_pred,
                'probabilities': y_proba
            }
            
            print(f"    {name}: å‡†ç¡®ç‡={accuracy:.4f}, F1={f1:.4f}, æ—¶é—´={train_time:.2f}s")
            
        except Exception as e:
            print(f"    {name} è®­ç»ƒå¤±è´¥: {str(e)}")
            baseline_results[name] = None
    
    return baseline_results


def analyze_scalability(
    base_config: dict,
    class_counts: list = [5, 10, 20, 50, 100],
    sample_counts: list = None
):
    """åˆ†æå¯æ‰©å±•æ€§"""
    print("\n" + "="*50)
    print("å¯æ‰©å±•æ€§åˆ†æ")
    print("="*50)
    
    if sample_counts is None:
        sample_counts = [count * 100 for count in class_counts]  # æ¯ç±»100æ ·æœ¬
    
    scalability_results = []
    
    for n_classes, n_samples in zip(class_counts, sample_counts):
        print(f"\næµ‹è¯• {n_classes} ç±»åˆ«, {n_samples} æ ·æœ¬...")
        
        # ç”Ÿæˆæ•°æ®
        X, y, _ = generate_multiclass_dataset(
            n_samples=n_samples,
            n_features=base_config['n_features'],
            n_classes=n_classes,
            random_state=42
        )
        
        data_loaders = create_data_loaders(X, y, batch_size=64)
        
        # è¿è¡Œå®éªŒ
        result = run_shared_cauchy_experiment(
            data_loaders=data_loaders,
            n_features=base_config['n_features'],
            n_classes=n_classes,
            epochs=30,  # å‡å°‘epochsä»¥èŠ‚çœæ—¶é—´
            device='cpu'
        )
        
        scalability_results.append({
            'n_classes': n_classes,
            'n_samples': n_samples,
            'accuracy': result['results']['accuracy'],
            'f1': result['results']['classification_report']['weighted avg']['f1-score'],
            'train_time': result['train_time'],
            'model_params': sum(p.numel() for p in result['model'].parameters()),
            'avg_uncertainty': result['uncertainty_analysis']['uncertainties'].mean()
        })
        
        print(f"  å‡†ç¡®ç‡: {result['results']['accuracy']:.4f}")
        print(f"  è®­ç»ƒæ—¶é—´: {result['train_time']:.2f}s")
        print(f"  æ¨¡å‹å‚æ•°: {sum(p.numel() for p in result['model'].parameters()):,}")
    
    return scalability_results


def analyze_loss_functions(data_loaders: dict, n_features: int, n_classes: int):
    """åˆ†æä¸åŒæŸå¤±å‡½æ•°çš„æ•ˆæœ"""
    print("\n" + "="*50)
    print("æŸå¤±å‡½æ•°æ¯”è¾ƒåˆ†æ")
    print("="*50)
    
    loss_functions = [
        'ovr_bce',
        'weighted_ovr_bce',
        'focal_ovr',
        'uncertainty_reg'
    ]
    
    loss_results = []
    
    for loss_type in loss_functions:
        print(f"\næµ‹è¯•æŸå¤±å‡½æ•°: {loss_type}")
        
        result = run_shared_cauchy_experiment(
            data_loaders=data_loaders,
            n_features=n_features,
            n_classes=n_classes,
            loss_type=loss_type,
            epochs=40,
            device='cpu'
        )
        
        loss_results.append({
            'loss_type': loss_type,
            'accuracy': result['results']['accuracy'],
            'f1': result['results']['classification_report']['weighted avg']['f1-score'],
            'train_time': result['train_time'],
            'final_train_loss': result['history']['train_loss'][-1],
            'final_val_loss': result['history']['val_loss'][-1],
            'avg_uncertainty': result['uncertainty_analysis']['uncertainties'].mean(),
            'convergence_epoch': len(result['history']['train_loss'])
        })
        
        print(f"  å‡†ç¡®ç‡: {result['results']['accuracy']:.4f}")
        print(f"  å¹³å‡ä¸ç¡®å®šæ€§: {result['uncertainty_analysis']['uncertainties'].mean():.4f}")
    
    return loss_results


def analyze_noise_robustness(
    base_config: dict,
    noise_levels: list = [0.0, 0.1, 0.2, 0.3],
    outlier_ratios: list = [0.0, 0.05, 0.1, 0.2]
):
    """åˆ†æå™ªå£°é²æ£’æ€§"""
    print("\n" + "="*50)
    print("å™ªå£°é²æ£’æ€§åˆ†æ")
    print("="*50)
    
    robustness_results = []
    
    # æµ‹è¯•å™ªå£°æ°´å¹³
    for noise_level in noise_levels:
        print(f"\næµ‹è¯•å™ªå£°æ°´å¹³: {noise_level}")
        
        X, y, _ = generate_multiclass_dataset(
            n_samples=base_config['n_samples'],
            n_features=base_config['n_features'],
            n_classes=base_config['n_classes'],
            noise_level=noise_level,
            random_state=42
        )
        
        data_loaders = create_data_loaders(X, y)
        
        # Shared Cauchy OvR
        cauchy_result = run_shared_cauchy_experiment(
            data_loaders=data_loaders,
            n_features=base_config['n_features'],
            n_classes=base_config['n_classes'],
            epochs=30,
            device='cpu'
        )
        
        # åŸºçº¿æ–¹æ³•
        baseline_results = run_baseline_experiments(data_loaders, base_config['n_classes'])
        
        result_entry = {
            'condition': 'noise',
            'level': noise_level,
            'cauchy_accuracy': cauchy_result['results']['accuracy'],
            'cauchy_uncertainty': cauchy_result['uncertainty_analysis']['uncertainties'].mean()
        }
        
        for name, baseline in baseline_results.items():
            if baseline is not None:
                result_entry[f'{name.lower()}_accuracy'] = baseline['accuracy']
        
        robustness_results.append(result_entry)
    
    # æµ‹è¯•å¼‚å¸¸å€¼æ¯”ä¾‹
    for outlier_ratio in outlier_ratios:
        print(f"\næµ‹è¯•å¼‚å¸¸å€¼æ¯”ä¾‹: {outlier_ratio}")
        
        X, y, _ = generate_multiclass_dataset(
            n_samples=base_config['n_samples'],
            n_features=base_config['n_features'],
            n_classes=base_config['n_classes'],
            outlier_ratio=outlier_ratio,
            random_state=42
        )
        
        data_loaders = create_data_loaders(X, y)
        
        # Shared Cauchy OvR
        cauchy_result = run_shared_cauchy_experiment(
            data_loaders=data_loaders,
            n_features=base_config['n_features'],
            n_classes=base_config['n_classes'],
            epochs=30,
            device='cpu'
        )
        
        # åŸºçº¿æ–¹æ³•
        baseline_results = run_baseline_experiments(data_loaders, base_config['n_classes'])
        
        result_entry = {
            'condition': 'outlier',
            'level': outlier_ratio,
            'cauchy_accuracy': cauchy_result['results']['accuracy'],
            'cauchy_uncertainty': cauchy_result['uncertainty_analysis']['uncertainties'].mean()
        }
        
        for name, baseline in baseline_results.items():
            if baseline is not None:
                result_entry[f'{name.lower()}_accuracy'] = baseline['accuracy']
        
        robustness_results.append(result_entry)
    
    return robustness_results


def save_results(results: dict, results_dir: str):
    """ä¿å­˜å®éªŒç»“æœ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜ä¸»è¦ç»“æœ
    main_results_file = os.path.join(results_dir, f"v2_experiment_results_{timestamp}.json")
    
    # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {key: convert_numpy(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(item) for item in obj]
        else:
            return obj
    
    # åˆ›å»ºå¯åºåˆ—åŒ–çš„ç»“æœå‰¯æœ¬ï¼Œæ’é™¤æ¨¡å‹å¯¹è±¡
    def make_serializable(obj):
        if isinstance(obj, dict):
            serializable_dict = {}
            for key, value in obj.items():
                # è·³è¿‡æ¨¡å‹å¯¹è±¡å’Œè®­ç»ƒå™¨å¯¹è±¡
                if key in ['model', 'trainer'] or str(type(value).__name__).endswith('Classifier') or str(type(value).__name__).endswith('Trainer'):
                    continue
                serializable_dict[key] = make_serializable(value)
            return serializable_dict
        elif isinstance(obj, list):
            return [make_serializable(item) for item in obj]
        else:
            return convert_numpy(obj)
    
    serializable_results = make_serializable(results)
    
    with open(main_results_file, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
    
    print(f"å®éªŒç»“æœå·²ä¿å­˜åˆ°: {main_results_file}")
    
    return main_results_file


def plot_comprehensive_results(results: dict, results_dir: str):
    """ç»˜åˆ¶ç»¼åˆç»“æœå›¾è¡¨"""
    plt.style.use('default')
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. å¯æ‰©å±•æ€§åˆ†æå›¾
    if 'scalability' in results:
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        scalability_df = pd.DataFrame(results['scalability'])
        
        # å‡†ç¡®ç‡ vs ç±»åˆ«æ•°
        axes[0, 0].plot(scalability_df['n_classes'], scalability_df['accuracy'], 'bo-')
        axes[0, 0].set_xlabel('Number of Classes')
        axes[0, 0].set_ylabel('Test Accuracy')
        axes[0, 0].set_title('Scalability: Accuracy vs Classes')
        axes[0, 0].grid(True)
        
        # è®­ç»ƒæ—¶é—´ vs ç±»åˆ«æ•°
        axes[0, 1].plot(scalability_df['n_classes'], scalability_df['train_time'], 'ro-')
        axes[0, 1].set_xlabel('Number of Classes')
        axes[0, 1].set_ylabel('Training Time (s)')
        axes[0, 1].set_title('Scalability: Training Time vs Classes')
        axes[0, 1].grid(True)
        
        # æ¨¡å‹å‚æ•° vs ç±»åˆ«æ•°
        axes[1, 0].plot(scalability_df['n_classes'], scalability_df['model_params'], 'go-')
        axes[1, 0].set_xlabel('Number of Classes')
        axes[1, 0].set_ylabel('Model Parameters')
        axes[1, 0].set_title('Scalability: Parameters vs Classes')
        axes[1, 0].grid(True)
        
        # ä¸ç¡®å®šæ€§ vs ç±»åˆ«æ•°
        axes[1, 1].plot(scalability_df['n_classes'], scalability_df['avg_uncertainty'], 'mo-')
        axes[1, 1].set_xlabel('Number of Classes')
        axes[1, 1].set_ylabel('Average Uncertainty')
        axes[1, 1].set_title('Scalability: Uncertainty vs Classes')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        scalability_plot_file = os.path.join(results_dir, f"scalability_analysis_{timestamp}.png")
        plt.savefig(scalability_plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"å¯æ‰©å±•æ€§åˆ†æå›¾å·²ä¿å­˜åˆ°: {scalability_plot_file}")
    
    # 2. æŸå¤±å‡½æ•°æ¯”è¾ƒå›¾
    if 'loss_comparison' in results:
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        loss_df = pd.DataFrame(results['loss_comparison'])
        
        # å‡†ç¡®ç‡æ¯”è¾ƒ
        axes[0].bar(loss_df['loss_type'], loss_df['accuracy'])
        axes[0].set_ylabel('Test Accuracy')
        axes[0].set_title('Loss Function Comparison: Accuracy')
        axes[0].tick_params(axis='x', rotation=45)
        
        # ä¸ç¡®å®šæ€§æ¯”è¾ƒ
        axes[1].bar(loss_df['loss_type'], loss_df['avg_uncertainty'])
        axes[1].set_ylabel('Average Uncertainty')
        axes[1].set_title('Loss Function Comparison: Uncertainty')
        axes[1].tick_params(axis='x', rotation=45)
        
        # æ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ
        axes[2].bar(loss_df['loss_type'], loss_df['convergence_epoch'])
        axes[2].set_ylabel('Convergence Epoch')
        axes[2].set_title('Loss Function Comparison: Convergence')
        axes[2].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        loss_plot_file = os.path.join(results_dir, f"loss_comparison_{timestamp}.png")
        plt.savefig(loss_plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"æŸå¤±å‡½æ•°æ¯”è¾ƒå›¾å·²ä¿å­˜åˆ°: {loss_plot_file}")
    
    # 3. é²æ£’æ€§åˆ†æå›¾
    if 'robustness' in results:
        robustness_df = pd.DataFrame(results['robustness'])
        
        # åˆ†åˆ«åˆ†æå™ªå£°å’Œå¼‚å¸¸å€¼
        noise_data = robustness_df[robustness_df['condition'] == 'noise']
        outlier_data = robustness_df[robustness_df['condition'] == 'outlier']
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # å™ªå£°é²æ£’æ€§
        if not noise_data.empty:
            model_cols = [col for col in noise_data.columns if col.endswith('_accuracy')]
            for col in model_cols:
                model_name = col.replace('_accuracy', '').upper()
                axes[0].plot(noise_data['level'], noise_data[col], 'o-', label=model_name)
            axes[0].set_xlabel('Noise Level')
            axes[0].set_ylabel('Test Accuracy')
            axes[0].set_title('Robustness to Noise')
            axes[0].legend()
            axes[0].grid(True)
        
        # å¼‚å¸¸å€¼é²æ£’æ€§
        if not outlier_data.empty:
            model_cols = [col for col in outlier_data.columns if col.endswith('_accuracy')]
            for col in model_cols:
                model_name = col.replace('_accuracy', '').upper()
                axes[1].plot(outlier_data['level'], outlier_data[col], 'o-', label=model_name)
            axes[1].set_xlabel('Outlier Ratio')
            axes[1].set_ylabel('Test Accuracy')
            axes[1].set_title('Robustness to Outliers')
            axes[1].legend()
            axes[1].grid(True)
        
        plt.tight_layout()
        robustness_plot_file = os.path.join(results_dir, f"robustness_analysis_{timestamp}.png")
        plt.savefig(robustness_plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"é²æ£’æ€§åˆ†æå›¾å·²ä¿å­˜åˆ°: {robustness_plot_file}")


def main():
    """ä¸»å®éªŒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹V2 Shared Cauchy OvR Classifierå®éªŒ")
    print("="*60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'results_v2')
    os.makedirs(results_dir, exist_ok=True)
    
    # åŸºç¡€é…ç½®
    base_config = {
        'n_samples': 2000,
        'n_features': 20,
        'n_classes': 10
    }
    
    all_results = {}
    
    try:
        # 1. åŸºç¡€æ€§èƒ½æµ‹è¯•
        print("\n" + "="*50)
        print("åŸºç¡€æ€§èƒ½æµ‹è¯•")
        print("="*50)
        
        X, y, scaler = generate_multiclass_dataset(**base_config)
        data_loaders = create_data_loaders(X, y)
        
        # Shared Cauchy OvRå®éªŒ
        cauchy_result = run_shared_cauchy_experiment(
            data_loaders=data_loaders,
            n_features=base_config['n_features'],
            n_classes=base_config['n_classes'],
            epochs=50
        )
        
        # åŸºçº¿æ–¹æ³•å®éªŒ
        baseline_results = run_baseline_experiments(data_loaders, base_config['n_classes'])
        
        all_results['main_experiment'] = {
            'cauchy': cauchy_result,
            'baselines': baseline_results
        }
        
        # 2. å¯æ‰©å±•æ€§åˆ†æ
        scalability_results = analyze_scalability(base_config)
        all_results['scalability'] = scalability_results
        
        # 3. æŸå¤±å‡½æ•°æ¯”è¾ƒ
        loss_results = analyze_loss_functions(
            data_loaders, base_config['n_features'], base_config['n_classes']
        )
        all_results['loss_comparison'] = loss_results
        
        # 4. å™ªå£°é²æ£’æ€§åˆ†æ
        robustness_results = analyze_noise_robustness(base_config)
        all_results['robustness'] = robustness_results
        
        # ä¿å­˜ç»“æœ
        results_file = save_results(all_results, results_dir)
        
        # ç»˜åˆ¶å›¾è¡¨
        plot_comprehensive_results(all_results, results_dir)
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ‰ å®éªŒå®Œæˆæ‘˜è¦")
        print("="*60)
        
        # ä¸»å®éªŒç»“æœ
        main_cauchy = all_results['main_experiment']['cauchy']
        print(f"Shared Cauchy OvRæ€§èƒ½:")
        print(f"  - æµ‹è¯•å‡†ç¡®ç‡: {main_cauchy['results']['accuracy']:.4f}")
        print(f"  - åŠ æƒF1åˆ†æ•°: {main_cauchy['results']['classification_report']['weighted avg']['f1-score']:.4f}")
        print(f"  - è®­ç»ƒæ—¶é—´: {main_cauchy['train_time']:.2f}s")
        print(f"  - å¹³å‡ä¸ç¡®å®šæ€§: {main_cauchy['uncertainty_analysis']['uncertainties'].mean():.4f}")
        
        print(f"\nåŸºçº¿æ–¹æ³•å¯¹æ¯”:")
        for name, result in all_results['main_experiment']['baselines'].items():
            if result is not None:
                print(f"  - {name}: å‡†ç¡®ç‡={result['accuracy']:.4f}, F1={result['f1']:.4f}")
        
        # å¯æ‰©å±•æ€§ç»“æœ
        if scalability_results:
            max_classes = max(r['n_classes'] for r in scalability_results)
            max_acc = [r['accuracy'] for r in scalability_results if r['n_classes'] == max_classes][0]
            print(f"\nå¯æ‰©å±•æ€§:")
            print(f"  - æœ€å¤§æµ‹è¯•ç±»åˆ«: {max_classes}")
            print(f"  - å¯¹åº”å‡†ç¡®ç‡: {max_acc:.4f}")
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
        
        return all_results, results_dir
        
    except Exception as e:
        print(f"\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, results_dir


if __name__ == "__main__":
    results, results_dir = main() 